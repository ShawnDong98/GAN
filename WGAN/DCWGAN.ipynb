{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ShawnDong98/GAN/blob/master/WGAN/DCWGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, channels=3, clip_value=1, img_size=64, latent_dim=100, lr=0.0002, n_cpu=8, n_critic=5, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"learning rate\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\n",
    "parser.add_argument(\"--clip_value\", type=float, default=1, help=\"lower and upper clip value for disc. weights\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "opt = parser.parse_known_args()[0]\n",
    "print(opt)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = opt.img_size // 4\n",
    "        # 第一层是为了把的噪声（batch_size, 100)\n",
    "        # 拉到 (batch_size, 128, img_size, img_size)\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=32768, bias=True)\n",
      "  )\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator()\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "        # 这里相较于DCGAN删去了sigmoid函数\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Dropout2d(p=0.25, inplace=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Dropout2d(p=0.25, inplace=False)\n",
      "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Dropout2d(p=0.25, inplace=False)\n",
      "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (13): Dropout2d(p=0.25, inplace=False)\n",
      "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (adv_layer): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator()\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里相较于DCGAN没有使用交叉熵\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        \"data/\",\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "# 这里相较于DCGAN，Adam优化器换成了RMSprop\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# i, (img, _) = next(enumerate(dataloader))\n",
    "# print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/12] [D loss: 0.000054] [G loss: 0.002819]\n",
      "[Epoch 0/200] [Batch 5/12] [D loss: -0.622657] [G loss: -0.057670]\n",
      "[Epoch 0/200] [Batch 10/12] [D loss: -5.629987] [G loss: -1.607274]\n",
      "[Epoch 1/200] [Batch 0/12] [D loss: -2.900025] [G loss: -6.575568]\n",
      "[Epoch 1/200] [Batch 5/12] [D loss: -4.808896] [G loss: -8.813005]\n",
      "[Epoch 1/200] [Batch 10/12] [D loss: -3.036086] [G loss: -11.293425]\n",
      "[Epoch 2/200] [Batch 0/12] [D loss: -2.375845] [G loss: -9.741644]\n",
      "[Epoch 2/200] [Batch 5/12] [D loss: 1.368963] [G loss: -12.234648]\n",
      "[Epoch 2/200] [Batch 10/12] [D loss: 1.076435] [G loss: -10.263998]\n",
      "[Epoch 3/200] [Batch 0/12] [D loss: -0.164483] [G loss: -11.341684]\n",
      "[Epoch 3/200] [Batch 5/12] [D loss: -0.915716] [G loss: -8.551557]\n",
      "[Epoch 3/200] [Batch 10/12] [D loss: -0.342010] [G loss: -8.815022]\n",
      "[Epoch 4/200] [Batch 0/12] [D loss: -0.731910] [G loss: -7.557527]\n",
      "[Epoch 4/200] [Batch 5/12] [D loss: 0.588730] [G loss: -6.643662]\n",
      "[Epoch 4/200] [Batch 10/12] [D loss: 0.150437] [G loss: -5.149102]\n",
      "[Epoch 5/200] [Batch 0/12] [D loss: 0.457991] [G loss: -4.730694]\n",
      "[Epoch 5/200] [Batch 5/12] [D loss: 0.351304] [G loss: -4.060314]\n",
      "[Epoch 5/200] [Batch 10/12] [D loss: -0.750370] [G loss: -3.649652]\n",
      "[Epoch 6/200] [Batch 0/12] [D loss: -0.664475] [G loss: -2.967104]\n",
      "[Epoch 6/200] [Batch 5/12] [D loss: -0.711595] [G loss: -2.028502]\n",
      "[Epoch 6/200] [Batch 10/12] [D loss: -1.401167] [G loss: -2.176431]\n",
      "[Epoch 7/200] [Batch 0/12] [D loss: -1.215695] [G loss: -2.327661]\n",
      "[Epoch 7/200] [Batch 5/12] [D loss: -1.639013] [G loss: -1.216183]\n",
      "[Epoch 7/200] [Batch 10/12] [D loss: -1.247185] [G loss: -1.332104]\n",
      "[Epoch 8/200] [Batch 0/12] [D loss: -0.598529] [G loss: -1.137654]\n",
      "[Epoch 8/200] [Batch 5/12] [D loss: -2.138675] [G loss: -1.763966]\n",
      "[Epoch 8/200] [Batch 10/12] [D loss: -1.533080] [G loss: -2.400023]\n",
      "[Epoch 9/200] [Batch 0/12] [D loss: -1.151877] [G loss: -1.730725]\n",
      "[Epoch 9/200] [Batch 5/12] [D loss: -1.324459] [G loss: -2.318214]\n",
      "[Epoch 9/200] [Batch 10/12] [D loss: -1.468165] [G loss: -2.346774]\n",
      "[Epoch 10/200] [Batch 0/12] [D loss: -1.591388] [G loss: -3.061102]\n",
      "[Epoch 10/200] [Batch 5/12] [D loss: -2.049803] [G loss: -3.233489]\n",
      "[Epoch 10/200] [Batch 10/12] [D loss: -1.497315] [G loss: -2.965793]\n",
      "[Epoch 11/200] [Batch 0/12] [D loss: -1.803635] [G loss: -3.042495]\n",
      "[Epoch 11/200] [Batch 5/12] [D loss: -1.268343] [G loss: -2.398565]\n",
      "[Epoch 11/200] [Batch 10/12] [D loss: -1.773048] [G loss: -2.952245]\n",
      "[Epoch 12/200] [Batch 0/12] [D loss: -2.089484] [G loss: -2.693628]\n",
      "[Epoch 12/200] [Batch 5/12] [D loss: -1.764746] [G loss: -2.237497]\n",
      "[Epoch 12/200] [Batch 10/12] [D loss: -1.663066] [G loss: -2.469412]\n",
      "[Epoch 13/200] [Batch 0/12] [D loss: -2.169145] [G loss: -2.978590]\n",
      "[Epoch 13/200] [Batch 5/12] [D loss: -1.970324] [G loss: -1.797363]\n",
      "[Epoch 13/200] [Batch 10/12] [D loss: -1.497734] [G loss: -2.071749]\n",
      "[Epoch 14/200] [Batch 0/12] [D loss: -2.194741] [G loss: -2.565660]\n",
      "[Epoch 14/200] [Batch 5/12] [D loss: -1.788853] [G loss: -2.292083]\n",
      "[Epoch 14/200] [Batch 10/12] [D loss: -2.375001] [G loss: -2.594490]\n",
      "[Epoch 15/200] [Batch 0/12] [D loss: -1.979030] [G loss: -2.389999]\n",
      "[Epoch 15/200] [Batch 5/12] [D loss: -2.448283] [G loss: -2.424608]\n",
      "[Epoch 15/200] [Batch 10/12] [D loss: -2.122327] [G loss: -2.642997]\n",
      "[Epoch 16/200] [Batch 0/12] [D loss: -1.897406] [G loss: -3.496554]\n",
      "[Epoch 16/200] [Batch 5/12] [D loss: -1.594139] [G loss: -2.783967]\n",
      "[Epoch 16/200] [Batch 10/12] [D loss: -1.696005] [G loss: -2.317808]\n",
      "[Epoch 17/200] [Batch 0/12] [D loss: -1.937752] [G loss: -2.402651]\n",
      "[Epoch 17/200] [Batch 5/12] [D loss: -2.005351] [G loss: -3.002690]\n",
      "[Epoch 17/200] [Batch 10/12] [D loss: -2.181938] [G loss: -2.516232]\n",
      "[Epoch 18/200] [Batch 0/12] [D loss: -2.302083] [G loss: -2.865133]\n",
      "[Epoch 18/200] [Batch 5/12] [D loss: -1.635072] [G loss: -2.463394]\n",
      "[Epoch 18/200] [Batch 10/12] [D loss: -2.523675] [G loss: -3.076443]\n",
      "[Epoch 19/200] [Batch 0/12] [D loss: -2.311106] [G loss: -3.210302]\n",
      "[Epoch 19/200] [Batch 5/12] [D loss: -2.019723] [G loss: -2.934709]\n",
      "[Epoch 19/200] [Batch 10/12] [D loss: -2.589899] [G loss: -2.488640]\n",
      "[Epoch 20/200] [Batch 0/12] [D loss: -2.051433] [G loss: -2.595542]\n",
      "[Epoch 20/200] [Batch 5/12] [D loss: -3.047109] [G loss: -2.431985]\n",
      "[Epoch 20/200] [Batch 10/12] [D loss: -1.872376] [G loss: -2.122684]\n",
      "[Epoch 21/200] [Batch 0/12] [D loss: -2.682943] [G loss: -2.927399]\n",
      "[Epoch 21/200] [Batch 5/12] [D loss: -1.811496] [G loss: -2.277444]\n",
      "[Epoch 21/200] [Batch 10/12] [D loss: -1.905163] [G loss: -2.129863]\n",
      "[Epoch 22/200] [Batch 0/12] [D loss: -2.457450] [G loss: -2.577795]\n",
      "[Epoch 22/200] [Batch 5/12] [D loss: -2.085422] [G loss: -2.626081]\n",
      "[Epoch 22/200] [Batch 10/12] [D loss: -2.216486] [G loss: -3.279980]\n",
      "[Epoch 23/200] [Batch 0/12] [D loss: -2.212715] [G loss: -3.625500]\n",
      "[Epoch 23/200] [Batch 5/12] [D loss: -2.933945] [G loss: -3.134465]\n",
      "[Epoch 23/200] [Batch 10/12] [D loss: -2.240123] [G loss: -3.200258]\n",
      "[Epoch 24/200] [Batch 0/12] [D loss: -2.359323] [G loss: -3.936653]\n",
      "[Epoch 24/200] [Batch 5/12] [D loss: -1.866898] [G loss: -3.345580]\n",
      "[Epoch 24/200] [Batch 10/12] [D loss: -2.111723] [G loss: -3.630101]\n",
      "[Epoch 25/200] [Batch 0/12] [D loss: -2.459692] [G loss: -2.973195]\n",
      "[Epoch 25/200] [Batch 5/12] [D loss: -1.483094] [G loss: -3.848206]\n",
      "[Epoch 25/200] [Batch 10/12] [D loss: -2.334197] [G loss: -3.178617]\n",
      "[Epoch 26/200] [Batch 0/12] [D loss: -1.424008] [G loss: -3.374234]\n",
      "[Epoch 26/200] [Batch 5/12] [D loss: -2.820329] [G loss: -3.158635]\n",
      "[Epoch 26/200] [Batch 10/12] [D loss: -2.991785] [G loss: -3.457574]\n",
      "[Epoch 27/200] [Batch 0/12] [D loss: -1.965149] [G loss: -4.314403]\n",
      "[Epoch 27/200] [Batch 5/12] [D loss: -2.227589] [G loss: -3.737217]\n",
      "[Epoch 27/200] [Batch 10/12] [D loss: -2.369865] [G loss: -3.519416]\n",
      "[Epoch 28/200] [Batch 0/12] [D loss: -2.173620] [G loss: -4.311761]\n",
      "[Epoch 28/200] [Batch 5/12] [D loss: -1.719957] [G loss: -4.305959]\n",
      "[Epoch 28/200] [Batch 10/12] [D loss: -2.003910] [G loss: -4.165915]\n",
      "[Epoch 29/200] [Batch 0/12] [D loss: -2.593261] [G loss: -4.575035]\n",
      "[Epoch 29/200] [Batch 5/12] [D loss: -1.006972] [G loss: -5.988815]\n",
      "[Epoch 29/200] [Batch 10/12] [D loss: -3.341601] [G loss: -4.777153]\n",
      "[Epoch 30/200] [Batch 0/12] [D loss: -1.306270] [G loss: -5.005955]\n",
      "[Epoch 30/200] [Batch 5/12] [D loss: -1.526805] [G loss: -4.125454]\n",
      "[Epoch 30/200] [Batch 10/12] [D loss: -1.991361] [G loss: -4.595824]\n",
      "[Epoch 31/200] [Batch 0/12] [D loss: -1.814172] [G loss: -5.994992]\n",
      "[Epoch 31/200] [Batch 5/12] [D loss: -1.936477] [G loss: -6.057969]\n",
      "[Epoch 31/200] [Batch 10/12] [D loss: -1.986750] [G loss: -4.605537]\n",
      "[Epoch 32/200] [Batch 0/12] [D loss: -2.039652] [G loss: -4.777997]\n",
      "[Epoch 32/200] [Batch 5/12] [D loss: -1.838258] [G loss: -4.915431]\n",
      "[Epoch 32/200] [Batch 10/12] [D loss: -2.227281] [G loss: -5.955860]\n",
      "[Epoch 33/200] [Batch 0/12] [D loss: -1.752680] [G loss: -5.806900]\n",
      "[Epoch 33/200] [Batch 5/12] [D loss: -2.269625] [G loss: -5.603348]\n",
      "[Epoch 33/200] [Batch 10/12] [D loss: -1.899533] [G loss: -4.434042]\n",
      "[Epoch 34/200] [Batch 0/12] [D loss: -2.403328] [G loss: -5.661497]\n",
      "[Epoch 34/200] [Batch 5/12] [D loss: -2.579400] [G loss: -5.241910]\n",
      "[Epoch 34/200] [Batch 10/12] [D loss: -1.895497] [G loss: -4.798047]\n",
      "[Epoch 35/200] [Batch 0/12] [D loss: -2.500922] [G loss: -4.978925]\n",
      "[Epoch 35/200] [Batch 5/12] [D loss: -1.172734] [G loss: -5.566508]\n",
      "[Epoch 35/200] [Batch 10/12] [D loss: -2.489787] [G loss: -4.621791]\n",
      "[Epoch 36/200] [Batch 0/12] [D loss: -3.353627] [G loss: -4.126333]\n",
      "[Epoch 36/200] [Batch 5/12] [D loss: -1.904161] [G loss: -5.087600]\n",
      "[Epoch 36/200] [Batch 10/12] [D loss: -2.981005] [G loss: -5.715138]\n",
      "[Epoch 37/200] [Batch 0/12] [D loss: -2.272204] [G loss: -5.776443]\n",
      "[Epoch 37/200] [Batch 5/12] [D loss: -2.286799] [G loss: -4.555065]\n",
      "[Epoch 37/200] [Batch 10/12] [D loss: -3.730428] [G loss: -7.016762]\n",
      "[Epoch 38/200] [Batch 0/12] [D loss: -2.040513] [G loss: -5.565241]\n",
      "[Epoch 38/200] [Batch 5/12] [D loss: -1.649238] [G loss: -5.198634]\n",
      "[Epoch 38/200] [Batch 10/12] [D loss: -2.791833] [G loss: -5.240105]\n",
      "[Epoch 39/200] [Batch 0/12] [D loss: -1.182160] [G loss: -5.635160]\n",
      "[Epoch 39/200] [Batch 5/12] [D loss: -1.670236] [G loss: -5.533861]\n",
      "[Epoch 39/200] [Batch 10/12] [D loss: -2.646812] [G loss: -6.176666]\n",
      "[Epoch 40/200] [Batch 0/12] [D loss: -3.245400] [G loss: -5.721485]\n",
      "[Epoch 40/200] [Batch 5/12] [D loss: -2.209293] [G loss: -6.728903]\n",
      "[Epoch 40/200] [Batch 10/12] [D loss: -2.993086] [G loss: -5.314285]\n",
      "[Epoch 41/200] [Batch 0/12] [D loss: -1.242063] [G loss: -6.042475]\n",
      "[Epoch 41/200] [Batch 5/12] [D loss: -3.308316] [G loss: -6.755085]\n",
      "[Epoch 41/200] [Batch 10/12] [D loss: -2.825280] [G loss: -6.014778]\n",
      "[Epoch 42/200] [Batch 0/12] [D loss: -2.308748] [G loss: -6.653695]\n",
      "[Epoch 42/200] [Batch 5/12] [D loss: -1.798419] [G loss: -7.257292]\n",
      "[Epoch 42/200] [Batch 10/12] [D loss: -3.292349] [G loss: -5.629210]\n",
      "[Epoch 43/200] [Batch 0/12] [D loss: -2.688496] [G loss: -6.590970]\n",
      "[Epoch 43/200] [Batch 5/12] [D loss: -2.410702] [G loss: -4.996682]\n",
      "[Epoch 43/200] [Batch 10/12] [D loss: -4.094598] [G loss: -5.373115]\n",
      "[Epoch 44/200] [Batch 0/12] [D loss: -4.036492] [G loss: -6.456110]\n",
      "[Epoch 44/200] [Batch 5/12] [D loss: -2.835443] [G loss: -6.210073]\n",
      "[Epoch 44/200] [Batch 10/12] [D loss: -2.295404] [G loss: -7.193883]\n",
      "[Epoch 45/200] [Batch 0/12] [D loss: -2.317411] [G loss: -5.916844]\n",
      "[Epoch 45/200] [Batch 5/12] [D loss: -3.647234] [G loss: -6.156775]\n",
      "[Epoch 45/200] [Batch 10/12] [D loss: -2.897945] [G loss: -5.855268]\n",
      "[Epoch 46/200] [Batch 0/12] [D loss: -3.611685] [G loss: -8.226472]\n",
      "[Epoch 46/200] [Batch 5/12] [D loss: -2.837955] [G loss: -6.905013]\n",
      "[Epoch 46/200] [Batch 10/12] [D loss: -4.827468] [G loss: -6.240850]\n",
      "[Epoch 47/200] [Batch 0/12] [D loss: -3.863476] [G loss: -7.121018]\n",
      "[Epoch 47/200] [Batch 5/12] [D loss: -1.350079] [G loss: -7.152635]\n",
      "[Epoch 47/200] [Batch 10/12] [D loss: -3.288484] [G loss: -6.527788]\n",
      "[Epoch 48/200] [Batch 0/12] [D loss: -3.414012] [G loss: -7.824725]\n",
      "[Epoch 48/200] [Batch 5/12] [D loss: -2.234633] [G loss: -7.927534]\n",
      "[Epoch 48/200] [Batch 10/12] [D loss: -3.040363] [G loss: -8.068151]\n",
      "[Epoch 49/200] [Batch 0/12] [D loss: -3.463262] [G loss: -8.855486]\n",
      "[Epoch 49/200] [Batch 5/12] [D loss: -2.685952] [G loss: -8.141762]\n",
      "[Epoch 49/200] [Batch 10/12] [D loss: -2.229670] [G loss: -7.237804]\n",
      "[Epoch 50/200] [Batch 0/12] [D loss: -3.825429] [G loss: -6.413142]\n",
      "[Epoch 50/200] [Batch 5/12] [D loss: -3.212777] [G loss: -7.205554]\n",
      "[Epoch 50/200] [Batch 10/12] [D loss: -2.510935] [G loss: -9.321281]\n",
      "[Epoch 51/200] [Batch 0/12] [D loss: -3.829730] [G loss: -8.416468]\n",
      "[Epoch 51/200] [Batch 5/12] [D loss: -2.261222] [G loss: -8.646055]\n",
      "[Epoch 51/200] [Batch 10/12] [D loss: -3.530041] [G loss: -7.503122]\n",
      "[Epoch 52/200] [Batch 0/12] [D loss: -4.554401] [G loss: -8.757099]\n",
      "[Epoch 52/200] [Batch 5/12] [D loss: -4.174999] [G loss: -8.582491]\n",
      "[Epoch 52/200] [Batch 10/12] [D loss: -3.316470] [G loss: -9.117985]\n",
      "[Epoch 53/200] [Batch 0/12] [D loss: -3.532451] [G loss: -9.230720]\n",
      "[Epoch 53/200] [Batch 5/12] [D loss: -3.652715] [G loss: -9.631169]\n",
      "[Epoch 53/200] [Batch 10/12] [D loss: -4.988713] [G loss: -8.829187]\n",
      "[Epoch 54/200] [Batch 0/12] [D loss: -4.217527] [G loss: -9.423870]\n",
      "[Epoch 54/200] [Batch 5/12] [D loss: -3.648940] [G loss: -9.468327]\n",
      "[Epoch 54/200] [Batch 10/12] [D loss: -4.603415] [G loss: -9.788164]\n",
      "[Epoch 55/200] [Batch 0/12] [D loss: -3.303416] [G loss: -7.516234]\n",
      "[Epoch 55/200] [Batch 5/12] [D loss: -5.413701] [G loss: -7.342178]\n",
      "[Epoch 55/200] [Batch 10/12] [D loss: -3.773218] [G loss: -8.240612]\n",
      "[Epoch 56/200] [Batch 0/12] [D loss: -5.616604] [G loss: -8.153499]\n",
      "[Epoch 56/200] [Batch 5/12] [D loss: -3.784395] [G loss: -7.906329]\n",
      "[Epoch 56/200] [Batch 10/12] [D loss: -4.975245] [G loss: -8.181816]\n",
      "[Epoch 57/200] [Batch 0/12] [D loss: -5.564424] [G loss: -7.577877]\n",
      "[Epoch 57/200] [Batch 5/12] [D loss: -6.545306] [G loss: -8.517791]\n",
      "[Epoch 57/200] [Batch 10/12] [D loss: -3.702767] [G loss: -9.738155]\n",
      "[Epoch 58/200] [Batch 0/12] [D loss: -3.789851] [G loss: -9.486016]\n",
      "[Epoch 58/200] [Batch 5/12] [D loss: -1.882384] [G loss: -10.557484]\n",
      "[Epoch 58/200] [Batch 10/12] [D loss: -4.082992] [G loss: -8.721715]\n",
      "[Epoch 59/200] [Batch 0/12] [D loss: -5.030734] [G loss: -9.540734]\n",
      "[Epoch 59/200] [Batch 5/12] [D loss: -5.255909] [G loss: -9.611398]\n",
      "[Epoch 59/200] [Batch 10/12] [D loss: -7.615376] [G loss: -6.873867]\n",
      "[Epoch 60/200] [Batch 0/12] [D loss: -5.255419] [G loss: -9.515093]\n",
      "[Epoch 60/200] [Batch 5/12] [D loss: -4.160859] [G loss: -8.620323]\n",
      "[Epoch 60/200] [Batch 10/12] [D loss: -8.486935] [G loss: -9.788394]\n",
      "[Epoch 61/200] [Batch 0/12] [D loss: -4.907758] [G loss: -9.396579]\n",
      "[Epoch 61/200] [Batch 5/12] [D loss: -5.165866] [G loss: -10.880883]\n",
      "[Epoch 61/200] [Batch 10/12] [D loss: -3.769733] [G loss: -10.601567]\n",
      "[Epoch 62/200] [Batch 0/12] [D loss: -6.087971] [G loss: -9.021147]\n",
      "[Epoch 62/200] [Batch 5/12] [D loss: -4.263964] [G loss: -8.450240]\n",
      "[Epoch 62/200] [Batch 10/12] [D loss: -6.246017] [G loss: -10.512899]\n",
      "[Epoch 63/200] [Batch 0/12] [D loss: -7.340670] [G loss: -8.691044]\n",
      "[Epoch 63/200] [Batch 5/12] [D loss: -5.383597] [G loss: -9.463903]\n",
      "[Epoch 63/200] [Batch 10/12] [D loss: -6.732022] [G loss: -9.890717]\n",
      "[Epoch 64/200] [Batch 0/12] [D loss: -5.842093] [G loss: -8.599746]\n",
      "[Epoch 64/200] [Batch 5/12] [D loss: -8.156292] [G loss: -10.079885]\n",
      "[Epoch 64/200] [Batch 10/12] [D loss: -6.609970] [G loss: -10.112864]\n",
      "[Epoch 65/200] [Batch 0/12] [D loss: -6.656576] [G loss: -7.891351]\n",
      "[Epoch 65/200] [Batch 5/12] [D loss: -7.578165] [G loss: -11.048954]\n",
      "[Epoch 65/200] [Batch 10/12] [D loss: -5.629879] [G loss: -11.056725]\n",
      "[Epoch 66/200] [Batch 0/12] [D loss: -7.813921] [G loss: -10.759443]\n",
      "[Epoch 66/200] [Batch 5/12] [D loss: -3.395770] [G loss: -12.038660]\n",
      "[Epoch 66/200] [Batch 10/12] [D loss: -4.743991] [G loss: -10.770893]\n",
      "[Epoch 67/200] [Batch 0/12] [D loss: -4.598638] [G loss: -11.146227]\n",
      "[Epoch 67/200] [Batch 5/12] [D loss: -5.637924] [G loss: -9.232178]\n",
      "[Epoch 67/200] [Batch 10/12] [D loss: -5.521033] [G loss: -9.521799]\n",
      "[Epoch 68/200] [Batch 0/12] [D loss: -7.995059] [G loss: -11.322443]\n",
      "[Epoch 68/200] [Batch 5/12] [D loss: -6.207742] [G loss: -10.115213]\n",
      "[Epoch 68/200] [Batch 10/12] [D loss: -4.216340] [G loss: -9.924387]\n",
      "[Epoch 69/200] [Batch 0/12] [D loss: -2.841573] [G loss: -11.551456]\n",
      "[Epoch 69/200] [Batch 5/12] [D loss: -5.738609] [G loss: -11.191366]\n",
      "[Epoch 69/200] [Batch 10/12] [D loss: -4.652542] [G loss: -10.670875]\n",
      "[Epoch 70/200] [Batch 0/12] [D loss: -6.959833] [G loss: -10.939796]\n",
      "[Epoch 70/200] [Batch 5/12] [D loss: -6.088408] [G loss: -12.478378]\n",
      "[Epoch 70/200] [Batch 10/12] [D loss: -7.234290] [G loss: -9.722841]\n",
      "[Epoch 71/200] [Batch 0/12] [D loss: -4.038532] [G loss: -13.731173]\n",
      "[Epoch 71/200] [Batch 5/12] [D loss: -8.060578] [G loss: -11.948436]\n",
      "[Epoch 71/200] [Batch 10/12] [D loss: -8.705464] [G loss: -12.762527]\n",
      "[Epoch 72/200] [Batch 0/12] [D loss: -2.808639] [G loss: -13.032560]\n",
      "[Epoch 72/200] [Batch 5/12] [D loss: -5.045108] [G loss: -12.288944]\n",
      "[Epoch 72/200] [Batch 10/12] [D loss: -9.925741] [G loss: -10.095104]\n",
      "[Epoch 73/200] [Batch 0/12] [D loss: -6.649543] [G loss: -12.362888]\n",
      "[Epoch 73/200] [Batch 5/12] [D loss: -5.235264] [G loss: -12.424293]\n",
      "[Epoch 73/200] [Batch 10/12] [D loss: -4.075474] [G loss: -12.022438]\n",
      "[Epoch 74/200] [Batch 0/12] [D loss: -5.883473] [G loss: -13.587185]\n",
      "[Epoch 74/200] [Batch 5/12] [D loss: -6.578621] [G loss: -12.183898]\n",
      "[Epoch 74/200] [Batch 10/12] [D loss: -6.012505] [G loss: -12.457500]\n",
      "[Epoch 75/200] [Batch 0/12] [D loss: -6.662930] [G loss: -12.134430]\n",
      "[Epoch 75/200] [Batch 5/12] [D loss: -5.835764] [G loss: -12.624458]\n",
      "[Epoch 75/200] [Batch 10/12] [D loss: -5.799575] [G loss: -11.764191]\n",
      "[Epoch 76/200] [Batch 0/12] [D loss: -6.046338] [G loss: -13.511857]\n",
      "[Epoch 76/200] [Batch 5/12] [D loss: -5.641345] [G loss: -12.614562]\n",
      "[Epoch 76/200] [Batch 10/12] [D loss: -5.805799] [G loss: -13.037759]\n",
      "[Epoch 77/200] [Batch 0/12] [D loss: -8.695689] [G loss: -13.819890]\n",
      "[Epoch 77/200] [Batch 5/12] [D loss: -6.021516] [G loss: -15.150867]\n",
      "[Epoch 77/200] [Batch 10/12] [D loss: -8.399633] [G loss: -12.946188]\n",
      "[Epoch 78/200] [Batch 0/12] [D loss: -3.831847] [G loss: -12.307278]\n",
      "[Epoch 78/200] [Batch 5/12] [D loss: -5.977244] [G loss: -15.657952]\n",
      "[Epoch 78/200] [Batch 10/12] [D loss: -7.350793] [G loss: -15.731379]\n",
      "[Epoch 79/200] [Batch 0/12] [D loss: -7.017366] [G loss: -17.371174]\n",
      "[Epoch 79/200] [Batch 5/12] [D loss: -5.668937] [G loss: -13.892958]\n",
      "[Epoch 79/200] [Batch 10/12] [D loss: -6.557493] [G loss: -13.917549]\n",
      "[Epoch 80/200] [Batch 0/12] [D loss: -6.575811] [G loss: -12.999029]\n",
      "[Epoch 80/200] [Batch 5/12] [D loss: -9.309733] [G loss: -15.273069]\n",
      "[Epoch 80/200] [Batch 10/12] [D loss: -10.202634] [G loss: -11.816308]\n",
      "[Epoch 81/200] [Batch 0/12] [D loss: -7.991293] [G loss: -17.908392]\n",
      "[Epoch 81/200] [Batch 5/12] [D loss: -7.711763] [G loss: -17.413956]\n",
      "[Epoch 81/200] [Batch 10/12] [D loss: -8.342163] [G loss: -16.629393]\n",
      "[Epoch 82/200] [Batch 0/12] [D loss: -8.918245] [G loss: -18.495852]\n",
      "[Epoch 82/200] [Batch 5/12] [D loss: -8.080027] [G loss: -18.142900]\n",
      "[Epoch 82/200] [Batch 10/12] [D loss: -5.736540] [G loss: -15.895817]\n",
      "[Epoch 83/200] [Batch 0/12] [D loss: -7.770934] [G loss: -17.822165]\n",
      "[Epoch 83/200] [Batch 5/12] [D loss: -4.380179] [G loss: -14.443436]\n",
      "[Epoch 83/200] [Batch 10/12] [D loss: -5.604198] [G loss: -15.439388]\n",
      "[Epoch 84/200] [Batch 0/12] [D loss: -7.929504] [G loss: -19.300369]\n",
      "[Epoch 84/200] [Batch 5/12] [D loss: -3.969672] [G loss: -16.766888]\n",
      "[Epoch 84/200] [Batch 10/12] [D loss: -6.659863] [G loss: -15.163972]\n",
      "[Epoch 85/200] [Batch 0/12] [D loss: -1.986098] [G loss: -17.557531]\n",
      "[Epoch 85/200] [Batch 5/12] [D loss: -5.378103] [G loss: -18.591808]\n",
      "[Epoch 85/200] [Batch 10/12] [D loss: -9.242455] [G loss: -20.278931]\n",
      "[Epoch 86/200] [Batch 0/12] [D loss: -7.700507] [G loss: -22.090019]\n",
      "[Epoch 86/200] [Batch 5/12] [D loss: -13.926882] [G loss: -15.057102]\n",
      "[Epoch 86/200] [Batch 10/12] [D loss: -7.919510] [G loss: -19.201191]\n",
      "[Epoch 87/200] [Batch 0/12] [D loss: -6.332357] [G loss: -19.291115]\n",
      "[Epoch 87/200] [Batch 5/12] [D loss: -5.298314] [G loss: -19.516624]\n",
      "[Epoch 87/200] [Batch 10/12] [D loss: -8.958657] [G loss: -15.668427]\n",
      "[Epoch 88/200] [Batch 0/12] [D loss: -4.230902] [G loss: -20.005178]\n",
      "[Epoch 88/200] [Batch 5/12] [D loss: -8.643373] [G loss: -17.744823]\n",
      "[Epoch 88/200] [Batch 10/12] [D loss: -12.444959] [G loss: -16.459896]\n",
      "[Epoch 89/200] [Batch 0/12] [D loss: -5.850688] [G loss: -20.885227]\n",
      "[Epoch 89/200] [Batch 5/12] [D loss: -4.857216] [G loss: -17.611370]\n",
      "[Epoch 89/200] [Batch 10/12] [D loss: -5.652805] [G loss: -17.992411]\n",
      "[Epoch 90/200] [Batch 0/12] [D loss: -8.581370] [G loss: -17.328802]\n",
      "[Epoch 90/200] [Batch 5/12] [D loss: -9.569265] [G loss: -18.440430]\n",
      "[Epoch 90/200] [Batch 10/12] [D loss: -8.055284] [G loss: -20.145947]\n",
      "[Epoch 91/200] [Batch 0/12] [D loss: -5.177446] [G loss: -17.518642]\n",
      "[Epoch 91/200] [Batch 5/12] [D loss: -7.227099] [G loss: -19.411047]\n",
      "[Epoch 91/200] [Batch 10/12] [D loss: -7.927898] [G loss: -20.406427]\n",
      "[Epoch 92/200] [Batch 0/12] [D loss: -6.893925] [G loss: -21.735031]\n",
      "[Epoch 92/200] [Batch 5/12] [D loss: -9.534757] [G loss: -18.402397]\n",
      "[Epoch 92/200] [Batch 10/12] [D loss: -9.064017] [G loss: -22.939928]\n",
      "[Epoch 93/200] [Batch 0/12] [D loss: -5.389844] [G loss: -21.198761]\n",
      "[Epoch 93/200] [Batch 5/12] [D loss: -5.658007] [G loss: -20.839907]\n",
      "[Epoch 93/200] [Batch 10/12] [D loss: -6.767990] [G loss: -22.435070]\n",
      "[Epoch 94/200] [Batch 0/12] [D loss: -10.537658] [G loss: -22.771030]\n",
      "[Epoch 94/200] [Batch 5/12] [D loss: -6.825886] [G loss: -24.445114]\n",
      "[Epoch 94/200] [Batch 10/12] [D loss: -10.525682] [G loss: -24.323490]\n",
      "[Epoch 95/200] [Batch 0/12] [D loss: -5.917907] [G loss: -20.164917]\n",
      "[Epoch 95/200] [Batch 5/12] [D loss: -8.088664] [G loss: -21.955894]\n",
      "[Epoch 95/200] [Batch 10/12] [D loss: -10.446316] [G loss: -17.178684]\n",
      "[Epoch 96/200] [Batch 0/12] [D loss: -7.609986] [G loss: -20.773430]\n",
      "[Epoch 96/200] [Batch 5/12] [D loss: -8.076675] [G loss: -23.959625]\n",
      "[Epoch 96/200] [Batch 10/12] [D loss: -7.649086] [G loss: -22.056192]\n",
      "[Epoch 97/200] [Batch 0/12] [D loss: -9.386905] [G loss: -20.486549]\n",
      "[Epoch 97/200] [Batch 5/12] [D loss: -6.420061] [G loss: -20.311460]\n",
      "[Epoch 97/200] [Batch 10/12] [D loss: -5.030504] [G loss: -17.901640]\n",
      "[Epoch 98/200] [Batch 0/12] [D loss: -5.023815] [G loss: -18.000486]\n",
      "[Epoch 98/200] [Batch 5/12] [D loss: -8.276704] [G loss: -20.116186]\n",
      "[Epoch 98/200] [Batch 10/12] [D loss: -10.767073] [G loss: -20.304893]\n",
      "[Epoch 99/200] [Batch 0/12] [D loss: -8.189207] [G loss: -20.843338]\n",
      "[Epoch 99/200] [Batch 5/12] [D loss: -4.522871] [G loss: -23.625168]\n",
      "[Epoch 99/200] [Batch 10/12] [D loss: -9.777949] [G loss: -16.567139]\n",
      "[Epoch 100/200] [Batch 0/12] [D loss: -9.566895] [G loss: -22.684036]\n",
      "[Epoch 100/200] [Batch 5/12] [D loss: -5.680445] [G loss: -23.173088]\n",
      "[Epoch 100/200] [Batch 10/12] [D loss: -8.497440] [G loss: -18.758003]\n",
      "[Epoch 101/200] [Batch 0/12] [D loss: -5.722210] [G loss: -21.591209]\n",
      "[Epoch 101/200] [Batch 5/12] [D loss: -2.625259] [G loss: -22.664507]\n",
      "[Epoch 101/200] [Batch 10/12] [D loss: -8.845385] [G loss: -21.758400]\n",
      "[Epoch 102/200] [Batch 0/12] [D loss: -5.316455] [G loss: -23.962801]\n",
      "[Epoch 102/200] [Batch 5/12] [D loss: -7.025072] [G loss: -27.076677]\n",
      "[Epoch 102/200] [Batch 10/12] [D loss: -13.892197] [G loss: -18.447767]\n",
      "[Epoch 103/200] [Batch 0/12] [D loss: -8.198130] [G loss: -21.816942]\n",
      "[Epoch 103/200] [Batch 5/12] [D loss: -5.606879] [G loss: -20.938986]\n",
      "[Epoch 103/200] [Batch 10/12] [D loss: -6.624876] [G loss: -22.978970]\n",
      "[Epoch 104/200] [Batch 0/12] [D loss: -11.312881] [G loss: -23.728434]\n",
      "[Epoch 104/200] [Batch 5/12] [D loss: -2.619732] [G loss: -25.589832]\n",
      "[Epoch 104/200] [Batch 10/12] [D loss: -8.081789] [G loss: -27.105576]\n",
      "[Epoch 105/200] [Batch 0/12] [D loss: -7.655940] [G loss: -26.478756]\n",
      "[Epoch 105/200] [Batch 5/12] [D loss: -4.773133] [G loss: -24.193066]\n",
      "[Epoch 105/200] [Batch 10/12] [D loss: -5.672297] [G loss: -21.830242]\n",
      "[Epoch 106/200] [Batch 0/12] [D loss: -6.596987] [G loss: -26.558668]\n",
      "[Epoch 106/200] [Batch 5/12] [D loss: -3.551667] [G loss: -23.618578]\n",
      "[Epoch 106/200] [Batch 10/12] [D loss: -6.750289] [G loss: -23.484261]\n",
      "[Epoch 107/200] [Batch 0/12] [D loss: -5.513267] [G loss: -19.613749]\n",
      "[Epoch 107/200] [Batch 5/12] [D loss: -9.006401] [G loss: -21.301844]\n",
      "[Epoch 107/200] [Batch 10/12] [D loss: -7.462480] [G loss: -23.009573]\n",
      "[Epoch 108/200] [Batch 0/12] [D loss: -13.502317] [G loss: -21.611671]\n",
      "[Epoch 108/200] [Batch 5/12] [D loss: -8.504889] [G loss: -22.348461]\n",
      "[Epoch 108/200] [Batch 10/12] [D loss: -6.377343] [G loss: -22.462563]\n",
      "[Epoch 109/200] [Batch 0/12] [D loss: -5.818997] [G loss: -21.470209]\n",
      "[Epoch 109/200] [Batch 5/12] [D loss: -8.448622] [G loss: -21.387959]\n",
      "[Epoch 109/200] [Batch 10/12] [D loss: -3.928319] [G loss: -26.031282]\n",
      "[Epoch 110/200] [Batch 0/12] [D loss: -8.635185] [G loss: -23.237572]\n",
      "[Epoch 110/200] [Batch 5/12] [D loss: -13.957185] [G loss: -20.160526]\n",
      "[Epoch 110/200] [Batch 10/12] [D loss: -6.891373] [G loss: -22.353230]\n",
      "[Epoch 111/200] [Batch 0/12] [D loss: -5.867414] [G loss: -27.151987]\n",
      "[Epoch 111/200] [Batch 5/12] [D loss: 1.923408] [G loss: -25.583691]\n",
      "[Epoch 111/200] [Batch 10/12] [D loss: -5.286783] [G loss: -19.554939]\n",
      "[Epoch 112/200] [Batch 0/12] [D loss: -0.915145] [G loss: -24.600807]\n",
      "[Epoch 112/200] [Batch 5/12] [D loss: -2.963376] [G loss: -25.108278]\n",
      "[Epoch 112/200] [Batch 10/12] [D loss: -8.584790] [G loss: -25.002954]\n",
      "[Epoch 113/200] [Batch 0/12] [D loss: -3.631854] [G loss: -23.993038]\n",
      "[Epoch 113/200] [Batch 5/12] [D loss: -9.267625] [G loss: -22.083122]\n",
      "[Epoch 113/200] [Batch 10/12] [D loss: -12.823802] [G loss: -19.037357]\n",
      "[Epoch 114/200] [Batch 0/12] [D loss: -9.552889] [G loss: -21.243425]\n",
      "[Epoch 114/200] [Batch 5/12] [D loss: -6.697513] [G loss: -29.498701]\n",
      "[Epoch 114/200] [Batch 10/12] [D loss: -7.445045] [G loss: -23.377665]\n",
      "[Epoch 115/200] [Batch 0/12] [D loss: -12.213387] [G loss: -27.216030]\n",
      "[Epoch 115/200] [Batch 5/12] [D loss: -3.919476] [G loss: -27.976776]\n",
      "[Epoch 115/200] [Batch 10/12] [D loss: -9.078448] [G loss: -23.735781]\n",
      "[Epoch 116/200] [Batch 0/12] [D loss: -7.665856] [G loss: -28.594620]\n",
      "[Epoch 116/200] [Batch 5/12] [D loss: -7.166147] [G loss: -24.591766]\n",
      "[Epoch 116/200] [Batch 10/12] [D loss: -4.976765] [G loss: -22.068668]\n",
      "[Epoch 117/200] [Batch 0/12] [D loss: -8.445073] [G loss: -26.156136]\n",
      "[Epoch 117/200] [Batch 5/12] [D loss: -5.682499] [G loss: -23.357658]\n",
      "[Epoch 117/200] [Batch 10/12] [D loss: -3.192054] [G loss: -23.308830]\n",
      "[Epoch 118/200] [Batch 0/12] [D loss: -1.170892] [G loss: -21.425941]\n",
      "[Epoch 118/200] [Batch 5/12] [D loss: -4.534320] [G loss: -20.527586]\n",
      "[Epoch 118/200] [Batch 10/12] [D loss: -9.013358] [G loss: -20.099205]\n",
      "[Epoch 119/200] [Batch 0/12] [D loss: -7.372980] [G loss: -20.138590]\n",
      "[Epoch 119/200] [Batch 5/12] [D loss: -4.645968] [G loss: -20.415867]\n",
      "[Epoch 119/200] [Batch 10/12] [D loss: -8.691345] [G loss: -27.648722]\n",
      "[Epoch 120/200] [Batch 0/12] [D loss: -9.145298] [G loss: -19.944178]\n",
      "[Epoch 120/200] [Batch 5/12] [D loss: -6.152799] [G loss: -24.721184]\n",
      "[Epoch 120/200] [Batch 10/12] [D loss: -8.055847] [G loss: -28.917192]\n",
      "[Epoch 121/200] [Batch 0/12] [D loss: -8.596780] [G loss: -23.656836]\n",
      "[Epoch 121/200] [Batch 5/12] [D loss: -15.622116] [G loss: -28.011683]\n",
      "[Epoch 121/200] [Batch 10/12] [D loss: -10.124356] [G loss: -24.863413]\n",
      "[Epoch 122/200] [Batch 0/12] [D loss: -7.327529] [G loss: -24.457602]\n",
      "[Epoch 122/200] [Batch 5/12] [D loss: -7.862775] [G loss: -25.876568]\n",
      "[Epoch 122/200] [Batch 10/12] [D loss: -12.553232] [G loss: -22.474915]\n",
      "[Epoch 123/200] [Batch 0/12] [D loss: -9.463528] [G loss: -27.545563]\n",
      "[Epoch 123/200] [Batch 5/12] [D loss: -9.616617] [G loss: -26.075367]\n",
      "[Epoch 123/200] [Batch 10/12] [D loss: -4.782337] [G loss: -26.172680]\n",
      "[Epoch 124/200] [Batch 0/12] [D loss: -6.696728] [G loss: -27.009367]\n",
      "[Epoch 124/200] [Batch 5/12] [D loss: -6.750195] [G loss: -22.805531]\n",
      "[Epoch 124/200] [Batch 10/12] [D loss: -6.474517] [G loss: -25.387455]\n",
      "[Epoch 125/200] [Batch 0/12] [D loss: -10.353546] [G loss: -26.977682]\n",
      "[Epoch 125/200] [Batch 5/12] [D loss: -17.561951] [G loss: -21.001171]\n",
      "[Epoch 125/200] [Batch 10/12] [D loss: -7.987147] [G loss: -21.546421]\n",
      "[Epoch 126/200] [Batch 0/12] [D loss: -10.203283] [G loss: -27.993795]\n",
      "[Epoch 126/200] [Batch 5/12] [D loss: -11.831221] [G loss: -22.096268]\n",
      "[Epoch 126/200] [Batch 10/12] [D loss: -13.846450] [G loss: -17.605503]\n",
      "[Epoch 127/200] [Batch 0/12] [D loss: -7.030781] [G loss: -27.285309]\n",
      "[Epoch 127/200] [Batch 5/12] [D loss: -7.663148] [G loss: -30.002499]\n",
      "[Epoch 127/200] [Batch 10/12] [D loss: -9.401738] [G loss: -21.697121]\n",
      "[Epoch 128/200] [Batch 0/12] [D loss: -0.620687] [G loss: -25.458902]\n",
      "[Epoch 128/200] [Batch 5/12] [D loss: -4.531833] [G loss: -27.136402]\n",
      "[Epoch 128/200] [Batch 10/12] [D loss: -6.581707] [G loss: -27.599476]\n",
      "[Epoch 129/200] [Batch 0/12] [D loss: -4.758475] [G loss: -28.702530]\n",
      "[Epoch 129/200] [Batch 5/12] [D loss: -7.506860] [G loss: -23.335081]\n",
      "[Epoch 129/200] [Batch 10/12] [D loss: -3.342724] [G loss: -28.631876]\n",
      "[Epoch 130/200] [Batch 0/12] [D loss: -5.751094] [G loss: -29.042042]\n",
      "[Epoch 130/200] [Batch 5/12] [D loss: -8.993148] [G loss: -26.533976]\n",
      "[Epoch 130/200] [Batch 10/12] [D loss: -9.866037] [G loss: -26.307959]\n",
      "[Epoch 131/200] [Batch 0/12] [D loss: -6.914085] [G loss: -27.145273]\n",
      "[Epoch 131/200] [Batch 5/12] [D loss: -6.919182] [G loss: -28.885899]\n",
      "[Epoch 131/200] [Batch 10/12] [D loss: -4.012417] [G loss: -25.297899]\n",
      "[Epoch 132/200] [Batch 0/12] [D loss: -7.713116] [G loss: -29.981339]\n",
      "[Epoch 132/200] [Batch 5/12] [D loss: -6.386634] [G loss: -29.407104]\n",
      "[Epoch 132/200] [Batch 10/12] [D loss: -5.468791] [G loss: -25.242203]\n",
      "[Epoch 133/200] [Batch 0/12] [D loss: -11.203826] [G loss: -30.290058]\n",
      "[Epoch 133/200] [Batch 5/12] [D loss: -8.614773] [G loss: -26.310852]\n",
      "[Epoch 133/200] [Batch 10/12] [D loss: -0.409862] [G loss: -27.610376]\n",
      "[Epoch 134/200] [Batch 0/12] [D loss: -5.348965] [G loss: -27.730230]\n",
      "[Epoch 134/200] [Batch 5/12] [D loss: -4.300078] [G loss: -30.795231]\n",
      "[Epoch 134/200] [Batch 10/12] [D loss: -4.494509] [G loss: -30.215534]\n",
      "[Epoch 135/200] [Batch 0/12] [D loss: -9.167996] [G loss: -24.609104]\n",
      "[Epoch 135/200] [Batch 5/12] [D loss: -9.991548] [G loss: -24.112036]\n",
      "[Epoch 135/200] [Batch 10/12] [D loss: -4.794270] [G loss: -25.668430]\n",
      "[Epoch 136/200] [Batch 0/12] [D loss: -10.495358] [G loss: -23.814983]\n",
      "[Epoch 136/200] [Batch 5/12] [D loss: -6.693636] [G loss: -23.247612]\n",
      "[Epoch 136/200] [Batch 10/12] [D loss: -9.695817] [G loss: -27.252720]\n",
      "[Epoch 137/200] [Batch 0/12] [D loss: -7.429480] [G loss: -28.171263]\n",
      "[Epoch 137/200] [Batch 5/12] [D loss: -3.694743] [G loss: -26.838127]\n",
      "[Epoch 137/200] [Batch 10/12] [D loss: -11.826969] [G loss: -26.668283]\n",
      "[Epoch 138/200] [Batch 0/12] [D loss: -7.322658] [G loss: -27.301731]\n",
      "[Epoch 138/200] [Batch 5/12] [D loss: -8.872515] [G loss: -27.001884]\n",
      "[Epoch 138/200] [Batch 10/12] [D loss: -13.118053] [G loss: -26.582329]\n",
      "[Epoch 139/200] [Batch 0/12] [D loss: -6.931292] [G loss: -27.357031]\n",
      "[Epoch 139/200] [Batch 5/12] [D loss: -11.389582] [G loss: -25.611176]\n",
      "[Epoch 139/200] [Batch 10/12] [D loss: -4.399206] [G loss: -30.294376]\n",
      "[Epoch 140/200] [Batch 0/12] [D loss: -2.326701] [G loss: -29.077587]\n",
      "[Epoch 140/200] [Batch 5/12] [D loss: -6.176825] [G loss: -26.766621]\n",
      "[Epoch 140/200] [Batch 10/12] [D loss: -10.139183] [G loss: -29.579020]\n",
      "[Epoch 141/200] [Batch 0/12] [D loss: -6.061530] [G loss: -28.287464]\n",
      "[Epoch 141/200] [Batch 5/12] [D loss: -5.287410] [G loss: -25.290596]\n",
      "[Epoch 141/200] [Batch 10/12] [D loss: -6.598251] [G loss: -32.445114]\n",
      "[Epoch 142/200] [Batch 0/12] [D loss: -10.034822] [G loss: -25.546741]\n",
      "[Epoch 142/200] [Batch 5/12] [D loss: -9.643749] [G loss: -28.666580]\n",
      "[Epoch 142/200] [Batch 10/12] [D loss: -7.408095] [G loss: -27.104618]\n",
      "[Epoch 143/200] [Batch 0/12] [D loss: -9.223736] [G loss: -29.665464]\n",
      "[Epoch 143/200] [Batch 5/12] [D loss: -10.722869] [G loss: -29.182690]\n",
      "[Epoch 143/200] [Batch 10/12] [D loss: -4.431509] [G loss: -27.914993]\n",
      "[Epoch 144/200] [Batch 0/12] [D loss: -9.228738] [G loss: -25.146423]\n",
      "[Epoch 144/200] [Batch 5/12] [D loss: -3.326529] [G loss: -25.306555]\n",
      "[Epoch 144/200] [Batch 10/12] [D loss: -5.264456] [G loss: -31.377832]\n",
      "[Epoch 145/200] [Batch 0/12] [D loss: -4.779728] [G loss: -28.719526]\n",
      "[Epoch 145/200] [Batch 5/12] [D loss: -10.160114] [G loss: -31.634750]\n",
      "[Epoch 145/200] [Batch 10/12] [D loss: -2.539566] [G loss: -30.940109]\n",
      "[Epoch 146/200] [Batch 0/12] [D loss: -6.820637] [G loss: -29.694387]\n",
      "[Epoch 146/200] [Batch 5/12] [D loss: -3.293041] [G loss: -29.303337]\n",
      "[Epoch 146/200] [Batch 10/12] [D loss: -7.543583] [G loss: -23.152599]\n",
      "[Epoch 147/200] [Batch 0/12] [D loss: -11.232315] [G loss: -30.972450]\n",
      "[Epoch 147/200] [Batch 5/12] [D loss: -11.132108] [G loss: -24.936144]\n",
      "[Epoch 147/200] [Batch 10/12] [D loss: -11.598886] [G loss: -22.862057]\n",
      "[Epoch 148/200] [Batch 0/12] [D loss: -3.129772] [G loss: -29.255882]\n",
      "[Epoch 148/200] [Batch 5/12] [D loss: -8.372370] [G loss: -26.194410]\n",
      "[Epoch 148/200] [Batch 10/12] [D loss: -8.006109] [G loss: -23.125463]\n",
      "[Epoch 149/200] [Batch 0/12] [D loss: -6.006096] [G loss: -26.968365]\n",
      "[Epoch 149/200] [Batch 5/12] [D loss: -12.214846] [G loss: -24.346888]\n",
      "[Epoch 149/200] [Batch 10/12] [D loss: -16.431623] [G loss: -29.480953]\n",
      "[Epoch 150/200] [Batch 0/12] [D loss: -4.219293] [G loss: -31.903353]\n",
      "[Epoch 150/200] [Batch 5/12] [D loss: -6.571192] [G loss: -24.859138]\n",
      "[Epoch 150/200] [Batch 10/12] [D loss: -2.586138] [G loss: -25.544603]\n",
      "[Epoch 151/200] [Batch 0/12] [D loss: -13.234756] [G loss: -22.510132]\n",
      "[Epoch 151/200] [Batch 5/12] [D loss: -9.288984] [G loss: -22.564875]\n",
      "[Epoch 151/200] [Batch 10/12] [D loss: -11.559710] [G loss: -20.362101]\n",
      "[Epoch 152/200] [Batch 0/12] [D loss: -9.431440] [G loss: -25.518353]\n",
      "[Epoch 152/200] [Batch 5/12] [D loss: -9.213414] [G loss: -26.105936]\n",
      "[Epoch 152/200] [Batch 10/12] [D loss: -12.573339] [G loss: -29.203789]\n",
      "[Epoch 153/200] [Batch 0/12] [D loss: -7.869831] [G loss: -25.356831]\n",
      "[Epoch 153/200] [Batch 5/12] [D loss: -12.753708] [G loss: -27.614101]\n",
      "[Epoch 153/200] [Batch 10/12] [D loss: -8.210699] [G loss: -26.035992]\n",
      "[Epoch 154/200] [Batch 0/12] [D loss: -12.564962] [G loss: -27.269262]\n",
      "[Epoch 154/200] [Batch 5/12] [D loss: -3.952792] [G loss: -30.267057]\n",
      "[Epoch 154/200] [Batch 10/12] [D loss: -10.803711] [G loss: -24.221458]\n",
      "[Epoch 155/200] [Batch 0/12] [D loss: -12.610161] [G loss: -24.986570]\n",
      "[Epoch 155/200] [Batch 5/12] [D loss: -5.619468] [G loss: -27.550404]\n",
      "[Epoch 155/200] [Batch 10/12] [D loss: -4.608439] [G loss: -24.258724]\n",
      "[Epoch 156/200] [Batch 0/12] [D loss: -12.437838] [G loss: -24.596375]\n",
      "[Epoch 156/200] [Batch 5/12] [D loss: -6.800481] [G loss: -28.189709]\n",
      "[Epoch 156/200] [Batch 10/12] [D loss: -15.611385] [G loss: -24.741301]\n",
      "[Epoch 157/200] [Batch 0/12] [D loss: -10.059371] [G loss: -28.356941]\n",
      "[Epoch 157/200] [Batch 5/12] [D loss: -6.030753] [G loss: -26.405098]\n",
      "[Epoch 157/200] [Batch 10/12] [D loss: -9.527946] [G loss: -28.182302]\n",
      "[Epoch 158/200] [Batch 0/12] [D loss: -4.378668] [G loss: -29.300676]\n",
      "[Epoch 158/200] [Batch 5/12] [D loss: -13.126699] [G loss: -30.133181]\n",
      "[Epoch 158/200] [Batch 10/12] [D loss: -8.344649] [G loss: -29.558588]\n",
      "[Epoch 159/200] [Batch 0/12] [D loss: -9.904139] [G loss: -29.831337]\n",
      "[Epoch 159/200] [Batch 5/12] [D loss: -6.894030] [G loss: -24.365734]\n",
      "[Epoch 159/200] [Batch 10/12] [D loss: -12.805870] [G loss: -23.464472]\n",
      "[Epoch 160/200] [Batch 0/12] [D loss: -0.195673] [G loss: -28.056959]\n",
      "[Epoch 160/200] [Batch 5/12] [D loss: -13.957296] [G loss: -22.282413]\n",
      "[Epoch 160/200] [Batch 10/12] [D loss: -6.514069] [G loss: -21.867817]\n",
      "[Epoch 161/200] [Batch 0/12] [D loss: -9.086842] [G loss: -27.617844]\n",
      "[Epoch 161/200] [Batch 5/12] [D loss: -4.407104] [G loss: -25.027298]\n",
      "[Epoch 161/200] [Batch 10/12] [D loss: -9.037957] [G loss: -27.917355]\n",
      "[Epoch 162/200] [Batch 0/12] [D loss: -10.054637] [G loss: -29.128761]\n",
      "[Epoch 162/200] [Batch 5/12] [D loss: -5.050723] [G loss: -24.528688]\n",
      "[Epoch 162/200] [Batch 10/12] [D loss: -6.991813] [G loss: -31.195093]\n",
      "[Epoch 163/200] [Batch 0/12] [D loss: -5.002031] [G loss: -23.240498]\n",
      "[Epoch 163/200] [Batch 5/12] [D loss: -5.417048] [G loss: -30.100964]\n",
      "[Epoch 163/200] [Batch 10/12] [D loss: -2.742141] [G loss: -31.017868]\n",
      "[Epoch 164/200] [Batch 0/12] [D loss: -10.785255] [G loss: -28.236767]\n",
      "[Epoch 164/200] [Batch 5/12] [D loss: -7.137265] [G loss: -21.707081]\n",
      "[Epoch 164/200] [Batch 10/12] [D loss: -10.232971] [G loss: -26.304291]\n",
      "[Epoch 165/200] [Batch 0/12] [D loss: -14.213056] [G loss: -23.832127]\n",
      "[Epoch 165/200] [Batch 5/12] [D loss: -6.586254] [G loss: -27.628443]\n",
      "[Epoch 165/200] [Batch 10/12] [D loss: -11.071819] [G loss: -25.944206]\n",
      "[Epoch 166/200] [Batch 0/12] [D loss: -3.931708] [G loss: -27.125786]\n",
      "[Epoch 166/200] [Batch 5/12] [D loss: -8.088547] [G loss: -26.148085]\n",
      "[Epoch 166/200] [Batch 10/12] [D loss: -7.804790] [G loss: -23.797089]\n",
      "[Epoch 167/200] [Batch 0/12] [D loss: -3.853007] [G loss: -24.161566]\n",
      "[Epoch 167/200] [Batch 5/12] [D loss: -13.285099] [G loss: -27.128771]\n",
      "[Epoch 167/200] [Batch 10/12] [D loss: -3.266479] [G loss: -24.446865]\n",
      "[Epoch 168/200] [Batch 0/12] [D loss: -10.767134] [G loss: -24.540258]\n",
      "[Epoch 168/200] [Batch 5/12] [D loss: -12.934456] [G loss: -24.051401]\n",
      "[Epoch 168/200] [Batch 10/12] [D loss: -11.210488] [G loss: -28.161148]\n",
      "[Epoch 169/200] [Batch 0/12] [D loss: -8.778232] [G loss: -23.583780]\n",
      "[Epoch 169/200] [Batch 5/12] [D loss: -15.075583] [G loss: -18.819050]\n",
      "[Epoch 169/200] [Batch 10/12] [D loss: -4.501667] [G loss: -23.183834]\n",
      "[Epoch 170/200] [Batch 0/12] [D loss: -9.262761] [G loss: -25.261580]\n",
      "[Epoch 170/200] [Batch 5/12] [D loss: -8.046150] [G loss: -20.886393]\n",
      "[Epoch 170/200] [Batch 10/12] [D loss: -11.838068] [G loss: -22.347406]\n",
      "[Epoch 171/200] [Batch 0/12] [D loss: -10.110345] [G loss: -26.367901]\n",
      "[Epoch 171/200] [Batch 5/12] [D loss: -9.226305] [G loss: -25.132290]\n",
      "[Epoch 171/200] [Batch 10/12] [D loss: -5.959225] [G loss: -29.291676]\n",
      "[Epoch 172/200] [Batch 0/12] [D loss: -10.975161] [G loss: -27.354107]\n",
      "[Epoch 172/200] [Batch 5/12] [D loss: -6.001202] [G loss: -25.897011]\n",
      "[Epoch 172/200] [Batch 10/12] [D loss: -9.927105] [G loss: -25.217541]\n",
      "[Epoch 173/200] [Batch 0/12] [D loss: -11.459467] [G loss: -25.930052]\n",
      "[Epoch 173/200] [Batch 5/12] [D loss: -5.680862] [G loss: -23.472248]\n",
      "[Epoch 173/200] [Batch 10/12] [D loss: -10.354376] [G loss: -22.735069]\n",
      "[Epoch 174/200] [Batch 0/12] [D loss: -5.545908] [G loss: -22.157742]\n",
      "[Epoch 174/200] [Batch 5/12] [D loss: -7.445526] [G loss: -20.973667]\n",
      "[Epoch 174/200] [Batch 10/12] [D loss: -8.686152] [G loss: -27.493961]\n",
      "[Epoch 175/200] [Batch 0/12] [D loss: -6.469454] [G loss: -25.171852]\n",
      "[Epoch 175/200] [Batch 5/12] [D loss: -12.799666] [G loss: -23.868725]\n",
      "[Epoch 175/200] [Batch 10/12] [D loss: -7.104703] [G loss: -20.446932]\n",
      "[Epoch 176/200] [Batch 0/12] [D loss: -9.034389] [G loss: -26.386158]\n",
      "[Epoch 176/200] [Batch 5/12] [D loss: -8.963402] [G loss: -24.438942]\n",
      "[Epoch 176/200] [Batch 10/12] [D loss: -8.782767] [G loss: -26.471535]\n",
      "[Epoch 177/200] [Batch 0/12] [D loss: -10.745635] [G loss: -31.690454]\n",
      "[Epoch 177/200] [Batch 5/12] [D loss: -9.433632] [G loss: -30.235790]\n",
      "[Epoch 177/200] [Batch 10/12] [D loss: -9.711781] [G loss: -28.158754]\n",
      "[Epoch 178/200] [Batch 0/12] [D loss: -1.857356] [G loss: -24.753153]\n",
      "[Epoch 178/200] [Batch 5/12] [D loss: -11.141481] [G loss: -27.007843]\n",
      "[Epoch 178/200] [Batch 10/12] [D loss: -9.076557] [G loss: -23.602718]\n",
      "[Epoch 179/200] [Batch 0/12] [D loss: -8.369833] [G loss: -25.810881]\n",
      "[Epoch 179/200] [Batch 5/12] [D loss: -8.833652] [G loss: -24.979935]\n",
      "[Epoch 179/200] [Batch 10/12] [D loss: -7.999428] [G loss: -25.112936]\n",
      "[Epoch 180/200] [Batch 0/12] [D loss: -13.747730] [G loss: -26.076565]\n",
      "[Epoch 180/200] [Batch 5/12] [D loss: -5.121232] [G loss: -22.479267]\n",
      "[Epoch 180/200] [Batch 10/12] [D loss: -19.736786] [G loss: -20.954746]\n",
      "[Epoch 181/200] [Batch 0/12] [D loss: -13.647430] [G loss: -25.845650]\n",
      "[Epoch 181/200] [Batch 5/12] [D loss: -9.976665] [G loss: -24.532915]\n",
      "[Epoch 181/200] [Batch 10/12] [D loss: -10.681804] [G loss: -23.312113]\n",
      "[Epoch 182/200] [Batch 0/12] [D loss: -9.835489] [G loss: -23.396091]\n",
      "[Epoch 182/200] [Batch 5/12] [D loss: -6.289362] [G loss: -29.224167]\n",
      "[Epoch 182/200] [Batch 10/12] [D loss: -9.020155] [G loss: -28.747507]\n",
      "[Epoch 183/200] [Batch 0/12] [D loss: -7.035989] [G loss: -30.881680]\n",
      "[Epoch 183/200] [Batch 5/12] [D loss: -8.229002] [G loss: -22.035480]\n",
      "[Epoch 183/200] [Batch 10/12] [D loss: -11.714369] [G loss: -28.367104]\n",
      "[Epoch 184/200] [Batch 0/12] [D loss: -9.818165] [G loss: -29.685577]\n",
      "[Epoch 184/200] [Batch 5/12] [D loss: -15.836838] [G loss: -25.131327]\n",
      "[Epoch 184/200] [Batch 10/12] [D loss: -10.729710] [G loss: -29.278946]\n",
      "[Epoch 185/200] [Batch 0/12] [D loss: -7.179891] [G loss: -28.596085]\n",
      "[Epoch 185/200] [Batch 5/12] [D loss: -18.121437] [G loss: -24.438175]\n",
      "[Epoch 185/200] [Batch 10/12] [D loss: -13.141439] [G loss: -18.516188]\n",
      "[Epoch 186/200] [Batch 0/12] [D loss: -15.741065] [G loss: -29.563066]\n",
      "[Epoch 186/200] [Batch 5/12] [D loss: -10.434446] [G loss: -27.713326]\n",
      "[Epoch 186/200] [Batch 10/12] [D loss: -16.862053] [G loss: -28.005863]\n",
      "[Epoch 187/200] [Batch 0/12] [D loss: -10.460724] [G loss: -27.739031]\n",
      "[Epoch 187/200] [Batch 5/12] [D loss: -6.606581] [G loss: -31.681339]\n",
      "[Epoch 187/200] [Batch 10/12] [D loss: -13.387813] [G loss: -29.961290]\n",
      "[Epoch 188/200] [Batch 0/12] [D loss: -12.712248] [G loss: -26.486744]\n",
      "[Epoch 188/200] [Batch 5/12] [D loss: -15.893183] [G loss: -28.722935]\n",
      "[Epoch 188/200] [Batch 10/12] [D loss: -12.344094] [G loss: -32.035774]\n",
      "[Epoch 189/200] [Batch 0/12] [D loss: -9.889580] [G loss: -29.789171]\n",
      "[Epoch 189/200] [Batch 5/12] [D loss: -12.726635] [G loss: -35.521984]\n",
      "[Epoch 189/200] [Batch 10/12] [D loss: -8.198760] [G loss: -30.271305]\n",
      "[Epoch 190/200] [Batch 0/12] [D loss: -7.944415] [G loss: -31.218517]\n",
      "[Epoch 190/200] [Batch 5/12] [D loss: -13.244489] [G loss: -25.180607]\n",
      "[Epoch 190/200] [Batch 10/12] [D loss: -18.466770] [G loss: -23.537865]\n",
      "[Epoch 191/200] [Batch 0/12] [D loss: -3.237881] [G loss: -27.195356]\n",
      "[Epoch 191/200] [Batch 5/12] [D loss: -13.347507] [G loss: -28.678127]\n",
      "[Epoch 191/200] [Batch 10/12] [D loss: -3.441242] [G loss: -34.482861]\n",
      "[Epoch 192/200] [Batch 0/12] [D loss: -7.256692] [G loss: -26.197649]\n",
      "[Epoch 192/200] [Batch 5/12] [D loss: -15.786221] [G loss: -31.832630]\n",
      "[Epoch 192/200] [Batch 10/12] [D loss: -14.356331] [G loss: -23.241129]\n",
      "[Epoch 193/200] [Batch 0/12] [D loss: -9.701699] [G loss: -28.193699]\n",
      "[Epoch 193/200] [Batch 5/12] [D loss: -9.271316] [G loss: -33.871204]\n",
      "[Epoch 193/200] [Batch 10/12] [D loss: -6.386981] [G loss: -28.116161]\n",
      "[Epoch 194/200] [Batch 0/12] [D loss: -11.258640] [G loss: -26.099209]\n",
      "[Epoch 194/200] [Batch 5/12] [D loss: -17.074400] [G loss: -27.291927]\n",
      "[Epoch 194/200] [Batch 10/12] [D loss: -18.433474] [G loss: -26.606712]\n",
      "[Epoch 195/200] [Batch 0/12] [D loss: -12.370716] [G loss: -28.285755]\n",
      "[Epoch 195/200] [Batch 5/12] [D loss: -11.945061] [G loss: -30.102530]\n",
      "[Epoch 195/200] [Batch 10/12] [D loss: -16.996206] [G loss: -29.963476]\n",
      "[Epoch 196/200] [Batch 0/12] [D loss: -21.310135] [G loss: -26.705872]\n",
      "[Epoch 196/200] [Batch 5/12] [D loss: -10.854596] [G loss: -24.346699]\n",
      "[Epoch 196/200] [Batch 10/12] [D loss: -16.458374] [G loss: -27.811357]\n",
      "[Epoch 197/200] [Batch 0/12] [D loss: -19.094194] [G loss: -29.130299]\n",
      "[Epoch 197/200] [Batch 5/12] [D loss: -11.503864] [G loss: -31.157848]\n",
      "[Epoch 197/200] [Batch 10/12] [D loss: -12.844382] [G loss: -30.616695]\n",
      "[Epoch 198/200] [Batch 0/12] [D loss: -10.382482] [G loss: -25.124496]\n",
      "[Epoch 198/200] [Batch 5/12] [D loss: -17.129009] [G loss: -28.331306]\n",
      "[Epoch 198/200] [Batch 10/12] [D loss: -13.734509] [G loss: -27.495819]\n",
      "[Epoch 199/200] [Batch 0/12] [D loss: -10.789416] [G loss: -33.808556]\n",
      "[Epoch 199/200] [Batch 5/12] [D loss: -8.867824] [G loss: -37.546440]\n",
      "[Epoch 199/200] [Batch 10/12] [D loss: -17.463997] [G loss: -26.453182]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        real_imgs = real_imgs.resize_(imgs.shape[0], opt.channels, opt.img_size,  opt.img_size)\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = discriminator(real_imgs)\n",
    "        fake_loss = discriminator(gen_imgs.detach())\n",
    "        # was距离是要最大化 real_loss - fake_loss\n",
    "        # 这里加上一个符号，loss下降就相当于最大化was距离\n",
    "        d_loss = torch.mean(-real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Clip weights of discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
    "\n",
    "        # Train the generator every n_critic iterations\n",
    "        if i % opt.n_critic == 0:\n",
    "    \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = torch.mean(-discriminator(gen_imgs))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
