{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ShawnDong98/GAN/blob/master/WGAN/DCWGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, channels=3, clip_value=0.01, img_size=64, latent_dim=100, lr=5e-05, n_cpu=8, n_critic=5, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.00005, help=\"learning rate\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\n",
    "parser.add_argument(\"--clip_value\", type=float, default=0.01, help=\"lower and upper clip value for disc. weights\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "opt = parser.parse_known_args()[0]\n",
    "print(opt)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = opt.img_size // 4\n",
    "        # 第一层是为了把的噪声（batch_size, 100)\n",
    "        # 拉到 (batch_size, 128, img_size, img_size)\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=32768, bias=True)\n",
      "  )\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator()\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32, bn=False),\n",
    "            *discriminator_block(32, 64, bn=False),\n",
    "            *discriminator_block(64, 128, bn=False),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "        # 这里相较于DCGAN删去了sigmoid函数\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Dropout2d(p=0.25, inplace=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Dropout2d(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Dropout2d(p=0.25, inplace=False)\n",
      "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Dropout2d(p=0.25, inplace=False)\n",
      "  )\n",
      "  (adv_layer): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator()\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里相较于DCGAN没有使用交叉熵\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        \"data/\",\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "# 这里相较于DCGAN，Adam优化器换成了RMSprop\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# i, (img, _) = next(enumerate(dataloader))\n",
    "# print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/12] [D loss: 0.000622] [G loss: 0.012907]\n",
      "[Epoch 0/200] [Batch 5/12] [D loss: -0.000025] [G loss: 0.013385]\n",
      "[Epoch 0/200] [Batch 10/12] [D loss: 0.000584] [G loss: 0.012893]\n",
      "[Epoch 1/200] [Batch 0/12] [D loss: 0.000043] [G loss: 0.012705]\n",
      "[Epoch 1/200] [Batch 5/12] [D loss: 0.000067] [G loss: 0.012136]\n",
      "[Epoch 1/200] [Batch 10/12] [D loss: -0.000418] [G loss: 0.012545]\n",
      "[Epoch 2/200] [Batch 0/12] [D loss: 0.000162] [G loss: 0.011972]\n",
      "[Epoch 2/200] [Batch 5/12] [D loss: -0.000352] [G loss: 0.012314]\n",
      "[Epoch 2/200] [Batch 10/12] [D loss: -0.000077] [G loss: 0.012486]\n",
      "[Epoch 3/200] [Batch 0/12] [D loss: -0.000421] [G loss: 0.012544]\n",
      "[Epoch 3/200] [Batch 5/12] [D loss: -0.000958] [G loss: 0.012562]\n",
      "[Epoch 3/200] [Batch 10/12] [D loss: -0.002700] [G loss: 0.012046]\n",
      "[Epoch 4/200] [Batch 0/12] [D loss: -0.003290] [G loss: 0.011612]\n",
      "[Epoch 4/200] [Batch 5/12] [D loss: -0.006375] [G loss: 0.011291]\n",
      "[Epoch 4/200] [Batch 10/12] [D loss: -0.013616] [G loss: 0.022294]\n",
      "[Epoch 5/200] [Batch 0/12] [D loss: -0.015033] [G loss: 0.019040]\n",
      "[Epoch 5/200] [Batch 5/12] [D loss: -0.027638] [G loss: 0.012968]\n",
      "[Epoch 5/200] [Batch 10/12] [D loss: -0.050105] [G loss: 0.009830]\n",
      "[Epoch 6/200] [Batch 0/12] [D loss: -0.046371] [G loss: 0.008643]\n",
      "[Epoch 6/200] [Batch 5/12] [D loss: -0.062221] [G loss: 0.022834]\n",
      "[Epoch 6/200] [Batch 10/12] [D loss: -0.097613] [G loss: 0.000962]\n",
      "[Epoch 7/200] [Batch 0/12] [D loss: -0.088140] [G loss: -0.025860]\n",
      "[Epoch 7/200] [Batch 5/12] [D loss: -0.105255] [G loss: -0.066709]\n",
      "[Epoch 7/200] [Batch 10/12] [D loss: -0.109161] [G loss: -0.072615]\n",
      "[Epoch 8/200] [Batch 0/12] [D loss: -0.076633] [G loss: -0.076172]\n",
      "[Epoch 8/200] [Batch 5/12] [D loss: -0.143875] [G loss: -0.084712]\n",
      "[Epoch 8/200] [Batch 10/12] [D loss: -0.077320] [G loss: -0.106652]\n",
      "[Epoch 9/200] [Batch 0/12] [D loss: -0.045568] [G loss: -0.158229]\n",
      "[Epoch 9/200] [Batch 5/12] [D loss: -0.043087] [G loss: -0.183664]\n",
      "[Epoch 9/200] [Batch 10/12] [D loss: -0.091691] [G loss: -0.305552]\n",
      "[Epoch 10/200] [Batch 0/12] [D loss: -0.046707] [G loss: -0.366177]\n",
      "[Epoch 10/200] [Batch 5/12] [D loss: -0.010580] [G loss: -0.350769]\n",
      "[Epoch 10/200] [Batch 10/12] [D loss: 0.000031] [G loss: -0.248647]\n",
      "[Epoch 11/200] [Batch 0/12] [D loss: -0.016593] [G loss: -0.247810]\n",
      "[Epoch 11/200] [Batch 5/12] [D loss: -0.055411] [G loss: -0.244813]\n",
      "[Epoch 11/200] [Batch 10/12] [D loss: 0.023161] [G loss: -0.217254]\n",
      "[Epoch 12/200] [Batch 0/12] [D loss: 0.023542] [G loss: -0.141653]\n",
      "[Epoch 12/200] [Batch 5/12] [D loss: 0.047737] [G loss: -0.191522]\n",
      "[Epoch 12/200] [Batch 10/12] [D loss: -0.015135] [G loss: -0.193589]\n",
      "[Epoch 13/200] [Batch 0/12] [D loss: 0.062736] [G loss: -0.182017]\n",
      "[Epoch 13/200] [Batch 5/12] [D loss: 0.060711] [G loss: -0.176150]\n",
      "[Epoch 13/200] [Batch 10/12] [D loss: -0.018410] [G loss: -0.149966]\n",
      "[Epoch 14/200] [Batch 0/12] [D loss: 0.038272] [G loss: -0.131750]\n",
      "[Epoch 14/200] [Batch 5/12] [D loss: 0.037560] [G loss: -0.104901]\n",
      "[Epoch 14/200] [Batch 10/12] [D loss: 0.009676] [G loss: -0.118058]\n",
      "[Epoch 15/200] [Batch 0/12] [D loss: 0.004609] [G loss: -0.114111]\n",
      "[Epoch 15/200] [Batch 5/12] [D loss: 0.002571] [G loss: -0.075351]\n",
      "[Epoch 15/200] [Batch 10/12] [D loss: -0.020385] [G loss: -0.078809]\n",
      "[Epoch 16/200] [Batch 0/12] [D loss: 0.012492] [G loss: -0.051729]\n",
      "[Epoch 16/200] [Batch 5/12] [D loss: -0.008948] [G loss: -0.044985]\n",
      "[Epoch 16/200] [Batch 10/12] [D loss: -0.007043] [G loss: -0.026830]\n",
      "[Epoch 17/200] [Batch 0/12] [D loss: -0.010132] [G loss: -0.008980]\n",
      "[Epoch 17/200] [Batch 5/12] [D loss: 0.002313] [G loss: -0.000072]\n",
      "[Epoch 17/200] [Batch 10/12] [D loss: -0.005966] [G loss: 0.015793]\n",
      "[Epoch 18/200] [Batch 0/12] [D loss: -0.019655] [G loss: 0.020378]\n",
      "[Epoch 18/200] [Batch 5/12] [D loss: -0.009711] [G loss: 0.028030]\n",
      "[Epoch 18/200] [Batch 10/12] [D loss: -0.023592] [G loss: 0.058263]\n",
      "[Epoch 19/200] [Batch 0/12] [D loss: -0.024464] [G loss: 0.062952]\n",
      "[Epoch 19/200] [Batch 5/12] [D loss: -0.040923] [G loss: 0.094943]\n",
      "[Epoch 19/200] [Batch 10/12] [D loss: -0.057272] [G loss: 0.113194]\n",
      "[Epoch 20/200] [Batch 0/12] [D loss: -0.054102] [G loss: 0.123617]\n",
      "[Epoch 20/200] [Batch 5/12] [D loss: -0.064535] [G loss: 0.160400]\n",
      "[Epoch 20/200] [Batch 10/12] [D loss: -0.074063] [G loss: 0.209762]\n",
      "[Epoch 21/200] [Batch 0/12] [D loss: -0.099539] [G loss: 0.228616]\n",
      "[Epoch 21/200] [Batch 5/12] [D loss: -0.107417] [G loss: 0.278132]\n",
      "[Epoch 21/200] [Batch 10/12] [D loss: -0.095376] [G loss: 0.316795]\n",
      "[Epoch 22/200] [Batch 0/12] [D loss: -0.103065] [G loss: 0.352077]\n",
      "[Epoch 22/200] [Batch 5/12] [D loss: -0.112549] [G loss: 0.403850]\n",
      "[Epoch 22/200] [Batch 10/12] [D loss: -0.107936] [G loss: 0.420685]\n",
      "[Epoch 23/200] [Batch 0/12] [D loss: -0.125141] [G loss: 0.435790]\n",
      "[Epoch 23/200] [Batch 5/12] [D loss: -0.122576] [G loss: 0.483407]\n",
      "[Epoch 23/200] [Batch 10/12] [D loss: -0.083448] [G loss: 0.467476]\n",
      "[Epoch 24/200] [Batch 0/12] [D loss: -0.077812] [G loss: 0.440873]\n",
      "[Epoch 24/200] [Batch 5/12] [D loss: -0.079593] [G loss: 0.416829]\n",
      "[Epoch 24/200] [Batch 10/12] [D loss: -0.050472] [G loss: 0.405992]\n",
      "[Epoch 25/200] [Batch 0/12] [D loss: -0.040776] [G loss: 0.401080]\n",
      "[Epoch 25/200] [Batch 5/12] [D loss: -0.009948] [G loss: 0.367460]\n",
      "[Epoch 25/200] [Batch 10/12] [D loss: -0.009579] [G loss: 0.341949]\n",
      "[Epoch 26/200] [Batch 0/12] [D loss: -0.018505] [G loss: 0.291008]\n",
      "[Epoch 26/200] [Batch 5/12] [D loss: -0.016292] [G loss: 0.237036]\n",
      "[Epoch 26/200] [Batch 10/12] [D loss: 0.003495] [G loss: 0.247698]\n",
      "[Epoch 27/200] [Batch 0/12] [D loss: 0.004279] [G loss: 0.223778]\n",
      "[Epoch 27/200] [Batch 5/12] [D loss: -0.003147] [G loss: 0.187046]\n",
      "[Epoch 27/200] [Batch 10/12] [D loss: -0.022160] [G loss: 0.143662]\n",
      "[Epoch 28/200] [Batch 0/12] [D loss: -0.014164] [G loss: 0.134631]\n",
      "[Epoch 28/200] [Batch 5/12] [D loss: -0.018307] [G loss: 0.109557]\n",
      "[Epoch 28/200] [Batch 10/12] [D loss: -0.029329] [G loss: 0.087917]\n",
      "[Epoch 29/200] [Batch 0/12] [D loss: -0.048180] [G loss: 0.074479]\n",
      "[Epoch 29/200] [Batch 5/12] [D loss: -0.057007] [G loss: 0.064007]\n",
      "[Epoch 29/200] [Batch 10/12] [D loss: -0.079319] [G loss: 0.042189]\n",
      "[Epoch 30/200] [Batch 0/12] [D loss: -0.082207] [G loss: 0.033354]\n",
      "[Epoch 30/200] [Batch 5/12] [D loss: -0.108409] [G loss: 0.014959]\n",
      "[Epoch 30/200] [Batch 10/12] [D loss: -0.110829] [G loss: -0.013196]\n",
      "[Epoch 31/200] [Batch 0/12] [D loss: -0.117710] [G loss: -0.020218]\n",
      "[Epoch 31/200] [Batch 5/12] [D loss: -0.072788] [G loss: -0.055667]\n",
      "[Epoch 31/200] [Batch 10/12] [D loss: -0.131973] [G loss: -0.091099]\n",
      "[Epoch 32/200] [Batch 0/12] [D loss: -0.093576] [G loss: -0.099714]\n",
      "[Epoch 32/200] [Batch 5/12] [D loss: -0.105235] [G loss: -0.108803]\n",
      "[Epoch 32/200] [Batch 10/12] [D loss: -0.120597] [G loss: -0.090282]\n",
      "[Epoch 33/200] [Batch 0/12] [D loss: -0.142869] [G loss: -0.080994]\n",
      "[Epoch 33/200] [Batch 5/12] [D loss: -0.104811] [G loss: -0.096145]\n",
      "[Epoch 33/200] [Batch 10/12] [D loss: -0.056263] [G loss: -0.129199]\n",
      "[Epoch 34/200] [Batch 0/12] [D loss: -0.063630] [G loss: -0.158262]\n",
      "[Epoch 34/200] [Batch 5/12] [D loss: -0.046819] [G loss: -0.227186]\n",
      "[Epoch 34/200] [Batch 10/12] [D loss: -0.117768] [G loss: -0.233850]\n",
      "[Epoch 35/200] [Batch 0/12] [D loss: -0.088117] [G loss: -0.320955]\n",
      "[Epoch 35/200] [Batch 5/12] [D loss: -0.077881] [G loss: -0.328300]\n",
      "[Epoch 35/200] [Batch 10/12] [D loss: -0.049216] [G loss: -0.334747]\n",
      "[Epoch 36/200] [Batch 0/12] [D loss: 0.020644] [G loss: -0.300490]\n",
      "[Epoch 36/200] [Batch 5/12] [D loss: -0.058179] [G loss: -0.155264]\n",
      "[Epoch 36/200] [Batch 10/12] [D loss: -0.035373] [G loss: -0.021803]\n",
      "[Epoch 37/200] [Batch 0/12] [D loss: -0.110141] [G loss: 0.029250]\n",
      "[Epoch 37/200] [Batch 5/12] [D loss: -0.042672] [G loss: 0.061374]\n",
      "[Epoch 37/200] [Batch 10/12] [D loss: -0.030070] [G loss: 0.091537]\n",
      "[Epoch 38/200] [Batch 0/12] [D loss: 0.062812] [G loss: 0.043950]\n",
      "[Epoch 38/200] [Batch 5/12] [D loss: -0.016706] [G loss: 0.018766]\n",
      "[Epoch 38/200] [Batch 10/12] [D loss: -0.070020] [G loss: 0.074211]\n",
      "[Epoch 39/200] [Batch 0/12] [D loss: -0.081144] [G loss: 0.079074]\n",
      "[Epoch 39/200] [Batch 5/12] [D loss: -0.136173] [G loss: 0.076795]\n",
      "[Epoch 39/200] [Batch 10/12] [D loss: -0.100141] [G loss: 0.084598]\n",
      "[Epoch 40/200] [Batch 0/12] [D loss: -0.056096] [G loss: 0.043797]\n",
      "[Epoch 40/200] [Batch 5/12] [D loss: -0.065527] [G loss: 0.141799]\n",
      "[Epoch 40/200] [Batch 10/12] [D loss: -0.100449] [G loss: 0.359381]\n",
      "[Epoch 41/200] [Batch 0/12] [D loss: -0.103911] [G loss: 0.447442]\n",
      "[Epoch 41/200] [Batch 5/12] [D loss: -0.107097] [G loss: 0.577995]\n",
      "[Epoch 41/200] [Batch 10/12] [D loss: -0.180725] [G loss: 0.612586]\n",
      "[Epoch 42/200] [Batch 0/12] [D loss: -0.101148] [G loss: 0.586521]\n",
      "[Epoch 42/200] [Batch 5/12] [D loss: -0.020474] [G loss: 0.514142]\n",
      "[Epoch 42/200] [Batch 10/12] [D loss: -0.103269] [G loss: 0.476220]\n",
      "[Epoch 43/200] [Batch 0/12] [D loss: -0.050567] [G loss: 0.461277]\n",
      "[Epoch 43/200] [Batch 5/12] [D loss: -0.090891] [G loss: 0.395614]\n",
      "[Epoch 43/200] [Batch 10/12] [D loss: -0.034887] [G loss: 0.344771]\n",
      "[Epoch 44/200] [Batch 0/12] [D loss: -0.029075] [G loss: 0.298019]\n",
      "[Epoch 44/200] [Batch 5/12] [D loss: -0.040442] [G loss: 0.328179]\n",
      "[Epoch 44/200] [Batch 10/12] [D loss: -0.016648] [G loss: 0.325689]\n",
      "[Epoch 45/200] [Batch 0/12] [D loss: -0.036594] [G loss: 0.315598]\n",
      "[Epoch 45/200] [Batch 5/12] [D loss: -0.015817] [G loss: 0.346504]\n",
      "[Epoch 45/200] [Batch 10/12] [D loss: -0.044758] [G loss: 0.360287]\n",
      "[Epoch 46/200] [Batch 0/12] [D loss: -0.014614] [G loss: 0.379223]\n",
      "[Epoch 46/200] [Batch 5/12] [D loss: -0.044658] [G loss: 0.336169]\n",
      "[Epoch 46/200] [Batch 10/12] [D loss: -0.013500] [G loss: 0.263908]\n",
      "[Epoch 47/200] [Batch 0/12] [D loss: -0.030236] [G loss: 0.238433]\n",
      "[Epoch 47/200] [Batch 5/12] [D loss: -0.029352] [G loss: 0.183552]\n",
      "[Epoch 47/200] [Batch 10/12] [D loss: -0.056919] [G loss: 0.141501]\n",
      "[Epoch 48/200] [Batch 0/12] [D loss: -0.033916] [G loss: 0.121633]\n",
      "[Epoch 48/200] [Batch 5/12] [D loss: -0.021796] [G loss: 0.086095]\n",
      "[Epoch 48/200] [Batch 10/12] [D loss: -0.041723] [G loss: 0.067310]\n",
      "[Epoch 49/200] [Batch 0/12] [D loss: -0.034772] [G loss: 0.075968]\n",
      "[Epoch 49/200] [Batch 5/12] [D loss: -0.037343] [G loss: 0.081321]\n",
      "[Epoch 49/200] [Batch 10/12] [D loss: -0.064075] [G loss: 0.100001]\n",
      "[Epoch 50/200] [Batch 0/12] [D loss: -0.082349] [G loss: 0.112236]\n",
      "[Epoch 50/200] [Batch 5/12] [D loss: -0.077261] [G loss: 0.086062]\n",
      "[Epoch 50/200] [Batch 10/12] [D loss: -0.070558] [G loss: 0.066306]\n",
      "[Epoch 51/200] [Batch 0/12] [D loss: -0.076539] [G loss: 0.020214]\n",
      "[Epoch 51/200] [Batch 5/12] [D loss: -0.034999] [G loss: -0.010374]\n",
      "[Epoch 51/200] [Batch 10/12] [D loss: -0.073572] [G loss: -0.069714]\n",
      "[Epoch 52/200] [Batch 0/12] [D loss: -0.028870] [G loss: -0.080872]\n",
      "[Epoch 52/200] [Batch 5/12] [D loss: -0.024985] [G loss: -0.136497]\n",
      "[Epoch 52/200] [Batch 10/12] [D loss: -0.079931] [G loss: -0.154203]\n",
      "[Epoch 53/200] [Batch 0/12] [D loss: -0.062438] [G loss: -0.190297]\n",
      "[Epoch 53/200] [Batch 5/12] [D loss: -0.040261] [G loss: -0.159134]\n",
      "[Epoch 53/200] [Batch 10/12] [D loss: -0.008040] [G loss: -0.136937]\n",
      "[Epoch 54/200] [Batch 0/12] [D loss: -0.045229] [G loss: -0.139266]\n",
      "[Epoch 54/200] [Batch 5/12] [D loss: -0.016493] [G loss: -0.110743]\n",
      "[Epoch 54/200] [Batch 10/12] [D loss: -0.005261] [G loss: -0.077421]\n",
      "[Epoch 55/200] [Batch 0/12] [D loss: -0.017069] [G loss: -0.054307]\n",
      "[Epoch 55/200] [Batch 5/12] [D loss: 0.032330] [G loss: -0.028191]\n",
      "[Epoch 55/200] [Batch 10/12] [D loss: 0.010437] [G loss: -0.038094]\n",
      "[Epoch 56/200] [Batch 0/12] [D loss: -0.027085] [G loss: -0.029385]\n",
      "[Epoch 56/200] [Batch 5/12] [D loss: 0.003120] [G loss: -0.046879]\n",
      "[Epoch 56/200] [Batch 10/12] [D loss: -0.010133] [G loss: -0.038869]\n",
      "[Epoch 57/200] [Batch 0/12] [D loss: -0.011911] [G loss: -0.058527]\n",
      "[Epoch 57/200] [Batch 5/12] [D loss: -0.010904] [G loss: -0.032998]\n",
      "[Epoch 57/200] [Batch 10/12] [D loss: -0.017840] [G loss: 0.004249]\n",
      "[Epoch 58/200] [Batch 0/12] [D loss: -0.023689] [G loss: 0.031623]\n",
      "[Epoch 58/200] [Batch 5/12] [D loss: -0.023867] [G loss: 0.101626]\n",
      "[Epoch 58/200] [Batch 10/12] [D loss: -0.031873] [G loss: 0.148310]\n",
      "[Epoch 59/200] [Batch 0/12] [D loss: -0.027302] [G loss: 0.185414]\n",
      "[Epoch 59/200] [Batch 5/12] [D loss: -0.023165] [G loss: 0.207507]\n",
      "[Epoch 59/200] [Batch 10/12] [D loss: -0.017414] [G loss: 0.223393]\n",
      "[Epoch 60/200] [Batch 0/12] [D loss: -0.042593] [G loss: 0.218465]\n",
      "[Epoch 60/200] [Batch 5/12] [D loss: -0.033406] [G loss: 0.229197]\n",
      "[Epoch 60/200] [Batch 10/12] [D loss: -0.055486] [G loss: 0.195601]\n",
      "[Epoch 61/200] [Batch 0/12] [D loss: -0.078760] [G loss: 0.189134]\n",
      "[Epoch 61/200] [Batch 5/12] [D loss: -0.050383] [G loss: 0.196018]\n",
      "[Epoch 61/200] [Batch 10/12] [D loss: -0.066907] [G loss: 0.225600]\n",
      "[Epoch 62/200] [Batch 0/12] [D loss: -0.068661] [G loss: 0.200947]\n",
      "[Epoch 62/200] [Batch 5/12] [D loss: -0.015981] [G loss: 0.302308]\n",
      "[Epoch 62/200] [Batch 10/12] [D loss: -0.050325] [G loss: 0.379377]\n",
      "[Epoch 63/200] [Batch 0/12] [D loss: -0.040910] [G loss: 0.352874]\n",
      "[Epoch 63/200] [Batch 5/12] [D loss: -0.026439] [G loss: 0.358670]\n",
      "[Epoch 63/200] [Batch 10/12] [D loss: -0.025533] [G loss: 0.319170]\n",
      "[Epoch 64/200] [Batch 0/12] [D loss: -0.017914] [G loss: 0.283428]\n",
      "[Epoch 64/200] [Batch 5/12] [D loss: -0.004665] [G loss: 0.241789]\n",
      "[Epoch 64/200] [Batch 10/12] [D loss: -0.004599] [G loss: 0.198408]\n",
      "[Epoch 65/200] [Batch 0/12] [D loss: -0.031640] [G loss: 0.180599]\n",
      "[Epoch 65/200] [Batch 5/12] [D loss: -0.007914] [G loss: 0.163698]\n",
      "[Epoch 65/200] [Batch 10/12] [D loss: -0.023537] [G loss: 0.162271]\n",
      "[Epoch 66/200] [Batch 0/12] [D loss: -0.016941] [G loss: 0.152698]\n",
      "[Epoch 66/200] [Batch 5/12] [D loss: -0.012686] [G loss: 0.165855]\n",
      "[Epoch 66/200] [Batch 10/12] [D loss: -0.027015] [G loss: 0.193495]\n",
      "[Epoch 67/200] [Batch 0/12] [D loss: -0.030121] [G loss: 0.176755]\n",
      "[Epoch 67/200] [Batch 5/12] [D loss: -0.032736] [G loss: 0.143198]\n",
      "[Epoch 67/200] [Batch 10/12] [D loss: -0.037521] [G loss: 0.103925]\n",
      "[Epoch 68/200] [Batch 0/12] [D loss: -0.038186] [G loss: 0.076987]\n",
      "[Epoch 68/200] [Batch 5/12] [D loss: -0.027772] [G loss: 0.071346]\n",
      "[Epoch 68/200] [Batch 10/12] [D loss: -0.036127] [G loss: 0.038500]\n",
      "[Epoch 69/200] [Batch 0/12] [D loss: -0.025850] [G loss: 0.010246]\n",
      "[Epoch 69/200] [Batch 5/12] [D loss: -0.042565] [G loss: -0.007999]\n",
      "[Epoch 69/200] [Batch 10/12] [D loss: -0.005825] [G loss: -0.047950]\n",
      "[Epoch 70/200] [Batch 0/12] [D loss: -0.016366] [G loss: -0.026876]\n",
      "[Epoch 70/200] [Batch 5/12] [D loss: -0.008176] [G loss: -0.025654]\n",
      "[Epoch 70/200] [Batch 10/12] [D loss: -0.033286] [G loss: -0.004627]\n",
      "[Epoch 71/200] [Batch 0/12] [D loss: -0.046416] [G loss: 0.005123]\n",
      "[Epoch 71/200] [Batch 5/12] [D loss: -0.054188] [G loss: 0.022941]\n",
      "[Epoch 71/200] [Batch 10/12] [D loss: -0.026678] [G loss: 0.034163]\n",
      "[Epoch 72/200] [Batch 0/12] [D loss: -0.029118] [G loss: 0.024100]\n",
      "[Epoch 72/200] [Batch 5/12] [D loss: 0.010308] [G loss: 0.039643]\n",
      "[Epoch 72/200] [Batch 10/12] [D loss: -0.001799] [G loss: 0.005081]\n",
      "[Epoch 73/200] [Batch 0/12] [D loss: 0.009234] [G loss: -0.034692]\n",
      "[Epoch 73/200] [Batch 5/12] [D loss: -0.038572] [G loss: -0.041048]\n",
      "[Epoch 73/200] [Batch 10/12] [D loss: -0.037258] [G loss: -0.064763]\n",
      "[Epoch 74/200] [Batch 0/12] [D loss: -0.002137] [G loss: -0.064409]\n",
      "[Epoch 74/200] [Batch 5/12] [D loss: 0.011848] [G loss: -0.085348]\n",
      "[Epoch 74/200] [Batch 10/12] [D loss: -0.007397] [G loss: -0.045874]\n",
      "[Epoch 75/200] [Batch 0/12] [D loss: 0.025840] [G loss: -0.000737]\n",
      "[Epoch 75/200] [Batch 5/12] [D loss: -0.017898] [G loss: 0.078072]\n",
      "[Epoch 75/200] [Batch 10/12] [D loss: -0.044108] [G loss: 0.131447]\n",
      "[Epoch 76/200] [Batch 0/12] [D loss: -0.019352] [G loss: 0.153442]\n",
      "[Epoch 76/200] [Batch 5/12] [D loss: -0.028331] [G loss: 0.204061]\n",
      "[Epoch 76/200] [Batch 10/12] [D loss: -0.001720] [G loss: 0.197367]\n",
      "[Epoch 77/200] [Batch 0/12] [D loss: -0.011665] [G loss: 0.166564]\n",
      "[Epoch 77/200] [Batch 5/12] [D loss: -0.018821] [G loss: 0.169085]\n",
      "[Epoch 77/200] [Batch 10/12] [D loss: -0.034689] [G loss: 0.181466]\n",
      "[Epoch 78/200] [Batch 0/12] [D loss: -0.026959] [G loss: 0.199780]\n",
      "[Epoch 78/200] [Batch 5/12] [D loss: -0.023240] [G loss: 0.160157]\n",
      "[Epoch 78/200] [Batch 10/12] [D loss: -0.019638] [G loss: 0.158602]\n",
      "[Epoch 79/200] [Batch 0/12] [D loss: -0.034307] [G loss: 0.178222]\n",
      "[Epoch 79/200] [Batch 5/12] [D loss: -0.016927] [G loss: 0.183535]\n",
      "[Epoch 79/200] [Batch 10/12] [D loss: 0.003903] [G loss: 0.201355]\n",
      "[Epoch 80/200] [Batch 0/12] [D loss: -0.006650] [G loss: 0.195105]\n",
      "[Epoch 80/200] [Batch 5/12] [D loss: -0.009737] [G loss: 0.214971]\n",
      "[Epoch 80/200] [Batch 10/12] [D loss: -0.021468] [G loss: 0.190228]\n",
      "[Epoch 81/200] [Batch 0/12] [D loss: -0.012300] [G loss: 0.179630]\n",
      "[Epoch 81/200] [Batch 5/12] [D loss: -0.003717] [G loss: 0.147328]\n",
      "[Epoch 81/200] [Batch 10/12] [D loss: -0.014264] [G loss: 0.119875]\n",
      "[Epoch 82/200] [Batch 0/12] [D loss: -0.015988] [G loss: 0.110233]\n",
      "[Epoch 82/200] [Batch 5/12] [D loss: 0.005488] [G loss: 0.114608]\n",
      "[Epoch 82/200] [Batch 10/12] [D loss: -0.000158] [G loss: 0.129171]\n",
      "[Epoch 83/200] [Batch 0/12] [D loss: 0.005224] [G loss: 0.120366]\n",
      "[Epoch 83/200] [Batch 5/12] [D loss: -0.036611] [G loss: 0.120836]\n",
      "[Epoch 83/200] [Batch 10/12] [D loss: -0.019881] [G loss: 0.103254]\n",
      "[Epoch 84/200] [Batch 0/12] [D loss: -0.014488] [G loss: 0.079295]\n",
      "[Epoch 84/200] [Batch 5/12] [D loss: -0.009671] [G loss: 0.036955]\n",
      "[Epoch 84/200] [Batch 10/12] [D loss: -0.024138] [G loss: 0.010007]\n",
      "[Epoch 85/200] [Batch 0/12] [D loss: -0.029180] [G loss: -0.002239]\n",
      "[Epoch 85/200] [Batch 5/12] [D loss: -0.038330] [G loss: -0.032471]\n",
      "[Epoch 85/200] [Batch 10/12] [D loss: -0.008407] [G loss: -0.038425]\n",
      "[Epoch 86/200] [Batch 0/12] [D loss: -0.021569] [G loss: -0.039157]\n",
      "[Epoch 86/200] [Batch 5/12] [D loss: -0.012839] [G loss: 0.011303]\n",
      "[Epoch 86/200] [Batch 10/12] [D loss: -0.026059] [G loss: 0.015900]\n",
      "[Epoch 87/200] [Batch 0/12] [D loss: -0.003512] [G loss: 0.051867]\n",
      "[Epoch 87/200] [Batch 5/12] [D loss: -0.013438] [G loss: 0.091296]\n",
      "[Epoch 87/200] [Batch 10/12] [D loss: -0.010564] [G loss: 0.084077]\n",
      "[Epoch 88/200] [Batch 0/12] [D loss: -0.018082] [G loss: 0.084271]\n",
      "[Epoch 88/200] [Batch 5/12] [D loss: -0.007254] [G loss: 0.065291]\n",
      "[Epoch 88/200] [Batch 10/12] [D loss: -0.028908] [G loss: 0.083739]\n",
      "[Epoch 89/200] [Batch 0/12] [D loss: -0.008021] [G loss: 0.077182]\n",
      "[Epoch 89/200] [Batch 5/12] [D loss: -0.011622] [G loss: 0.082601]\n",
      "[Epoch 89/200] [Batch 10/12] [D loss: -0.019046] [G loss: 0.096284]\n",
      "[Epoch 90/200] [Batch 0/12] [D loss: -0.010103] [G loss: 0.108691]\n",
      "[Epoch 90/200] [Batch 5/12] [D loss: -0.013521] [G loss: 0.106607]\n",
      "[Epoch 90/200] [Batch 10/12] [D loss: -0.018703] [G loss: 0.106562]\n",
      "[Epoch 91/200] [Batch 0/12] [D loss: -0.010113] [G loss: 0.145913]\n",
      "[Epoch 91/200] [Batch 5/12] [D loss: -0.011343] [G loss: 0.123458]\n",
      "[Epoch 91/200] [Batch 10/12] [D loss: -0.002575] [G loss: 0.145519]\n",
      "[Epoch 92/200] [Batch 0/12] [D loss: -0.034595] [G loss: 0.147420]\n",
      "[Epoch 92/200] [Batch 5/12] [D loss: -0.007549] [G loss: 0.155227]\n",
      "[Epoch 92/200] [Batch 10/12] [D loss: -0.017763] [G loss: 0.146844]\n",
      "[Epoch 93/200] [Batch 0/12] [D loss: -0.019994] [G loss: 0.138934]\n",
      "[Epoch 93/200] [Batch 5/12] [D loss: -0.016288] [G loss: 0.122915]\n",
      "[Epoch 93/200] [Batch 10/12] [D loss: 0.008274] [G loss: 0.101743]\n",
      "[Epoch 94/200] [Batch 0/12] [D loss: -0.006918] [G loss: 0.094494]\n",
      "[Epoch 94/200] [Batch 5/12] [D loss: -0.012322] [G loss: 0.079933]\n",
      "[Epoch 94/200] [Batch 10/12] [D loss: 0.000750] [G loss: 0.092287]\n",
      "[Epoch 95/200] [Batch 0/12] [D loss: -0.016272] [G loss: 0.084378]\n",
      "[Epoch 95/200] [Batch 5/12] [D loss: -0.018939] [G loss: 0.089677]\n",
      "[Epoch 95/200] [Batch 10/12] [D loss: -0.008290] [G loss: 0.081387]\n",
      "[Epoch 96/200] [Batch 0/12] [D loss: -0.015421] [G loss: 0.067811]\n",
      "[Epoch 96/200] [Batch 5/12] [D loss: -0.006846] [G loss: 0.067791]\n",
      "[Epoch 96/200] [Batch 10/12] [D loss: -0.039540] [G loss: 0.079545]\n",
      "[Epoch 97/200] [Batch 0/12] [D loss: 0.006258] [G loss: 0.100605]\n",
      "[Epoch 97/200] [Batch 5/12] [D loss: -0.007005] [G loss: 0.068756]\n",
      "[Epoch 97/200] [Batch 10/12] [D loss: -0.023292] [G loss: 0.091854]\n",
      "[Epoch 98/200] [Batch 0/12] [D loss: -0.029377] [G loss: 0.122587]\n",
      "[Epoch 98/200] [Batch 5/12] [D loss: -0.012182] [G loss: 0.114262]\n",
      "[Epoch 98/200] [Batch 10/12] [D loss: -0.008299] [G loss: 0.086929]\n",
      "[Epoch 99/200] [Batch 0/12] [D loss: -0.013420] [G loss: 0.062770]\n",
      "[Epoch 99/200] [Batch 5/12] [D loss: -0.033053] [G loss: 0.035349]\n",
      "[Epoch 99/200] [Batch 10/12] [D loss: -0.023266] [G loss: 0.008984]\n",
      "[Epoch 100/200] [Batch 0/12] [D loss: -0.004483] [G loss: 0.021678]\n",
      "[Epoch 100/200] [Batch 5/12] [D loss: 0.015254] [G loss: 0.048727]\n",
      "[Epoch 100/200] [Batch 10/12] [D loss: -0.021509] [G loss: 0.064095]\n",
      "[Epoch 101/200] [Batch 0/12] [D loss: -0.031894] [G loss: 0.083547]\n",
      "[Epoch 101/200] [Batch 5/12] [D loss: 0.004721] [G loss: 0.121444]\n",
      "[Epoch 101/200] [Batch 10/12] [D loss: -0.028638] [G loss: 0.144182]\n",
      "[Epoch 102/200] [Batch 0/12] [D loss: -0.018968] [G loss: 0.127313]\n",
      "[Epoch 102/200] [Batch 5/12] [D loss: -0.004785] [G loss: 0.122560]\n",
      "[Epoch 102/200] [Batch 10/12] [D loss: -0.017022] [G loss: 0.091035]\n",
      "[Epoch 103/200] [Batch 0/12] [D loss: -0.015477] [G loss: 0.069090]\n",
      "[Epoch 103/200] [Batch 5/12] [D loss: -0.016384] [G loss: 0.047225]\n",
      "[Epoch 103/200] [Batch 10/12] [D loss: -0.028068] [G loss: 0.008422]\n",
      "[Epoch 104/200] [Batch 0/12] [D loss: -0.038168] [G loss: -0.016140]\n",
      "[Epoch 104/200] [Batch 5/12] [D loss: -0.001993] [G loss: -0.000992]\n",
      "[Epoch 104/200] [Batch 10/12] [D loss: -0.026630] [G loss: 0.016188]\n",
      "[Epoch 105/200] [Batch 0/12] [D loss: 0.002224] [G loss: 0.040356]\n",
      "[Epoch 105/200] [Batch 5/12] [D loss: -0.033682] [G loss: 0.102963]\n",
      "[Epoch 105/200] [Batch 10/12] [D loss: -0.039600] [G loss: 0.180689]\n",
      "[Epoch 106/200] [Batch 0/12] [D loss: -0.004013] [G loss: 0.172536]\n",
      "[Epoch 106/200] [Batch 5/12] [D loss: -0.021829] [G loss: 0.147708]\n",
      "[Epoch 106/200] [Batch 10/12] [D loss: -0.022401] [G loss: 0.127149]\n",
      "[Epoch 107/200] [Batch 0/12] [D loss: -0.002515] [G loss: 0.099313]\n",
      "[Epoch 107/200] [Batch 5/12] [D loss: -0.008467] [G loss: 0.102739]\n",
      "[Epoch 107/200] [Batch 10/12] [D loss: -0.023173] [G loss: 0.062598]\n",
      "[Epoch 108/200] [Batch 0/12] [D loss: -0.017222] [G loss: 0.045766]\n",
      "[Epoch 108/200] [Batch 5/12] [D loss: -0.016732] [G loss: 0.035892]\n",
      "[Epoch 108/200] [Batch 10/12] [D loss: -0.020993] [G loss: 0.018417]\n",
      "[Epoch 109/200] [Batch 0/12] [D loss: -0.026948] [G loss: 0.021425]\n",
      "[Epoch 109/200] [Batch 5/12] [D loss: 0.020646] [G loss: 0.050948]\n",
      "[Epoch 109/200] [Batch 10/12] [D loss: -0.003370] [G loss: 0.073894]\n",
      "[Epoch 110/200] [Batch 0/12] [D loss: -0.007569] [G loss: 0.083197]\n",
      "[Epoch 110/200] [Batch 5/12] [D loss: 0.012326] [G loss: 0.132905]\n",
      "[Epoch 110/200] [Batch 10/12] [D loss: -0.024723] [G loss: 0.138394]\n",
      "[Epoch 111/200] [Batch 0/12] [D loss: -0.013371] [G loss: 0.131503]\n",
      "[Epoch 111/200] [Batch 5/12] [D loss: -0.000492] [G loss: 0.115584]\n",
      "[Epoch 111/200] [Batch 10/12] [D loss: -0.001419] [G loss: 0.067107]\n",
      "[Epoch 112/200] [Batch 0/12] [D loss: -0.010134] [G loss: 0.052075]\n",
      "[Epoch 112/200] [Batch 5/12] [D loss: -0.025849] [G loss: 0.069160]\n",
      "[Epoch 112/200] [Batch 10/12] [D loss: -0.019552] [G loss: 0.011274]\n",
      "[Epoch 113/200] [Batch 0/12] [D loss: -0.027131] [G loss: 0.011821]\n",
      "[Epoch 113/200] [Batch 5/12] [D loss: 0.001766] [G loss: 0.004562]\n",
      "[Epoch 113/200] [Batch 10/12] [D loss: -0.021134] [G loss: 0.054823]\n",
      "[Epoch 114/200] [Batch 0/12] [D loss: -0.000471] [G loss: 0.079270]\n",
      "[Epoch 114/200] [Batch 5/12] [D loss: -0.024850] [G loss: 0.097072]\n",
      "[Epoch 114/200] [Batch 10/12] [D loss: -0.003976] [G loss: 0.097859]\n",
      "[Epoch 115/200] [Batch 0/12] [D loss: -0.015089] [G loss: 0.102790]\n",
      "[Epoch 115/200] [Batch 5/12] [D loss: -0.005957] [G loss: 0.088310]\n",
      "[Epoch 115/200] [Batch 10/12] [D loss: -0.006555] [G loss: 0.085471]\n",
      "[Epoch 116/200] [Batch 0/12] [D loss: -0.004235] [G loss: 0.072986]\n",
      "[Epoch 116/200] [Batch 5/12] [D loss: -0.006962] [G loss: 0.024368]\n",
      "[Epoch 116/200] [Batch 10/12] [D loss: 0.009689] [G loss: 0.047863]\n",
      "[Epoch 117/200] [Batch 0/12] [D loss: -0.008271] [G loss: 0.040864]\n",
      "[Epoch 117/200] [Batch 5/12] [D loss: -0.010465] [G loss: 0.052105]\n",
      "[Epoch 117/200] [Batch 10/12] [D loss: -0.027089] [G loss: 0.106780]\n",
      "[Epoch 118/200] [Batch 0/12] [D loss: -0.014853] [G loss: 0.129377]\n",
      "[Epoch 118/200] [Batch 5/12] [D loss: 0.004911] [G loss: 0.091359]\n",
      "[Epoch 118/200] [Batch 10/12] [D loss: -0.011838] [G loss: 0.074562]\n",
      "[Epoch 119/200] [Batch 0/12] [D loss: -0.005636] [G loss: 0.053773]\n",
      "[Epoch 119/200] [Batch 5/12] [D loss: -0.000301] [G loss: 0.031140]\n",
      "[Epoch 119/200] [Batch 10/12] [D loss: -0.003839] [G loss: 0.019718]\n",
      "[Epoch 120/200] [Batch 0/12] [D loss: -0.021831] [G loss: 0.014990]\n",
      "[Epoch 120/200] [Batch 5/12] [D loss: -0.022808] [G loss: 0.041356]\n",
      "[Epoch 120/200] [Batch 10/12] [D loss: -0.021292] [G loss: 0.061470]\n",
      "[Epoch 121/200] [Batch 0/12] [D loss: -0.004062] [G loss: 0.061404]\n",
      "[Epoch 121/200] [Batch 5/12] [D loss: -0.012682] [G loss: 0.092699]\n",
      "[Epoch 121/200] [Batch 10/12] [D loss: -0.018243] [G loss: 0.086706]\n",
      "[Epoch 122/200] [Batch 0/12] [D loss: -0.002190] [G loss: 0.093086]\n",
      "[Epoch 122/200] [Batch 5/12] [D loss: 0.006507] [G loss: 0.093207]\n",
      "[Epoch 122/200] [Batch 10/12] [D loss: 0.013634] [G loss: 0.074391]\n",
      "[Epoch 123/200] [Batch 0/12] [D loss: -0.017220] [G loss: 0.050392]\n",
      "[Epoch 123/200] [Batch 5/12] [D loss: -0.023345] [G loss: 0.039509]\n",
      "[Epoch 123/200] [Batch 10/12] [D loss: -0.028736] [G loss: 0.028136]\n",
      "[Epoch 124/200] [Batch 0/12] [D loss: -0.029327] [G loss: 0.055551]\n",
      "[Epoch 124/200] [Batch 5/12] [D loss: -0.010436] [G loss: 0.042619]\n",
      "[Epoch 124/200] [Batch 10/12] [D loss: -0.013098] [G loss: 0.088455]\n",
      "[Epoch 125/200] [Batch 0/12] [D loss: -0.021201] [G loss: 0.063387]\n",
      "[Epoch 125/200] [Batch 5/12] [D loss: -0.029273] [G loss: 0.071185]\n",
      "[Epoch 125/200] [Batch 10/12] [D loss: -0.019930] [G loss: 0.064755]\n",
      "[Epoch 126/200] [Batch 0/12] [D loss: -0.024337] [G loss: 0.062250]\n",
      "[Epoch 126/200] [Batch 5/12] [D loss: -0.019636] [G loss: 0.057362]\n",
      "[Epoch 126/200] [Batch 10/12] [D loss: -0.028171] [G loss: 0.090226]\n",
      "[Epoch 127/200] [Batch 0/12] [D loss: -0.019445] [G loss: 0.083428]\n",
      "[Epoch 127/200] [Batch 5/12] [D loss: -0.005616] [G loss: 0.047022]\n",
      "[Epoch 127/200] [Batch 10/12] [D loss: -0.014335] [G loss: 0.039390]\n",
      "[Epoch 128/200] [Batch 0/12] [D loss: -0.009320] [G loss: 0.040954]\n",
      "[Epoch 128/200] [Batch 5/12] [D loss: -0.025911] [G loss: 0.023800]\n",
      "[Epoch 128/200] [Batch 10/12] [D loss: -0.029805] [G loss: 0.048051]\n",
      "[Epoch 129/200] [Batch 0/12] [D loss: -0.011889] [G loss: 0.052421]\n",
      "[Epoch 129/200] [Batch 5/12] [D loss: -0.007221] [G loss: 0.047672]\n",
      "[Epoch 129/200] [Batch 10/12] [D loss: -0.015100] [G loss: 0.095743]\n",
      "[Epoch 130/200] [Batch 0/12] [D loss: -0.020510] [G loss: 0.068134]\n",
      "[Epoch 130/200] [Batch 5/12] [D loss: -0.015484] [G loss: 0.072028]\n",
      "[Epoch 130/200] [Batch 10/12] [D loss: -0.024928] [G loss: 0.069922]\n",
      "[Epoch 131/200] [Batch 0/12] [D loss: -0.009709] [G loss: 0.066276]\n",
      "[Epoch 131/200] [Batch 5/12] [D loss: -0.006791] [G loss: 0.049283]\n",
      "[Epoch 131/200] [Batch 10/12] [D loss: -0.010544] [G loss: 0.022773]\n",
      "[Epoch 132/200] [Batch 0/12] [D loss: -0.012541] [G loss: 0.008521]\n",
      "[Epoch 132/200] [Batch 5/12] [D loss: -0.020228] [G loss: -0.008556]\n",
      "[Epoch 132/200] [Batch 10/12] [D loss: -0.008701] [G loss: 0.027702]\n",
      "[Epoch 133/200] [Batch 0/12] [D loss: -0.015436] [G loss: 0.037623]\n",
      "[Epoch 133/200] [Batch 5/12] [D loss: -0.019528] [G loss: 0.070924]\n",
      "[Epoch 133/200] [Batch 10/12] [D loss: -0.016851] [G loss: 0.114948]\n",
      "[Epoch 134/200] [Batch 0/12] [D loss: 0.007849] [G loss: 0.095358]\n",
      "[Epoch 134/200] [Batch 5/12] [D loss: -0.025537] [G loss: 0.084828]\n",
      "[Epoch 134/200] [Batch 10/12] [D loss: 0.000685] [G loss: 0.081520]\n",
      "[Epoch 135/200] [Batch 0/12] [D loss: -0.012907] [G loss: 0.077823]\n",
      "[Epoch 135/200] [Batch 5/12] [D loss: -0.017026] [G loss: 0.048128]\n",
      "[Epoch 135/200] [Batch 10/12] [D loss: -0.009871] [G loss: 0.027675]\n",
      "[Epoch 136/200] [Batch 0/12] [D loss: -0.017287] [G loss: 0.030055]\n",
      "[Epoch 136/200] [Batch 5/12] [D loss: -0.018635] [G loss: 0.052986]\n",
      "[Epoch 136/200] [Batch 10/12] [D loss: -0.005897] [G loss: 0.078556]\n",
      "[Epoch 137/200] [Batch 0/12] [D loss: -0.004920] [G loss: 0.075823]\n",
      "[Epoch 137/200] [Batch 5/12] [D loss: -0.029803] [G loss: 0.113686]\n",
      "[Epoch 137/200] [Batch 10/12] [D loss: -0.022184] [G loss: 0.097111]\n",
      "[Epoch 138/200] [Batch 0/12] [D loss: -0.014124] [G loss: 0.102846]\n",
      "[Epoch 138/200] [Batch 5/12] [D loss: -0.017929] [G loss: 0.074477]\n",
      "[Epoch 138/200] [Batch 10/12] [D loss: -0.000692] [G loss: 0.040095]\n",
      "[Epoch 139/200] [Batch 0/12] [D loss: -0.001607] [G loss: 0.045965]\n",
      "[Epoch 139/200] [Batch 5/12] [D loss: -0.006741] [G loss: 0.057208]\n",
      "[Epoch 139/200] [Batch 10/12] [D loss: -0.013241] [G loss: 0.084023]\n",
      "[Epoch 140/200] [Batch 0/12] [D loss: -0.023402] [G loss: 0.069601]\n",
      "[Epoch 140/200] [Batch 5/12] [D loss: -0.012782] [G loss: 0.053317]\n",
      "[Epoch 140/200] [Batch 10/12] [D loss: 0.004306] [G loss: 0.066221]\n",
      "[Epoch 141/200] [Batch 0/12] [D loss: 0.004102] [G loss: 0.052343]\n",
      "[Epoch 141/200] [Batch 5/12] [D loss: -0.019017] [G loss: 0.037219]\n",
      "[Epoch 141/200] [Batch 10/12] [D loss: -0.008105] [G loss: 0.036950]\n",
      "[Epoch 142/200] [Batch 0/12] [D loss: -0.018386] [G loss: 0.036987]\n",
      "[Epoch 142/200] [Batch 5/12] [D loss: -0.008839] [G loss: 0.051961]\n",
      "[Epoch 142/200] [Batch 10/12] [D loss: -0.011232] [G loss: 0.070726]\n",
      "[Epoch 143/200] [Batch 0/12] [D loss: -0.011573] [G loss: 0.072402]\n",
      "[Epoch 143/200] [Batch 5/12] [D loss: -0.013410] [G loss: 0.042534]\n",
      "[Epoch 143/200] [Batch 10/12] [D loss: 0.000105] [G loss: 0.051405]\n",
      "[Epoch 144/200] [Batch 0/12] [D loss: -0.031229] [G loss: 0.053238]\n",
      "[Epoch 144/200] [Batch 5/12] [D loss: -0.011842] [G loss: 0.020271]\n",
      "[Epoch 144/200] [Batch 10/12] [D loss: -0.023216] [G loss: 0.031360]\n",
      "[Epoch 145/200] [Batch 0/12] [D loss: -0.000350] [G loss: 0.024064]\n",
      "[Epoch 145/200] [Batch 5/12] [D loss: -0.015520] [G loss: 0.017024]\n",
      "[Epoch 145/200] [Batch 10/12] [D loss: 0.006748] [G loss: 0.056497]\n",
      "[Epoch 146/200] [Batch 0/12] [D loss: -0.028054] [G loss: 0.047760]\n",
      "[Epoch 146/200] [Batch 5/12] [D loss: -0.021279] [G loss: 0.094137]\n",
      "[Epoch 146/200] [Batch 10/12] [D loss: -0.014056] [G loss: 0.094947]\n",
      "[Epoch 147/200] [Batch 0/12] [D loss: 0.002525] [G loss: 0.084867]\n",
      "[Epoch 147/200] [Batch 5/12] [D loss: -0.019703] [G loss: 0.067326]\n",
      "[Epoch 147/200] [Batch 10/12] [D loss: -0.013176] [G loss: 0.079024]\n",
      "[Epoch 148/200] [Batch 0/12] [D loss: -0.018003] [G loss: 0.075254]\n",
      "[Epoch 148/200] [Batch 5/12] [D loss: -0.004728] [G loss: 0.066903]\n",
      "[Epoch 148/200] [Batch 10/12] [D loss: -0.019866] [G loss: 0.091469]\n",
      "[Epoch 149/200] [Batch 0/12] [D loss: 0.008567] [G loss: 0.082791]\n",
      "[Epoch 149/200] [Batch 5/12] [D loss: 0.007005] [G loss: 0.069389]\n",
      "[Epoch 149/200] [Batch 10/12] [D loss: -0.015240] [G loss: 0.063103]\n",
      "[Epoch 150/200] [Batch 0/12] [D loss: -0.014134] [G loss: 0.074403]\n",
      "[Epoch 150/200] [Batch 5/12] [D loss: -0.023051] [G loss: 0.052459]\n",
      "[Epoch 150/200] [Batch 10/12] [D loss: -0.010373] [G loss: 0.025161]\n",
      "[Epoch 151/200] [Batch 0/12] [D loss: -0.004199] [G loss: 0.046706]\n",
      "[Epoch 151/200] [Batch 5/12] [D loss: -0.008353] [G loss: 0.072903]\n",
      "[Epoch 151/200] [Batch 10/12] [D loss: -0.011543] [G loss: 0.058809]\n",
      "[Epoch 152/200] [Batch 0/12] [D loss: -0.022672] [G loss: 0.068833]\n",
      "[Epoch 152/200] [Batch 5/12] [D loss: -0.001895] [G loss: 0.056008]\n",
      "[Epoch 152/200] [Batch 10/12] [D loss: -0.013420] [G loss: 0.082174]\n",
      "[Epoch 153/200] [Batch 0/12] [D loss: -0.015036] [G loss: 0.079682]\n",
      "[Epoch 153/200] [Batch 5/12] [D loss: -0.009483] [G loss: 0.042111]\n",
      "[Epoch 153/200] [Batch 10/12] [D loss: -0.018064] [G loss: 0.060852]\n",
      "[Epoch 154/200] [Batch 0/12] [D loss: -0.012804] [G loss: 0.051507]\n",
      "[Epoch 154/200] [Batch 5/12] [D loss: -0.008439] [G loss: 0.039981]\n",
      "[Epoch 154/200] [Batch 10/12] [D loss: -0.006772] [G loss: 0.034383]\n",
      "[Epoch 155/200] [Batch 0/12] [D loss: -0.019504] [G loss: 0.030280]\n",
      "[Epoch 155/200] [Batch 5/12] [D loss: -0.007320] [G loss: 0.037733]\n",
      "[Epoch 155/200] [Batch 10/12] [D loss: -0.010511] [G loss: 0.027156]\n",
      "[Epoch 156/200] [Batch 0/12] [D loss: -0.010078] [G loss: 0.039900]\n",
      "[Epoch 156/200] [Batch 5/12] [D loss: -0.003803] [G loss: 0.060462]\n",
      "[Epoch 156/200] [Batch 10/12] [D loss: -0.011966] [G loss: 0.056892]\n",
      "[Epoch 157/200] [Batch 0/12] [D loss: -0.005601] [G loss: 0.051762]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        real_imgs = real_imgs.resize_(imgs.shape[0], opt.channels, opt.img_size,  opt.img_size)\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = discriminator(real_imgs)\n",
    "        fake_loss = discriminator(gen_imgs.detach())\n",
    "        # was距离是要最大化 real_loss - fake_loss\n",
    "        # 这里加上一个符号，loss下降就相当于最大化was距离\n",
    "        d_loss = torch.mean(-real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Clip weights of discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
    "\n",
    "        # Train the generator every n_critic iterations\n",
    "        if i % opt.n_critic == 0:\n",
    "    \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = torch.mean(-discriminator(gen_imgs))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
