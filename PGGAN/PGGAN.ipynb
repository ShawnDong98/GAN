{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timeit\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending Conv2D and Deconv2D layers for equalized learning rate logic\n",
    "class _equalized_conv2d(torch.nn.Module):\n",
    "    \"\"\" conv2d with the concept of equalized learning rate\n",
    "        Args:\n",
    "            :param c_in: input channels\n",
    "            :param c_out:  output channels\n",
    "            :param k_size: kernel size (h, w) should be a tuple or a single integer\n",
    "            :param stride: stride for conv\n",
    "            :param pad: padding\n",
    "            :param bias: whether to use bias or not\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n",
    "        from torch.nn.modules.utils import _pair\n",
    "        from numpy import sqrt, prod\n",
    "\n",
    "        super(_equalized_conv2d, self).__init__()\n",
    "\n",
    "        # define the weight and bias if to be used\n",
    "        self.weight = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(c_out, c_in, *_pair(k_size))))\n",
    "\n",
    "        self.use_bias = bias\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.FloatTensor(c_out).fill_(0))\n",
    "\n",
    "        # prod 计算所有元素的积，可以指定维度\n",
    "        # scale为什么这么算？\n",
    "        fan_in = prod(_pair(k_size)) * c_in\n",
    "        self.scale = sqrt(2) / sqrt(fan_in) \n",
    "\n",
    "    def forward(self, x):\n",
    "        from torch.nn.functional import conv2d\n",
    "\n",
    "        return conv2d(input=x,\n",
    "                      weight=self.weight * self.scale,\n",
    "                      bias=self.bias if self.use_bias else None,\n",
    "                      stride=self.stride,\n",
    "                      padding=self.pad)\n",
    "\n",
    "    # 输出的是各层权重的形状\n",
    "    def extra_repr(self):\n",
    "        return \",\".join(map(str, self.weight.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _equalized_deconv2d(torch.nn.Module):\n",
    "    \"\"\" Transpose convolution using the equalized learning rate\n",
    "        Args:\n",
    "            :param c_in: input channels\n",
    "            :param c_out: output channels\n",
    "            :param k_size: kernel size\n",
    "            :param stride: stride for convolution transpose\n",
    "            :param pad: padding\n",
    "            :param bias: whether to use bias or not\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_out, k_size, stride=1, pad=0, bias=True):\n",
    "        \"\"\" constructor for the class \"\"\"\n",
    "        from torch.nn.modules.utils import _pair\n",
    "        from numpy import sqrt\n",
    "\n",
    "        super(_equalized_deconv2d, self).__init__()\n",
    "\n",
    "        # define the weight and bias if to be used\n",
    "        self.weight = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(c_in, c_out, *_pair(k_size))))\n",
    "\n",
    "        self.use_bias = bias\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.FloatTensor(c_out).fill_(0))\n",
    "\n",
    "        fan_in = c_in  # value of fan_in for deconv\n",
    "        self.scale = sqrt(2) / sqrt(fan_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        from torch.nn.functional import conv_transpose2d\n",
    "\n",
    "        return conv_transpose2d(input=x,\n",
    "                                weight=self.weight * self.scale,\n",
    "                                bias=self.bias if self.use_bias else None,\n",
    "                                stride=self.stride,\n",
    "                                padding=self.pad)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \",\".join(map(str, self.weight.shape))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "# Pixelwise feature vector normalization.\n",
    "# reference: https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L120\n",
    "#----------------------------------------------------------------\n",
    "class PixelwiseNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelwiseNorm, self).__init__()\n",
    "    \n",
    "    def forward(self, x, alpha=1e-8):\n",
    "        # [N1HW], 对三个通道求均值，三个通道压缩成了单个通道\n",
    "        y = x.pow(2.).mean(dim=1, keepdim=True).add(alpha).sqrt()\n",
    "        # 带有广播机制，应该对图片的每个像素都除了一个均值\n",
    "        y = x / y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Layers required for Building The generator and\n",
    "# discriminator\n",
    "# ==========================================================\n",
    "class GenInitialBlock(torch.nn.Module):\n",
    "    \"\"\" Module implementing the initial block of the input \"\"\"\n",
    "    def __init__(self, in_channels, use_eql):\n",
    "        \"\"\"\n",
    "        constructor for the inner class\n",
    "        :param in_channels: number of input channels to the block\n",
    "        :param use_eql: whether to use equalized learning rate\n",
    "        \"\"\"\n",
    "        from torch.nn import LeakyReLU\n",
    "\n",
    "        super(GenInitialBlock, self).__init__()\n",
    "\n",
    "        if use_eql:\n",
    "            self.conv_1 = _equalized_deconv2d(in_channels, in_channels, (4, 4), bias=True)\n",
    "            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (3, 3), pad=1, bias=True)\n",
    "\n",
    "        else:\n",
    "            from torch.nn import Conv2d, ConvTranspose2d\n",
    "            self.conv_1 = ConvTranspose2d(in_channels, in_channels, (4, 4), bias=True)\n",
    "            self.conv_2 = Conv2d(in_channels, in_channels, (3, 3), padding=1, bias=True)\n",
    "\n",
    "        self.pixNorm = PixelwiseNorm()\n",
    "\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在前面加两维度\n",
    "        y = torch.unsqueeze(torch.unsqueeze(x, -1), -1)\n",
    "\n",
    "        # 先升采样再降采样？为什么这样做？\n",
    "        y = self.lrelu(self.conv_1(y))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "\n",
    "        y = self.pixNorm(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class GenGeneralConvBlock(torch.nn.Module):\n",
    "    \"\"\" Module implementing a general convolutional block \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, use_eql):\n",
    "        from torch.nn import LeakyReLU\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        super(GenGeneralConvBlock, self).__init__()\n",
    "\n",
    "        self.upsample = lambda x: interpolate(x, scale_factor=2)\n",
    "\n",
    "        if use_eql:\n",
    "            self.conv_1 = _equalized_conv2d(in_channels, out_channels, (3, 3), pad=1, bias=True)\n",
    "            self.conv_2 = _equalized_conv2d(out_channels, out_channels, (3, 3), pad=1, bias=True)\n",
    "        \n",
    "        else:\n",
    "            from torch.nn import Conv2d\n",
    "            self.conv_1 = Conv2d(in_channels, out_channels, (3, 3), padding=1, bias=True)\n",
    "            self.conv_2 = Conv2d(out_channels, out_channels, (3, 3), padding=1, bias=True)\n",
    "\n",
    "        self.pixNorm = PixelwiseNorm()\n",
    "\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.upsample(x)\n",
    "        y = self.pixNorm(self.lrelu(self.conv_1(y)))\n",
    "        y = self.pixNorm(self.lrelu(self.conv_2(y)))\n",
    "\n",
    "        return y\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, depth=7, latent_size=512, use_eql=True):\n",
    "        \"\"\"\n",
    "        constructor for the Generator class\n",
    "        :param depth: required depth of the Network\n",
    "        :param latent_size: size of the latent manifold\n",
    "        :param use_eql: whether to use equalized learning rate\n",
    "        \"\"\"\n",
    "        from torch.nn import ModuleList\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # 这个trick值得学习，如果是2的n次幂，那么它和自身减一按位与必是0\n",
    "        assert latent_size != 0 and ((latent_size & (latent_size -1))==0), \"latent size not a power of 2\"\n",
    "\n",
    "        if depth >= 4:\n",
    "            assert latent_size >= np.power(2, depth -4), \"latent size will diminish to zero\"\n",
    "\n",
    "        self.use_eql = use_eql\n",
    "        self.depth = depth\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.initial_block = GenInitialBlock(self.latent_size, use_eql=self.use_eql)\n",
    "\n",
    "        self.layers = ModuleList([])\n",
    "\n",
    "        if self.use_eql:\n",
    "            self.toRGB = lambda in_channels: _equalized_conv2d(in_channels, 3, (1, 1), bias=True)\n",
    "\n",
    "        else:\n",
    "            from torch.nn import Conv2d\n",
    "            self.toRGB = lambda in_channels: Conv2d(in_channels, 3, (1, 1), bias=True)\n",
    "\n",
    "        self.rgb_converters = ModuleList([self.toRGB(self.latent_size)])\n",
    "\n",
    "        # create the remaining layers\n",
    "        for i in range(self.depth - 1):\n",
    "            if i <= 2:\n",
    "                layer = GenGeneralConvBlock(self.latent_size, self.latent_size, use_eql=self.use_eql)\n",
    "                rgb = self.toRGB(self.latent_size)\n",
    "\n",
    "            else:\n",
    "                layer = GenGeneralConvBlock(\n",
    "                    int(self.latent_size // np.power(2, i-3)),\n",
    "                    int(self.latent_size // np.power(2, i-2)),\n",
    "                    use_eql = self.use_eql\n",
    "                )\n",
    "                rgb = self.toRGB(int(self.latent_size // np.power(2, i-2)))\n",
    "            self.layers.append(layer)\n",
    "            self.rgb_converters.append(rgb)\n",
    "        \n",
    "        # register the temporary upsampler\n",
    "        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n",
    "\n",
    "    def forward(self, x, depth, alpha):\n",
    "\n",
    "        assert depth < self.depth, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        y = self.initial_block(x)\n",
    "\n",
    "        if depth > 0:\n",
    "            for block in self.layers[:depth-1]:\n",
    "                y = block(y)\n",
    "            \n",
    "            # 旧层\n",
    "            residual = self.rgb_converters[depth-1](self.temporaryUpsampler(y))\n",
    "            # 新层\n",
    "            straight = self.rgb_converters[depth](self.layers[depth - 1](y))\n",
    "\n",
    "            out = (alpha * straight) + ((1 - alpha) * residual)\n",
    "        else:\n",
    "            out = self.rgb_converters[0](y)\n",
    "\n",
    "        return out\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStdDev(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Minibatch standard deviation layer for the discriminator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MinibatchStdDev, self).__init__()\n",
    "    \n",
    "    def forward(self, x, alpha=1e-8):\n",
    "        \"\"\"\n",
    "        forward pass of the layer\n",
    "        :param x: input activation volume\n",
    "        :param alpha: small number for numerical stability\n",
    "        :return: y => x appended with standard deviation constant map\n",
    "        \"\"\"\n",
    "        batch_size, _, height. width = x.shape\n",
    "\n",
    "        # [B x C x H x W] Subtract mean over batch.\n",
    "        y = x - x.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # [1 x C x H x W]  Calc standard deviation over batch\n",
    "        # y.pow: 方差 --> sqrt: 标准差 --> 对batch求均值，Minibatch的标准差\n",
    "        y = torch.sqrt(y.pow(2.).mean(dim=0, keepdim=False) + alpha)\n",
    "\n",
    "        # 所有batch，所有通道，所有像素点的平均标准差\n",
    "        y = y.mean().view(1, 1, 1, 1)\n",
    "\n",
    "        # 将上面求得的标准差广播成[B x 1 x H x W]\n",
    "        y = y.repeat(batch_size, 1, height, width)\n",
    "\n",
    "        # 在通道那一维拼接x和y\n",
    "        y = torch.cat([x, y], 1)\n",
    "\n",
    "        return y\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisFinalBlock(torch.nn.Module):\n",
    "    \"\"\" Final block for the Discriminator \"\"\"\n",
    "    def __init__(self, in_channels, use_eql):\n",
    "        from torch.nn import LeakyReLU\n",
    "\n",
    "        super(DisFinalBlock, self).__init__()\n",
    "\n",
    "        # declare the required modules for forward pass\n",
    "        self.batch_discriminator = MinibatchStdDev()\n",
    "\n",
    "        if use_eql:\n",
    "            self.conv_1 = _equalized_conv2d(in_channels + 1, in_channels, (3, 3), pad=1, bias=True)\n",
    "            self.conv_2 = _equalized_conv2d(in_channels, in_channels, (4, 4), bias=True)\n",
    "            # final conv layer emulates a fully connected layer\n",
    "            self.conv_3 = _equalized_conv2d(in_channels, 1, (1, 1), bias=True)\n",
    "        else:\n",
    "            from torch.nn import _equalized_conv2d\n",
    "            self.conv_1 = Conv2d(in_channels + 1, in_channels, (3, 3), padding=1, bias=True)\n",
    "            self.conv_2 = Conv2d(in_channels, in_channels, (4, 4), bias=True)\n",
    "            # final conv layer emulates a fully connected layer\n",
    "            self.conv_3 = Conv2d(in_channels, 1, (1, 1), bias=True)\n",
    "\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # minibatch_std_dev layer\n",
    "        y = self.batch_discriminator(x)\n",
    "\n",
    "        y = self.lrelu(self.conv_1(y))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "\n",
    "        # fully connected layer\n",
    "        # 原文中用的全连接层，这里用1x1卷积+view替代了\n",
    "        y = self.conv_3(y)\n",
    "\n",
    "        # flatten the output raw discriminator scores\n",
    "        return y.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisGeneralConvBlock(torch.nn.Module):\n",
    "    \"\"\" General block in the discriminator  \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_eql):\n",
    "        from torch.nn import AvgPool2d, LeakyReLU\n",
    "\n",
    "        super(DisGeneralConvBlock, self).__init__()\n",
    "\n",
    "        if use_eql:\n",
    "            self.conv_1 = _equalized_conv2d(in_channels, in_channels, (3, 3), pad=1, bias=True)\n",
    "            self.conv_2 = _equalized_conv2d(in_channels, out_channels, (3, 3), pad=1, bias=True)\n",
    "        \n",
    "        else:\n",
    "            from torch.nn import _equalized_conv2d\n",
    "            self.conv_1 = Conv2d(in_channels, in_channels, (3, 3), padding=1, bias=True)\n",
    "            self.conv_2 = Conv2d(in_channels, out_channels, (3, 3), padding=1, bias=True)\n",
    "\n",
    "        self.downSampler = AvgPool2d(2)\n",
    "\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.lrelu(self.conv_1(x))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "        y = self.downSampler(y)\n",
    "\n",
    "        return y\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# Discriminator Module\n",
    "# can be used with ProGAN or standalone (for inference).\n",
    "# Note this cannot be used with ConditionalProGAN\n",
    "#================================================================\n",
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\" Discriminator of the GAN \"\"\"\n",
    "    def __init__(self, height=7, feature_size=512, use_eql=True):\n",
    "        \"\"\"\n",
    "        constructor for the class\n",
    "        :param height: total height of the discriminator (Must be equal to the Generator depth)\n",
    "        :param feature_size: size of the deepest features extracted\n",
    "                             (Must be equal to Generator latent_size)\n",
    "        :param use_eql: whether to use equalized learning rate\n",
    "        \"\"\"\n",
    "        from torch.nn import ModuleList, AvgPool2d\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        assert feature_size != 0 and ((feature_size & feature_size - 1) == 0), \"feature size not a power of 2\"\n",
    "        if height >= 4:\n",
    "            assert feature_size >= np.power(2, height-4), \"feature size cannot be produced\"\n",
    "\n",
    "        self.use_eql = use_eql\n",
    "        self.height = height\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.final_block = DisFinalBlock(self.feature_size, use_eql=self.use_eql)\n",
    "\n",
    "        self.layers = ModuleList([])\n",
    "\n",
    "        if self.use_eql:\n",
    "            self.fromRGB = lambda out_channels: _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n",
    "        else:\n",
    "            from torch.nn import _equalized_conv2d\n",
    "            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n",
    "\n",
    "        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n",
    "\n",
    "        # create the remaining layers\n",
    "        for i in range(self.height - 1):\n",
    "            if i > 2:\n",
    "                layer = DisGeneralConvBlock(\n",
    "                    int(self.feature_size // np.power(2, i-2)),\n",
    "                    int(self.feature_size // np.power(2, i-3)),\n",
    "                    use_eql = self.use_eql\n",
    "                )\n",
    "                rgb = self.fromRGB(int(self.feature_size // np.power(2, i-2)))\n",
    "            else:\n",
    "                layer = DisGeneralConvBlock(self.feature_size, \n",
    "                                            self.feature_size,\n",
    "                                            use_eql=self.use_eql)\n",
    "                rgb - self.fromRGB(self.feature_size)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "            self.rgb_to_features.append(rgb)\n",
    "\n",
    "        self.temporaryDownsampler = AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x, height, alpha):\n",
    "        assert height < self.height, \"Requested output depth cannot be produced\"\n",
    "\n",
    "        if height > 0:\n",
    "            # 旧层\n",
    "            residual = self.rgb_to_features[height-1](self.temporaryDownsampler(x))\n",
    "            # 新层\n",
    "            straight = self.layer[height-1](self.rgb_to_features[height](x))\n",
    "\n",
    "            for block in reversed(self.layers[:height - 1]):\n",
    "                y = block(y)\n",
    "        else:\n",
    "            y = self.rgb_to_features[0](x)\n",
    "\n",
    "        out = self.final_block(y)\n",
    "\n",
    "        return out\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the Exponential moving averages for the Generator weights\n",
    "# This function updates the exponential average weights based on the current training\n",
    "def update_average(model_tgt, model_src, beta):\n",
    "\n",
    "    # utility function for toggling the gradient requirements of the models\n",
    "    def toggle_grad(model, requires_grad):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(requires_grad)\n",
    "\n",
    "    # turn off gradient calculation\n",
    "    toggle_grad(model_tgt, False)\n",
    "    toggle_grad(model_src, False)\n",
    "\n",
    "    param_dict_src = dict(model_src.named_parameters())\n",
    "\n",
    "    for p_name, p_tgt in model_tgt.named_parameters():\n",
    "        p_src = param_dict_src[p_name]\n",
    "        assert (p_src is not p_tgt)\n",
    "        p_tgt.copy_(beta * p_tgt + (1. - beta) * p_src)\n",
    "\n",
    "    toggle_grad(model_tgt, True)\n",
    "    toggle_grad(model_src, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss:\n",
    "    \"\"\" Base class for all losses\n",
    "        @args:\n",
    "            dis: Discriminator used for calculating the loss\n",
    "                 Note this must be a part of the GAN framework\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dis):\n",
    "        self.dis = dis\n",
    "\n",
    "    def dis_loss(self, real_samps, fake_samps, height, alpha):\n",
    "        raise NotImplementedError(\"dis_loss method has not been implemented\")\n",
    "\n",
    "\n",
    "    def gen_loss(self, real_samps, fake_samps, height, alpha):\n",
    "        raise NotImplementedError(\"gen_loss method has not been implemented\")\n",
    "\n",
    "class WGAN_GP(GANLoss):\n",
    "    \n",
    "    def __init__(self, dis, drift=0.001, use_gp=False):\n",
    "        # 在python3中super().xxx相当于super(Class, self).xxx\n",
    "        super().__init__(dis)\n",
    "\n",
    "        self.drift = drift\n",
    "        self.use_gp = use_gp\n",
    "\n",
    "    def __gradient_penalty(self, real_samps, fake_samps, height, alpha, reg_lambda=10):\n",
    "\n",
    "        batch_size = real_samps.shape[0]\n",
    "\n",
    "        epsilon = torch.rand((batch_size, 1, 1 ,1)).to(fake_samps.device)\n",
    "\n",
    "        merged = epsilon * real_samps + ((1 - epsilon) * fake_samps)\n",
    "        merged.requires_grad_(True)\n",
    "\n",
    "        op = self.dis(merged, height, alpha)\n",
    "\n",
    "        gradient = torch.autograd.grad(\n",
    "                                outputs=op, \n",
    "                                inputs = merged,\n",
    "                                grad_outputs=torch.ones_like(op),\n",
    "                                create_graph=True,\n",
    "                                retain_graph=True,\n",
    "                                only_inputs=True)[0]\n",
    "        \n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        # 对第二维的梯度求L2范数\n",
    "        penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def dis_loss(self, real_samps, fake_samps, height, alpha):\n",
    "        \n",
    "        fake_out = self.dis(fake_samps, height, alpha)\n",
    "        real_out = self.dis(real_samps, height, alpha)\n",
    "\n",
    "        # 我们的目的是real和fake越相近越好，也就是real - fake越小越好\n",
    "        # 而Discriminator的目的是区分real和fake，也就是real - fake越大越好\n",
    "        # 也就是fake-real越小越好\n",
    "        loss = (torch.mean(fake_out) - torch.mean(real_out) + (self.drift * torch.mean(real_out ** 2)))\n",
    "\n",
    "        if self.use_gp:\n",
    "            # gradient penalty使得梯度稳在1-Lipschitz范数\n",
    "            gp = slef.__gradient_penalty(real_samps, fake_samps, height, alpha)\n",
    "            loss += gp\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def gen_loss(self, real_samps, fake_samps, height, alpha):\n",
    "        \n",
    "        loss = -torch.mean(self.dis(fake_samps, height, alpha))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(new_size=None):\n",
    "    \"\"\"\n",
    "    obtain the image transforms required for the input data\n",
    "    :param new_size: size of the resized images\n",
    "    :return: image_transform => transform object from TorchVision\n",
    "    \"\"\"\n",
    "    from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "\n",
    "    if new_size is not None:\n",
    "        image_transform = Compose([\n",
    "            Resize(new_size),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        image_transform = Compose([\n",
    "            ToTensor(),\n",
    "            Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    return image_transform\n",
    "\n",
    "def get_data_loader(dataset, batch_size, num_workers):\n",
    "    \"\"\"\n",
    "    generate the data_loader from the given dataset\n",
    "    :param dataset: dataset for training (Should be a PyTorch dataset)\n",
    "                    Make sure every item is an Image\n",
    "    :param batch_size: batch size of the data\n",
    "    :param num_workers: num of parallel readers\n",
    "    :return: dl => dataloader for the dataset\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    dl = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#================================================================\n",
    "# ProGAN Module (Unconditional)\n",
    "#================================================================\n",
    "class ProGAN:\n",
    "    \"\"\" Wrapper around the Generator and the Discriminator \"\"\"\n",
    "\n",
    "    def __init__(self, depth=7, latent_size=512, learning_rate=0.001, beta_1=0, beta_2=0.99, eps=1e-8, drift=0.001, n_critic=1, use_eql=True, loss=\"wgan-gp\", use_ema=True, ema_decay=0.999, device=torch.device(\"cuda\")):\n",
    "        from torch.optim import Adam\n",
    "        from torch.nn import DataParallel\n",
    "\n",
    "        self.gen = Generator(depth, latent_size, use_eql=use_eql).to(device)\n",
    "        self.dis = Discriminator(depth, latent_size, use_eql=use_eql).to(device)\n",
    "\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            self.gen = DataParallel(self.gen)\n",
    "            self.dis = DataParallel(self.dis)\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.depth = depth\n",
    "        self.use_ema = use_ema\n",
    "        self.ema_decay = ema_decay\n",
    "        self.n_critic = n_critic\n",
    "        self.use_eql = use_eql\n",
    "        self.device = device\n",
    "        self.drift = drift\n",
    "\n",
    "        self.gen_optim = Adam(self.gen.parameters(), lr=learning_rate, betas=(beta_1, beta_2), eps=eps)\n",
    "\n",
    "        self.loss = self.__setup_loss(loss)\n",
    "\n",
    "        if self.use_ema:\n",
    "            \n",
    "            # create a shadow copy of the generator\n",
    "            self.gen_shadow = copy.deepcopy(self.gen)\n",
    "\n",
    "            self.ema_updater = update_average\n",
    "\n",
    "            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n",
    "\n",
    "    def __setup_loss(self, loss):\n",
    "        \n",
    "        loss = WGAN_GP(self.dis, self.drift, use_gp=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __progressive_downsampling(self, real_batch, depth, alpha):\n",
    "        from torch.nn import AvgPool2d\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        # downsample the real_batch for the given depth\n",
    "        down_sample_factor = int(np.power(2, self.depth  - depth -1))\n",
    "        prior_downsample_factor = max(int(np.power(2, self.depth - depth)), 0)\n",
    "\n",
    "        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n",
    "\n",
    "        if depth > 0:\n",
    "            prior_ds_real_samples = interpolate(AvgPool2d(prior_downsample_factor))\n",
    "\n",
    "        else:\n",
    "            prior_ds_real_samples = ds_real_samples\n",
    "\n",
    "        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n",
    "        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n",
    "\n",
    "        return real_samples\n",
    "\n",
    "\n",
    "    def optimize_discriminator(self, noise, real_batch, depth, alpha):\n",
    "        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n",
    "\n",
    "        loss_val = 0\n",
    "        for _ in range(self.n_critic):\n",
    "\n",
    "            # generate a batch of samples\n",
    "            fake_samples = self.gen(noise, depth, alpha).detach()\n",
    "\n",
    "            loss = self.loss.dis_loss(real_samples, fake_samples, depth, alpha)\n",
    "\n",
    "            # optimize discriminator\n",
    "            self.dis_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.dis_optim.step()\n",
    "\n",
    "            loss_val += loss.item()\n",
    "\n",
    "        return loss_val / self.n_critic\n",
    "\n",
    "    \n",
    "    def optimize_generator(self, noise, real_batch, depth, alpha):\n",
    "\n",
    "        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n",
    "\n",
    "        # generate fake samples:\n",
    "        fake_samples = self.gen(noise, depth, alpha)\n",
    "\n",
    "        loss = self.loss.gen_loss(real_samples, fake_samples, depth, alpha)\n",
    "\n",
    "        # optimize the generator\n",
    "        self.gen_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.gen_optim.step()\n",
    "\n",
    "        # if use_ema is true, apply ema to the generator parameters\n",
    "        if self.use_ema:\n",
    "            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n",
    "\n",
    "        # return the loss value\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def create_grid(samples, scale_factor, img_file):\n",
    "\n",
    "        from torchvision.utils import save_image\n",
    "        from torch.nn.functional import interpolate\n",
    "\n",
    "        # upsample the image\n",
    "        if scale_factor > 1:\n",
    "            samples = interpolate(samples, scale_factor=scale_factor)\n",
    "\n",
    "        # save the images:\n",
    "        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))), normalize=True, scale_each=True)\n",
    "\n",
    "    # def train(self, dataset, epochs, batch_sizes,\n",
    "    #           fade_in_percentage, num_samples=16,\n",
    "    #           start_depth=0, num_workers=3, feedback_factor=100,\n",
    "    #           log_dir=\"./models/\", sample_dir=\"./samples/\", save_dir=\"./models/\",\n",
    "    #           checkpoint_factor=1):\n",
    "\n",
    "    #     assert self.depth == len(batch_sizes), \"batch_sizes not compatible with depth\"\n",
    "\n",
    "    #     # turn the generator and discriminator into train mode\n",
    "    #     self.gen.train()\n",
    "    #     self.dis.train()\n",
    "    #     if self.use_ema:\n",
    "    #         self.gen_shadow.train()\n",
    "\n",
    "    #     # create a global time counter\n",
    "    #     global_time = time.time()\n",
    "\n",
    "    #     # create fixed_input for debugging\n",
    "    #     fixed_input = torch.randn(num_samples, self.latent_size).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.backends import cudnn\n",
    "from torch.nn.functional import interpolate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on the fast GPU processing mode on\n",
    "cudnn.benchmark = True\n",
    "# define the device for the training script\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"\n",
    "    default command line argument parser\n",
    "    :return: args => parsed command line arguments\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--generator_file\", \n",
    "                        type=str, \n",
    "                        default=\"GAN_GEN_8.pth\",\n",
    "                        help=\"pretrained weights file for generator\")\n",
    "\n",
    "    parser.add_argument(\"--latent_size\",\n",
    "                        type=int,\n",
    "                        default=512,\n",
    "                        help=\"latent size for the generator\")\n",
    "\n",
    "    parser.add_argument(\"--depth\", action=\"store\",\n",
    "                        default=9,\n",
    "                        help=\"depth of the network. **Starts from 1\")\n",
    "\n",
    "    parser.add_argument(\"--out_depth\", \n",
    "                        type=int,\n",
    "                        default=6,\n",
    "                        help=\"output depth of images. **Starts from 0\")\n",
    "\n",
    "    parser.add_argument(\"--num_samples\",\n",
    "                        type=int,\n",
    "                        default=300,\n",
    "                        help=\"number of synchronized grids to be generated\")\n",
    "\n",
    "    parser.add_argument(\"--out_dir\", \n",
    "                        type=str,\n",
    "                        default=\"interp_animation_frames/\",\n",
    "                        help=\"path to the output directory for the frames\")\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):\n",
    "    \"\"\"\n",
    "    adjust the dynamic colour range of the given input data\n",
    "    :param data: input image data\n",
    "    :param drange_in: original range of input\n",
    "    :param drange_out: required range of output\n",
    "    :return: img => colour range adjusted images\n",
    "    \"\"\"\n",
    "    if drange_in != drange_out:\n",
    "        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (np.float32(drange_in[1]) - np.float32(drange_in[0]))\n",
    "        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n",
    "        data = data * scale + bias\n",
    "    return torch.clamp(data, min=0, max=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# loader使用torchvision中自带的transforms函数\n",
    "loader = transforms.Compose([\n",
    "    transforms.ToTensor()])  \n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def save_image(tensor, epoch):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)  # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    image.save('save_pic/epoch_%d.png'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating generator object ...\n  0%|          | 0/300 [00:00<?, ?it/s]Loading the generator weights from: GAN_GEN_8.pth\nGenerating scale synchronized images ...\ntorch.Size([3, 256, 256])\n  0%|          | 0/300 [00:04<?, ?it/s]Generated 300 images at interp_animation_frames/\n\n"
    }
   ],
   "source": [
    "args = parse_arguments()\n",
    "print(\"Creating generator object ...\")\n",
    "# create the generator object\n",
    "gen = torch.nn.DataParallel(Generator(\n",
    "    depth=args.depth,\n",
    "    latent_size=args.latent_size\n",
    "))\n",
    "\n",
    "print(\"Loading the generator weights from:\", args.generator_file)\n",
    "# load the weights into it\n",
    "gen.load_state_dict(\n",
    "    torch.load(args.generator_file, map_location=str(device))\n",
    ")\n",
    "\n",
    "#path for saving the files:\n",
    "save_path = args.out_dir\n",
    "\n",
    "print(\"Generating scale synchronized images ...\")\n",
    "for img_num in tqdm(range(1, args.num_samples + 1)):\n",
    "    # generate the images:\n",
    "    with torch.no_grad():\n",
    "        point = torch.randn(1, args.latent_size)\n",
    "        point = (point / point.norm()) * (args.latent_size ** 0.5)\n",
    "        ss_image = gen(point, depth=args.out_depth, alpha=1)\n",
    "        # color adjust the generated image:\n",
    "        ss_image = adjust_dynamic_range(ss_image)\n",
    "\n",
    "    #save the ss_image in the directory\n",
    "    save_image(ss_image, img_num)\n",
    "    break\n",
    "print(\"Generated %d images at %s\" % (args.num_samples, save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}