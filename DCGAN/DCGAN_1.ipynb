{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ShawnDong98/GAN/blob/master/DCGAN/DCGAN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=3, img_size=32, latent_dim=100, lr=0.0002, n_cpu=8, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "opt = parser.parse_known_args()[0]\n",
    "print(opt)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        \"./data/\",\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# i, (img, _) = next(enumerate(dataloader))\n",
    "# print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/3166] [D loss: 0.693269] [G loss: 0.694843]\n",
      "[Epoch 0/200] [Batch 1/3166] [D loss: 0.693231] [G loss: 0.694094]\n",
      "[Epoch 0/200] [Batch 2/3166] [D loss: 0.693060] [G loss: 0.693530]\n",
      "[Epoch 0/200] [Batch 3/3166] [D loss: 0.693142] [G loss: 0.693121]\n",
      "[Epoch 0/200] [Batch 4/3166] [D loss: 0.693187] [G loss: 0.692884]\n",
      "[Epoch 0/200] [Batch 5/3166] [D loss: 0.693188] [G loss: 0.692865]\n",
      "[Epoch 0/200] [Batch 6/3166] [D loss: 0.693213] [G loss: 0.692997]\n",
      "[Epoch 0/200] [Batch 7/3166] [D loss: 0.693232] [G loss: 0.693146]\n",
      "[Epoch 0/200] [Batch 8/3166] [D loss: 0.693115] [G loss: 0.693117]\n",
      "[Epoch 0/200] [Batch 9/3166] [D loss: 0.693151] [G loss: 0.693221]\n",
      "[Epoch 0/200] [Batch 10/3166] [D loss: 0.693236] [G loss: 0.693226]\n",
      "[Epoch 0/200] [Batch 11/3166] [D loss: 0.693045] [G loss: 0.693179]\n",
      "[Epoch 0/200] [Batch 12/3166] [D loss: 0.693031] [G loss: 0.693291]\n",
      "[Epoch 0/200] [Batch 13/3166] [D loss: 0.693039] [G loss: 0.693179]\n",
      "[Epoch 0/200] [Batch 14/3166] [D loss: 0.693038] [G loss: 0.693155]\n",
      "[Epoch 0/200] [Batch 15/3166] [D loss: 0.692975] [G loss: 0.693078]\n",
      "[Epoch 0/200] [Batch 16/3166] [D loss: 0.693107] [G loss: 0.693204]\n",
      "[Epoch 0/200] [Batch 17/3166] [D loss: 0.692990] [G loss: 0.693163]\n",
      "[Epoch 0/200] [Batch 18/3166] [D loss: 0.693138] [G loss: 0.693350]\n",
      "[Epoch 0/200] [Batch 19/3166] [D loss: 0.692978] [G loss: 0.693370]\n",
      "[Epoch 0/200] [Batch 20/3166] [D loss: 0.692963] [G loss: 0.693380]\n",
      "[Epoch 0/200] [Batch 21/3166] [D loss: 0.693312] [G loss: 0.693128]\n",
      "[Epoch 0/200] [Batch 22/3166] [D loss: 0.692993] [G loss: 0.693088]\n",
      "[Epoch 0/200] [Batch 23/3166] [D loss: 0.693055] [G loss: 0.693086]\n",
      "[Epoch 0/200] [Batch 24/3166] [D loss: 0.693113] [G loss: 0.693024]\n",
      "[Epoch 0/200] [Batch 25/3166] [D loss: 0.693101] [G loss: 0.693102]\n",
      "[Epoch 0/200] [Batch 26/3166] [D loss: 0.693269] [G loss: 0.693240]\n",
      "[Epoch 0/200] [Batch 27/3166] [D loss: 0.693036] [G loss: 0.693224]\n",
      "[Epoch 0/200] [Batch 28/3166] [D loss: 0.693232] [G loss: 0.693195]\n",
      "[Epoch 0/200] [Batch 29/3166] [D loss: 0.693136] [G loss: 0.693410]\n",
      "[Epoch 0/200] [Batch 30/3166] [D loss: 0.693244] [G loss: 0.693315]\n",
      "[Epoch 0/200] [Batch 31/3166] [D loss: 0.693304] [G loss: 0.692947]\n",
      "[Epoch 0/200] [Batch 32/3166] [D loss: 0.692987] [G loss: 0.693131]\n",
      "[Epoch 0/200] [Batch 33/3166] [D loss: 0.692936] [G loss: 0.693336]\n",
      "[Epoch 0/200] [Batch 34/3166] [D loss: 0.693053] [G loss: 0.693532]\n",
      "[Epoch 0/200] [Batch 35/3166] [D loss: 0.693033] [G loss: 0.693673]\n",
      "[Epoch 0/200] [Batch 36/3166] [D loss: 0.692946] [G loss: 0.693678]\n",
      "[Epoch 0/200] [Batch 37/3166] [D loss: 0.692703] [G loss: 0.694088]\n",
      "[Epoch 0/200] [Batch 38/3166] [D loss: 0.692776] [G loss: 0.693736]\n",
      "[Epoch 0/200] [Batch 39/3166] [D loss: 0.692347] [G loss: 0.694334]\n",
      "[Epoch 0/200] [Batch 40/3166] [D loss: 0.692939] [G loss: 0.693369]\n",
      "[Epoch 0/200] [Batch 41/3166] [D loss: 0.692422] [G loss: 0.693806]\n",
      "[Epoch 0/200] [Batch 42/3166] [D loss: 0.693315] [G loss: 0.692660]\n",
      "[Epoch 0/200] [Batch 43/3166] [D loss: 0.693004] [G loss: 0.693855]\n",
      "[Epoch 0/200] [Batch 44/3166] [D loss: 0.693012] [G loss: 0.692903]\n",
      "[Epoch 0/200] [Batch 45/3166] [D loss: 0.693205] [G loss: 0.692987]\n",
      "[Epoch 0/200] [Batch 46/3166] [D loss: 0.693101] [G loss: 0.693158]\n",
      "[Epoch 0/200] [Batch 47/3166] [D loss: 0.693240] [G loss: 0.692947]\n",
      "[Epoch 0/200] [Batch 48/3166] [D loss: 0.693042] [G loss: 0.692797]\n",
      "[Epoch 0/200] [Batch 49/3166] [D loss: 0.692707] [G loss: 0.693616]\n",
      "[Epoch 0/200] [Batch 50/3166] [D loss: 0.692351] [G loss: 0.694363]\n",
      "[Epoch 0/200] [Batch 51/3166] [D loss: 0.691817] [G loss: 0.695251]\n",
      "[Epoch 0/200] [Batch 52/3166] [D loss: 0.692350] [G loss: 0.692844]\n",
      "[Epoch 0/200] [Batch 53/3166] [D loss: 0.691727] [G loss: 0.693430]\n",
      "[Epoch 0/200] [Batch 54/3166] [D loss: 0.691933] [G loss: 0.693458]\n",
      "[Epoch 0/200] [Batch 55/3166] [D loss: 0.691406] [G loss: 0.695202]\n",
      "[Epoch 0/200] [Batch 56/3166] [D loss: 0.691184] [G loss: 0.696527]\n",
      "[Epoch 0/200] [Batch 57/3166] [D loss: 0.690938] [G loss: 0.693732]\n",
      "[Epoch 0/200] [Batch 58/3166] [D loss: 0.690811] [G loss: 0.694632]\n",
      "[Epoch 0/200] [Batch 59/3166] [D loss: 0.693535] [G loss: 0.692534]\n",
      "[Epoch 0/200] [Batch 60/3166] [D loss: 0.691270] [G loss: 0.692002]\n",
      "[Epoch 0/200] [Batch 61/3166] [D loss: 0.690817] [G loss: 0.695527]\n",
      "[Epoch 0/200] [Batch 62/3166] [D loss: 0.691463] [G loss: 0.696137]\n",
      "[Epoch 0/200] [Batch 63/3166] [D loss: 0.690789] [G loss: 0.695883]\n",
      "[Epoch 0/200] [Batch 64/3166] [D loss: 0.690451] [G loss: 0.695847]\n",
      "[Epoch 0/200] [Batch 65/3166] [D loss: 0.692158] [G loss: 0.697737]\n",
      "[Epoch 0/200] [Batch 66/3166] [D loss: 0.692586] [G loss: 0.698784]\n",
      "[Epoch 0/200] [Batch 67/3166] [D loss: 0.690826] [G loss: 0.696724]\n",
      "[Epoch 0/200] [Batch 68/3166] [D loss: 0.690089] [G loss: 0.699953]\n",
      "[Epoch 0/200] [Batch 69/3166] [D loss: 0.692006] [G loss: 0.690851]\n",
      "[Epoch 0/200] [Batch 70/3166] [D loss: 0.693325] [G loss: 0.691123]\n",
      "[Epoch 0/200] [Batch 71/3166] [D loss: 0.695729] [G loss: 0.685237]\n",
      "[Epoch 0/200] [Batch 72/3166] [D loss: 0.698439] [G loss: 0.675610]\n",
      "[Epoch 0/200] [Batch 73/3166] [D loss: 0.706673] [G loss: 0.674178]\n",
      "[Epoch 0/200] [Batch 74/3166] [D loss: 0.701763] [G loss: 0.683899]\n",
      "[Epoch 0/200] [Batch 75/3166] [D loss: 0.699865] [G loss: 0.688398]\n",
      "[Epoch 0/200] [Batch 76/3166] [D loss: 0.696259] [G loss: 0.694274]\n",
      "[Epoch 0/200] [Batch 77/3166] [D loss: 0.695237] [G loss: 0.698195]\n",
      "[Epoch 0/200] [Batch 78/3166] [D loss: 0.692265] [G loss: 0.699893]\n",
      "[Epoch 0/200] [Batch 79/3166] [D loss: 0.689234] [G loss: 0.703577]\n",
      "[Epoch 0/200] [Batch 80/3166] [D loss: 0.686617] [G loss: 0.703730]\n",
      "[Epoch 0/200] [Batch 81/3166] [D loss: 0.684589] [G loss: 0.706558]\n",
      "[Epoch 0/200] [Batch 82/3166] [D loss: 0.684372] [G loss: 0.706297]\n",
      "[Epoch 0/200] [Batch 83/3166] [D loss: 0.679980] [G loss: 0.709378]\n",
      "[Epoch 0/200] [Batch 84/3166] [D loss: 0.684695] [G loss: 0.712236]\n",
      "[Epoch 0/200] [Batch 85/3166] [D loss: 0.679078] [G loss: 0.696615]\n",
      "[Epoch 0/200] [Batch 86/3166] [D loss: 0.680658] [G loss: 0.692930]\n",
      "[Epoch 0/200] [Batch 87/3166] [D loss: 0.682885] [G loss: 0.679876]\n",
      "[Epoch 0/200] [Batch 88/3166] [D loss: 0.697861] [G loss: 0.663706]\n",
      "[Epoch 0/200] [Batch 89/3166] [D loss: 0.706533] [G loss: 0.658828]\n",
      "[Epoch 0/200] [Batch 90/3166] [D loss: 0.712160] [G loss: 0.656130]\n",
      "[Epoch 0/200] [Batch 91/3166] [D loss: 0.695545] [G loss: 0.677790]\n",
      "[Epoch 0/200] [Batch 92/3166] [D loss: 0.699979] [G loss: 0.671183]\n",
      "[Epoch 0/200] [Batch 93/3166] [D loss: 0.702195] [G loss: 0.674138]\n",
      "[Epoch 0/200] [Batch 94/3166] [D loss: 0.698174] [G loss: 0.688010]\n",
      "[Epoch 0/200] [Batch 95/3166] [D loss: 0.692496] [G loss: 0.691681]\n",
      "[Epoch 0/200] [Batch 96/3166] [D loss: 0.691756] [G loss: 0.701443]\n",
      "[Epoch 0/200] [Batch 97/3166] [D loss: 0.692160] [G loss: 0.704739]\n",
      "[Epoch 0/200] [Batch 98/3166] [D loss: 0.687262] [G loss: 0.713391]\n",
      "[Epoch 0/200] [Batch 99/3166] [D loss: 0.686308] [G loss: 0.717523]\n",
      "[Epoch 0/200] [Batch 100/3166] [D loss: 0.683789] [G loss: 0.724507]\n",
      "[Epoch 0/200] [Batch 101/3166] [D loss: 0.677053] [G loss: 0.732066]\n",
      "[Epoch 0/200] [Batch 102/3166] [D loss: 0.669408] [G loss: 0.748428]\n",
      "[Epoch 0/200] [Batch 103/3166] [D loss: 0.670381] [G loss: 0.738860]\n",
      "[Epoch 0/200] [Batch 104/3166] [D loss: 0.666486] [G loss: 0.767660]\n",
      "[Epoch 0/200] [Batch 105/3166] [D loss: 0.659204] [G loss: 0.755848]\n",
      "[Epoch 0/200] [Batch 106/3166] [D loss: 0.665533] [G loss: 0.747315]\n",
      "[Epoch 0/200] [Batch 107/3166] [D loss: 0.669651] [G loss: 0.730792]\n",
      "[Epoch 0/200] [Batch 108/3166] [D loss: 0.680242] [G loss: 0.683510]\n",
      "[Epoch 0/200] [Batch 109/3166] [D loss: 0.690793] [G loss: 0.676337]\n",
      "[Epoch 0/200] [Batch 110/3166] [D loss: 0.742072] [G loss: 0.599684]\n",
      "[Epoch 0/200] [Batch 111/3166] [D loss: 0.737940] [G loss: 0.608483]\n",
      "[Epoch 0/200] [Batch 112/3166] [D loss: 0.710608] [G loss: 0.618410]\n",
      "[Epoch 0/200] [Batch 113/3166] [D loss: 0.703608] [G loss: 0.634740]\n",
      "[Epoch 0/200] [Batch 114/3166] [D loss: 0.695473] [G loss: 0.652307]\n",
      "[Epoch 0/200] [Batch 115/3166] [D loss: 0.693052] [G loss: 0.668811]\n",
      "[Epoch 0/200] [Batch 116/3166] [D loss: 0.691104] [G loss: 0.675426]\n",
      "[Epoch 0/200] [Batch 117/3166] [D loss: 0.684031] [G loss: 0.685476]\n",
      "[Epoch 0/200] [Batch 118/3166] [D loss: 0.677674] [G loss: 0.703568]\n",
      "[Epoch 0/200] [Batch 119/3166] [D loss: 0.678418] [G loss: 0.715781]\n",
      "[Epoch 0/200] [Batch 120/3166] [D loss: 0.675928] [G loss: 0.729227]\n",
      "[Epoch 0/200] [Batch 121/3166] [D loss: 0.691939] [G loss: 0.714855]\n",
      "[Epoch 0/200] [Batch 122/3166] [D loss: 0.677493] [G loss: 0.715363]\n",
      "[Epoch 0/200] [Batch 123/3166] [D loss: 0.693015] [G loss: 0.705373]\n",
      "[Epoch 0/200] [Batch 124/3166] [D loss: 0.699235] [G loss: 0.704858]\n",
      "[Epoch 0/200] [Batch 125/3166] [D loss: 0.695240] [G loss: 0.710978]\n",
      "[Epoch 0/200] [Batch 126/3166] [D loss: 0.697688] [G loss: 0.711832]\n",
      "[Epoch 0/200] [Batch 127/3166] [D loss: 0.693704] [G loss: 0.705300]\n",
      "[Epoch 0/200] [Batch 128/3166] [D loss: 0.689549] [G loss: 0.723274]\n",
      "[Epoch 0/200] [Batch 129/3166] [D loss: 0.692420] [G loss: 0.708074]\n",
      "[Epoch 0/200] [Batch 130/3166] [D loss: 0.686805] [G loss: 0.701724]\n",
      "[Epoch 0/200] [Batch 131/3166] [D loss: 0.699353] [G loss: 0.695711]\n",
      "[Epoch 0/200] [Batch 132/3166] [D loss: 0.687060] [G loss: 0.685865]\n",
      "[Epoch 0/200] [Batch 133/3166] [D loss: 0.697262] [G loss: 0.676308]\n",
      "[Epoch 0/200] [Batch 134/3166] [D loss: 0.716859] [G loss: 0.666482]\n",
      "[Epoch 0/200] [Batch 135/3166] [D loss: 0.722964] [G loss: 0.651523]\n",
      "[Epoch 0/200] [Batch 136/3166] [D loss: 0.713141] [G loss: 0.682446]\n",
      "[Epoch 0/200] [Batch 137/3166] [D loss: 0.703102] [G loss: 0.678237]\n",
      "[Epoch 0/200] [Batch 138/3166] [D loss: 0.713895] [G loss: 0.673401]\n",
      "[Epoch 0/200] [Batch 139/3166] [D loss: 0.704513] [G loss: 0.680752]\n",
      "[Epoch 0/200] [Batch 140/3166] [D loss: 0.690985] [G loss: 0.699190]\n",
      "[Epoch 0/200] [Batch 141/3166] [D loss: 0.690337] [G loss: 0.706132]\n",
      "[Epoch 0/200] [Batch 142/3166] [D loss: 0.690919] [G loss: 0.696493]\n",
      "[Epoch 0/200] [Batch 143/3166] [D loss: 0.683920] [G loss: 0.720702]\n",
      "[Epoch 0/200] [Batch 144/3166] [D loss: 0.685114] [G loss: 0.726280]\n",
      "[Epoch 0/200] [Batch 145/3166] [D loss: 0.685384] [G loss: 0.716501]\n",
      "[Epoch 0/200] [Batch 146/3166] [D loss: 0.681384] [G loss: 0.724285]\n",
      "[Epoch 0/200] [Batch 147/3166] [D loss: 0.684618] [G loss: 0.712059]\n",
      "[Epoch 0/200] [Batch 148/3166] [D loss: 0.676680] [G loss: 0.727705]\n",
      "[Epoch 0/200] [Batch 149/3166] [D loss: 0.672070] [G loss: 0.726855]\n",
      "[Epoch 0/200] [Batch 150/3166] [D loss: 0.679580] [G loss: 0.717940]\n",
      "[Epoch 0/200] [Batch 151/3166] [D loss: 0.680455] [G loss: 0.714554]\n",
      "[Epoch 0/200] [Batch 152/3166] [D loss: 0.669476] [G loss: 0.723476]\n",
      "[Epoch 0/200] [Batch 153/3166] [D loss: 0.666254] [G loss: 0.726173]\n",
      "[Epoch 0/200] [Batch 154/3166] [D loss: 0.688963] [G loss: 0.706331]\n",
      "[Epoch 0/200] [Batch 155/3166] [D loss: 0.714096] [G loss: 0.665113]\n",
      "[Epoch 0/200] [Batch 156/3166] [D loss: 0.693161] [G loss: 0.685029]\n",
      "[Epoch 0/200] [Batch 157/3166] [D loss: 0.699871] [G loss: 0.676350]\n",
      "[Epoch 0/200] [Batch 158/3166] [D loss: 0.711698] [G loss: 0.676838]\n",
      "[Epoch 0/200] [Batch 159/3166] [D loss: 0.695803] [G loss: 0.683814]\n",
      "[Epoch 0/200] [Batch 160/3166] [D loss: 0.701425] [G loss: 0.678705]\n",
      "[Epoch 0/200] [Batch 161/3166] [D loss: 0.699434] [G loss: 0.682222]\n",
      "[Epoch 0/200] [Batch 162/3166] [D loss: 0.701010] [G loss: 0.683153]\n",
      "[Epoch 0/200] [Batch 163/3166] [D loss: 0.698057] [G loss: 0.682403]\n",
      "[Epoch 0/200] [Batch 164/3166] [D loss: 0.689678] [G loss: 0.696958]\n",
      "[Epoch 0/200] [Batch 165/3166] [D loss: 0.704508] [G loss: 0.682704]\n",
      "[Epoch 0/200] [Batch 166/3166] [D loss: 0.701761] [G loss: 0.687261]\n",
      "[Epoch 0/200] [Batch 167/3166] [D loss: 0.695882] [G loss: 0.685457]\n",
      "[Epoch 0/200] [Batch 168/3166] [D loss: 0.694014] [G loss: 0.695812]\n",
      "[Epoch 0/200] [Batch 169/3166] [D loss: 0.690652] [G loss: 0.705086]\n",
      "[Epoch 0/200] [Batch 170/3166] [D loss: 0.689081] [G loss: 0.704169]\n",
      "[Epoch 0/200] [Batch 171/3166] [D loss: 0.687932] [G loss: 0.704645]\n",
      "[Epoch 0/200] [Batch 172/3166] [D loss: 0.689041] [G loss: 0.702570]\n",
      "[Epoch 0/200] [Batch 173/3166] [D loss: 0.683698] [G loss: 0.706205]\n",
      "[Epoch 0/200] [Batch 174/3166] [D loss: 0.687828] [G loss: 0.704560]\n",
      "[Epoch 0/200] [Batch 175/3166] [D loss: 0.681994] [G loss: 0.709245]\n",
      "[Epoch 0/200] [Batch 176/3166] [D loss: 0.676212] [G loss: 0.712536]\n",
      "[Epoch 0/200] [Batch 177/3166] [D loss: 0.684170] [G loss: 0.708245]\n",
      "[Epoch 0/200] [Batch 178/3166] [D loss: 0.682754] [G loss: 0.707007]\n",
      "[Epoch 0/200] [Batch 179/3166] [D loss: 0.697022] [G loss: 0.701437]\n",
      "[Epoch 0/200] [Batch 180/3166] [D loss: 0.688656] [G loss: 0.695830]\n",
      "[Epoch 0/200] [Batch 181/3166] [D loss: 0.695538] [G loss: 0.689565]\n",
      "[Epoch 0/200] [Batch 182/3166] [D loss: 0.683053] [G loss: 0.696837]\n",
      "[Epoch 0/200] [Batch 183/3166] [D loss: 0.688841] [G loss: 0.697659]\n",
      "[Epoch 0/200] [Batch 184/3166] [D loss: 0.689478] [G loss: 0.690922]\n",
      "[Epoch 0/200] [Batch 185/3166] [D loss: 0.684005] [G loss: 0.692897]\n",
      "[Epoch 0/200] [Batch 186/3166] [D loss: 0.686946] [G loss: 0.693899]\n",
      "[Epoch 0/200] [Batch 187/3166] [D loss: 0.685322] [G loss: 0.700551]\n",
      "[Epoch 0/200] [Batch 188/3166] [D loss: 0.693172] [G loss: 0.696897]\n",
      "[Epoch 0/200] [Batch 189/3166] [D loss: 0.680210] [G loss: 0.693677]\n",
      "[Epoch 0/200] [Batch 190/3166] [D loss: 0.682700] [G loss: 0.695183]\n",
      "[Epoch 0/200] [Batch 191/3166] [D loss: 0.681847] [G loss: 0.689602]\n",
      "[Epoch 0/200] [Batch 192/3166] [D loss: 0.679105] [G loss: 0.689174]\n",
      "[Epoch 0/200] [Batch 193/3166] [D loss: 0.687267] [G loss: 0.696055]\n",
      "[Epoch 0/200] [Batch 194/3166] [D loss: 0.679455] [G loss: 0.699612]\n",
      "[Epoch 0/200] [Batch 195/3166] [D loss: 0.682633] [G loss: 0.692700]\n",
      "[Epoch 0/200] [Batch 196/3166] [D loss: 0.686115] [G loss: 0.686116]\n",
      "[Epoch 0/200] [Batch 197/3166] [D loss: 0.678367] [G loss: 0.698965]\n",
      "[Epoch 0/200] [Batch 198/3166] [D loss: 0.679543] [G loss: 0.695964]\n",
      "[Epoch 0/200] [Batch 199/3166] [D loss: 0.695351] [G loss: 0.691203]\n",
      "[Epoch 0/200] [Batch 200/3166] [D loss: 0.681638] [G loss: 0.683725]\n",
      "[Epoch 0/200] [Batch 201/3166] [D loss: 0.686794] [G loss: 0.696741]\n",
      "[Epoch 0/200] [Batch 202/3166] [D loss: 0.690020] [G loss: 0.677828]\n",
      "[Epoch 0/200] [Batch 203/3166] [D loss: 0.689382] [G loss: 0.692616]\n",
      "[Epoch 0/200] [Batch 204/3166] [D loss: 0.699065] [G loss: 0.676023]\n",
      "[Epoch 0/200] [Batch 205/3166] [D loss: 0.686180] [G loss: 0.687970]\n",
      "[Epoch 0/200] [Batch 206/3166] [D loss: 0.699170] [G loss: 0.683064]\n",
      "[Epoch 0/200] [Batch 207/3166] [D loss: 0.683294] [G loss: 0.683380]\n",
      "[Epoch 0/200] [Batch 208/3166] [D loss: 0.704650] [G loss: 0.666955]\n",
      "[Epoch 0/200] [Batch 209/3166] [D loss: 0.690535] [G loss: 0.674333]\n",
      "[Epoch 0/200] [Batch 210/3166] [D loss: 0.689413] [G loss: 0.691950]\n",
      "[Epoch 0/200] [Batch 211/3166] [D loss: 0.697065] [G loss: 0.677682]\n",
      "[Epoch 0/200] [Batch 212/3166] [D loss: 0.696869] [G loss: 0.685425]\n",
      "[Epoch 0/200] [Batch 213/3166] [D loss: 0.696659] [G loss: 0.697729]\n",
      "[Epoch 0/200] [Batch 214/3166] [D loss: 0.699488] [G loss: 0.688228]\n",
      "[Epoch 0/200] [Batch 215/3166] [D loss: 0.693385] [G loss: 0.690330]\n",
      "[Epoch 0/200] [Batch 216/3166] [D loss: 0.702257] [G loss: 0.682878]\n",
      "[Epoch 0/200] [Batch 217/3166] [D loss: 0.699371] [G loss: 0.691897]\n",
      "[Epoch 0/200] [Batch 218/3166] [D loss: 0.696907] [G loss: 0.701937]\n",
      "[Epoch 0/200] [Batch 219/3166] [D loss: 0.689323] [G loss: 0.698476]\n",
      "[Epoch 0/200] [Batch 220/3166] [D loss: 0.699940] [G loss: 0.692334]\n",
      "[Epoch 0/200] [Batch 221/3166] [D loss: 0.689940] [G loss: 0.703478]\n",
      "[Epoch 0/200] [Batch 222/3166] [D loss: 0.692747] [G loss: 0.700569]\n",
      "[Epoch 0/200] [Batch 223/3166] [D loss: 0.691228] [G loss: 0.706801]\n",
      "[Epoch 0/200] [Batch 224/3166] [D loss: 0.694080] [G loss: 0.708799]\n",
      "[Epoch 0/200] [Batch 225/3166] [D loss: 0.691202] [G loss: 0.705565]\n",
      "[Epoch 0/200] [Batch 226/3166] [D loss: 0.692556] [G loss: 0.700942]\n",
      "[Epoch 0/200] [Batch 227/3166] [D loss: 0.692212] [G loss: 0.700428]\n",
      "[Epoch 0/200] [Batch 228/3166] [D loss: 0.697745] [G loss: 0.694270]\n",
      "[Epoch 0/200] [Batch 229/3166] [D loss: 0.696145] [G loss: 0.696706]\n",
      "[Epoch 0/200] [Batch 230/3166] [D loss: 0.706697] [G loss: 0.689775]\n",
      "[Epoch 0/200] [Batch 231/3166] [D loss: 0.695300] [G loss: 0.689157]\n",
      "[Epoch 0/200] [Batch 232/3166] [D loss: 0.699457] [G loss: 0.691911]\n",
      "[Epoch 0/200] [Batch 233/3166] [D loss: 0.701706] [G loss: 0.691359]\n",
      "[Epoch 0/200] [Batch 234/3166] [D loss: 0.699820] [G loss: 0.695588]\n",
      "[Epoch 0/200] [Batch 235/3166] [D loss: 0.698874] [G loss: 0.695920]\n",
      "[Epoch 0/200] [Batch 236/3166] [D loss: 0.699494] [G loss: 0.699909]\n",
      "[Epoch 0/200] [Batch 237/3166] [D loss: 0.693476] [G loss: 0.701006]\n",
      "[Epoch 0/200] [Batch 238/3166] [D loss: 0.698633] [G loss: 0.700722]\n",
      "[Epoch 0/200] [Batch 239/3166] [D loss: 0.697800] [G loss: 0.704304]\n",
      "[Epoch 0/200] [Batch 240/3166] [D loss: 0.695760] [G loss: 0.710800]\n",
      "[Epoch 0/200] [Batch 241/3166] [D loss: 0.687165] [G loss: 0.711394]\n",
      "[Epoch 0/200] [Batch 242/3166] [D loss: 0.690544] [G loss: 0.713622]\n",
      "[Epoch 0/200] [Batch 243/3166] [D loss: 0.692854] [G loss: 0.712760]\n",
      "[Epoch 0/200] [Batch 244/3166] [D loss: 0.688993] [G loss: 0.714110]\n",
      "[Epoch 0/200] [Batch 245/3166] [D loss: 0.687378] [G loss: 0.714554]\n",
      "[Epoch 0/200] [Batch 246/3166] [D loss: 0.690386] [G loss: 0.715284]\n",
      "[Epoch 0/200] [Batch 247/3166] [D loss: 0.682896] [G loss: 0.725147]\n",
      "[Epoch 0/200] [Batch 248/3166] [D loss: 0.684095] [G loss: 0.717940]\n",
      "[Epoch 0/200] [Batch 249/3166] [D loss: 0.689181] [G loss: 0.720487]\n",
      "[Epoch 0/200] [Batch 250/3166] [D loss: 0.686756] [G loss: 0.720899]\n",
      "[Epoch 0/200] [Batch 251/3166] [D loss: 0.683215] [G loss: 0.718035]\n",
      "[Epoch 0/200] [Batch 252/3166] [D loss: 0.682800] [G loss: 0.722018]\n",
      "[Epoch 0/200] [Batch 253/3166] [D loss: 0.686896] [G loss: 0.724970]\n",
      "[Epoch 0/200] [Batch 254/3166] [D loss: 0.685498] [G loss: 0.715532]\n",
      "[Epoch 0/200] [Batch 255/3166] [D loss: 0.690988] [G loss: 0.722358]\n",
      "[Epoch 0/200] [Batch 256/3166] [D loss: 0.691359] [G loss: 0.707623]\n",
      "[Epoch 0/200] [Batch 257/3166] [D loss: 0.689175] [G loss: 0.713794]\n",
      "[Epoch 0/200] [Batch 258/3166] [D loss: 0.689538] [G loss: 0.713294]\n",
      "[Epoch 0/200] [Batch 259/3166] [D loss: 0.690883] [G loss: 0.704603]\n",
      "[Epoch 0/200] [Batch 260/3166] [D loss: 0.697879] [G loss: 0.695577]\n",
      "[Epoch 0/200] [Batch 261/3166] [D loss: 0.694586] [G loss: 0.701571]\n",
      "[Epoch 0/200] [Batch 262/3166] [D loss: 0.691306] [G loss: 0.701420]\n",
      "[Epoch 0/200] [Batch 263/3166] [D loss: 0.698316] [G loss: 0.689571]\n",
      "[Epoch 0/200] [Batch 264/3166] [D loss: 0.698181] [G loss: 0.693166]\n",
      "[Epoch 0/200] [Batch 265/3166] [D loss: 0.691025] [G loss: 0.697510]\n",
      "[Epoch 0/200] [Batch 266/3166] [D loss: 0.697625] [G loss: 0.694566]\n",
      "[Epoch 0/200] [Batch 267/3166] [D loss: 0.693747] [G loss: 0.699890]\n",
      "[Epoch 0/200] [Batch 268/3166] [D loss: 0.698767] [G loss: 0.687046]\n",
      "[Epoch 0/200] [Batch 269/3166] [D loss: 0.700788] [G loss: 0.689879]\n",
      "[Epoch 0/200] [Batch 270/3166] [D loss: 0.695049] [G loss: 0.694255]\n",
      "[Epoch 0/200] [Batch 271/3166] [D loss: 0.693448] [G loss: 0.697054]\n",
      "[Epoch 0/200] [Batch 272/3166] [D loss: 0.692378] [G loss: 0.700424]\n",
      "[Epoch 0/200] [Batch 273/3166] [D loss: 0.697955] [G loss: 0.692427]\n",
      "[Epoch 0/200] [Batch 274/3166] [D loss: 0.690683] [G loss: 0.693513]\n",
      "[Epoch 0/200] [Batch 275/3166] [D loss: 0.691501] [G loss: 0.695153]\n",
      "[Epoch 0/200] [Batch 276/3166] [D loss: 0.692835] [G loss: 0.692853]\n",
      "[Epoch 0/200] [Batch 277/3166] [D loss: 0.692478] [G loss: 0.694656]\n",
      "[Epoch 0/200] [Batch 278/3166] [D loss: 0.691197] [G loss: 0.697199]\n",
      "[Epoch 0/200] [Batch 279/3166] [D loss: 0.692542] [G loss: 0.690903]\n",
      "[Epoch 0/200] [Batch 280/3166] [D loss: 0.688568] [G loss: 0.691356]\n",
      "[Epoch 0/200] [Batch 281/3166] [D loss: 0.691322] [G loss: 0.692739]\n",
      "[Epoch 0/200] [Batch 282/3166] [D loss: 0.689271] [G loss: 0.690098]\n",
      "[Epoch 0/200] [Batch 283/3166] [D loss: 0.685285] [G loss: 0.691629]\n",
      "[Epoch 0/200] [Batch 284/3166] [D loss: 0.688329] [G loss: 0.691541]\n",
      "[Epoch 0/200] [Batch 285/3166] [D loss: 0.687886] [G loss: 0.689727]\n",
      "[Epoch 0/200] [Batch 286/3166] [D loss: 0.690163] [G loss: 0.688402]\n",
      "[Epoch 0/200] [Batch 287/3166] [D loss: 0.684276] [G loss: 0.686117]\n",
      "[Epoch 0/200] [Batch 288/3166] [D loss: 0.685455] [G loss: 0.686573]\n",
      "[Epoch 0/200] [Batch 289/3166] [D loss: 0.682404] [G loss: 0.687610]\n",
      "[Epoch 0/200] [Batch 290/3166] [D loss: 0.685041] [G loss: 0.683619]\n",
      "[Epoch 0/200] [Batch 291/3166] [D loss: 0.681349] [G loss: 0.685955]\n",
      "[Epoch 0/200] [Batch 292/3166] [D loss: 0.684673] [G loss: 0.679417]\n",
      "[Epoch 0/200] [Batch 293/3166] [D loss: 0.680073] [G loss: 0.679673]\n",
      "[Epoch 0/200] [Batch 294/3166] [D loss: 0.682812] [G loss: 0.679199]\n",
      "[Epoch 0/200] [Batch 295/3166] [D loss: 0.688167] [G loss: 0.674666]\n",
      "[Epoch 0/200] [Batch 296/3166] [D loss: 0.687179] [G loss: 0.671562]\n",
      "[Epoch 0/200] [Batch 297/3166] [D loss: 0.683525] [G loss: 0.668574]\n",
      "[Epoch 0/200] [Batch 298/3166] [D loss: 0.690118] [G loss: 0.662546]\n",
      "[Epoch 0/200] [Batch 299/3166] [D loss: 0.697957] [G loss: 0.653806]\n",
      "[Epoch 0/200] [Batch 300/3166] [D loss: 0.695576] [G loss: 0.664271]\n",
      "[Epoch 0/200] [Batch 301/3166] [D loss: 0.694305] [G loss: 0.668023]\n",
      "[Epoch 0/200] [Batch 302/3166] [D loss: 0.697373] [G loss: 0.665071]\n",
      "[Epoch 0/200] [Batch 303/3166] [D loss: 0.698156] [G loss: 0.669504]\n",
      "[Epoch 0/200] [Batch 304/3166] [D loss: 0.688564] [G loss: 0.685529]\n",
      "[Epoch 0/200] [Batch 305/3166] [D loss: 0.693064] [G loss: 0.684445]\n",
      "[Epoch 0/200] [Batch 306/3166] [D loss: 0.694668] [G loss: 0.690916]\n",
      "[Epoch 0/200] [Batch 307/3166] [D loss: 0.693298] [G loss: 0.696582]\n",
      "[Epoch 0/200] [Batch 308/3166] [D loss: 0.698447] [G loss: 0.682387]\n",
      "[Epoch 0/200] [Batch 309/3166] [D loss: 0.702402] [G loss: 0.687569]\n",
      "[Epoch 0/200] [Batch 310/3166] [D loss: 0.694805] [G loss: 0.697234]\n",
      "[Epoch 0/200] [Batch 311/3166] [D loss: 0.684514] [G loss: 0.701042]\n",
      "[Epoch 0/200] [Batch 312/3166] [D loss: 0.695756] [G loss: 0.700071]\n",
      "[Epoch 0/200] [Batch 313/3166] [D loss: 0.691602] [G loss: 0.704887]\n",
      "[Epoch 0/200] [Batch 314/3166] [D loss: 0.684098] [G loss: 0.707130]\n",
      "[Epoch 0/200] [Batch 315/3166] [D loss: 0.690114] [G loss: 0.704188]\n",
      "[Epoch 0/200] [Batch 316/3166] [D loss: 0.695458] [G loss: 0.699785]\n",
      "[Epoch 0/200] [Batch 317/3166] [D loss: 0.696301] [G loss: 0.697345]\n",
      "[Epoch 0/200] [Batch 318/3166] [D loss: 0.696634] [G loss: 0.699834]\n",
      "[Epoch 0/200] [Batch 319/3166] [D loss: 0.690637] [G loss: 0.704546]\n",
      "[Epoch 0/200] [Batch 320/3166] [D loss: 0.690570] [G loss: 0.699710]\n",
      "[Epoch 0/200] [Batch 321/3166] [D loss: 0.688588] [G loss: 0.701188]\n",
      "[Epoch 0/200] [Batch 322/3166] [D loss: 0.689723] [G loss: 0.699155]\n",
      "[Epoch 0/200] [Batch 323/3166] [D loss: 0.693898] [G loss: 0.695591]\n",
      "[Epoch 0/200] [Batch 324/3166] [D loss: 0.694212] [G loss: 0.697356]\n",
      "[Epoch 0/200] [Batch 325/3166] [D loss: 0.696350] [G loss: 0.691786]\n",
      "[Epoch 0/200] [Batch 326/3166] [D loss: 0.699398] [G loss: 0.691215]\n",
      "[Epoch 0/200] [Batch 327/3166] [D loss: 0.696862] [G loss: 0.694461]\n",
      "[Epoch 0/200] [Batch 328/3166] [D loss: 0.696308] [G loss: 0.692916]\n",
      "[Epoch 0/200] [Batch 329/3166] [D loss: 0.691644] [G loss: 0.696770]\n",
      "[Epoch 0/200] [Batch 330/3166] [D loss: 0.690866] [G loss: 0.703619]\n",
      "[Epoch 0/200] [Batch 331/3166] [D loss: 0.692512] [G loss: 0.702343]\n",
      "[Epoch 0/200] [Batch 332/3166] [D loss: 0.687998] [G loss: 0.707015]\n",
      "[Epoch 0/200] [Batch 333/3166] [D loss: 0.691390] [G loss: 0.705301]\n",
      "[Epoch 0/200] [Batch 334/3166] [D loss: 0.691858] [G loss: 0.711513]\n",
      "[Epoch 0/200] [Batch 335/3166] [D loss: 0.687157] [G loss: 0.711598]\n",
      "[Epoch 0/200] [Batch 336/3166] [D loss: 0.698375] [G loss: 0.707276]\n",
      "[Epoch 0/200] [Batch 337/3166] [D loss: 0.690847] [G loss: 0.711437]\n",
      "[Epoch 0/200] [Batch 338/3166] [D loss: 0.686154] [G loss: 0.712275]\n",
      "[Epoch 0/200] [Batch 339/3166] [D loss: 0.687968] [G loss: 0.711235]\n",
      "[Epoch 0/200] [Batch 340/3166] [D loss: 0.689507] [G loss: 0.709865]\n",
      "[Epoch 0/200] [Batch 341/3166] [D loss: 0.683452] [G loss: 0.709371]\n",
      "[Epoch 0/200] [Batch 342/3166] [D loss: 0.688792] [G loss: 0.706220]\n",
      "[Epoch 0/200] [Batch 343/3166] [D loss: 0.690642] [G loss: 0.702116]\n",
      "[Epoch 0/200] [Batch 344/3166] [D loss: 0.692150] [G loss: 0.695559]\n",
      "[Epoch 0/200] [Batch 345/3166] [D loss: 0.690498] [G loss: 0.699518]\n",
      "[Epoch 0/200] [Batch 346/3166] [D loss: 0.687987] [G loss: 0.694984]\n",
      "[Epoch 0/200] [Batch 347/3166] [D loss: 0.686276] [G loss: 0.696284]\n",
      "[Epoch 0/200] [Batch 348/3166] [D loss: 0.685158] [G loss: 0.696703]\n",
      "[Epoch 0/200] [Batch 349/3166] [D loss: 0.680477] [G loss: 0.691384]\n",
      "[Epoch 0/200] [Batch 350/3166] [D loss: 0.678477] [G loss: 0.690389]\n",
      "[Epoch 0/200] [Batch 351/3166] [D loss: 0.671928] [G loss: 0.691705]\n",
      "[Epoch 0/200] [Batch 352/3166] [D loss: 0.677622] [G loss: 0.693210]\n",
      "[Epoch 0/200] [Batch 353/3166] [D loss: 0.682243] [G loss: 0.680587]\n",
      "[Epoch 0/200] [Batch 354/3166] [D loss: 0.692541] [G loss: 0.678347]\n",
      "[Epoch 0/200] [Batch 355/3166] [D loss: 0.676851] [G loss: 0.659304]\n",
      "[Epoch 0/200] [Batch 356/3166] [D loss: 0.693807] [G loss: 0.635731]\n",
      "[Epoch 0/200] [Batch 357/3166] [D loss: 0.702709] [G loss: 0.631662]\n",
      "[Epoch 0/200] [Batch 358/3166] [D loss: 0.700766] [G loss: 0.636233]\n",
      "[Epoch 0/200] [Batch 359/3166] [D loss: 0.702776] [G loss: 0.627600]\n",
      "[Epoch 0/200] [Batch 360/3166] [D loss: 0.703170] [G loss: 0.659891]\n",
      "[Epoch 0/200] [Batch 361/3166] [D loss: 0.706146] [G loss: 0.660959]\n",
      "[Epoch 0/200] [Batch 362/3166] [D loss: 0.698260] [G loss: 0.662764]\n",
      "[Epoch 0/200] [Batch 363/3166] [D loss: 0.701774] [G loss: 0.684288]\n",
      "[Epoch 0/200] [Batch 364/3166] [D loss: 0.691824] [G loss: 0.696949]\n",
      "[Epoch 0/200] [Batch 365/3166] [D loss: 0.697382] [G loss: 0.703708]\n",
      "[Epoch 0/200] [Batch 366/3166] [D loss: 0.706838] [G loss: 0.696812]\n",
      "[Epoch 0/200] [Batch 367/3166] [D loss: 0.692797] [G loss: 0.711888]\n",
      "[Epoch 0/200] [Batch 368/3166] [D loss: 0.703624] [G loss: 0.700406]\n",
      "[Epoch 0/200] [Batch 369/3166] [D loss: 0.694372] [G loss: 0.712364]\n",
      "[Epoch 0/200] [Batch 370/3166] [D loss: 0.692877] [G loss: 0.722122]\n",
      "[Epoch 0/200] [Batch 371/3166] [D loss: 0.701617] [G loss: 0.708357]\n",
      "[Epoch 0/200] [Batch 372/3166] [D loss: 0.695809] [G loss: 0.718288]\n",
      "[Epoch 0/200] [Batch 373/3166] [D loss: 0.694279] [G loss: 0.718003]\n",
      "[Epoch 0/200] [Batch 374/3166] [D loss: 0.700860] [G loss: 0.708836]\n",
      "[Epoch 0/200] [Batch 375/3166] [D loss: 0.697143] [G loss: 0.719930]\n",
      "[Epoch 0/200] [Batch 376/3166] [D loss: 0.696777] [G loss: 0.726932]\n",
      "[Epoch 0/200] [Batch 377/3166] [D loss: 0.698525] [G loss: 0.719848]\n",
      "[Epoch 0/200] [Batch 378/3166] [D loss: 0.693707] [G loss: 0.723548]\n",
      "[Epoch 0/200] [Batch 379/3166] [D loss: 0.692374] [G loss: 0.713170]\n",
      "[Epoch 0/200] [Batch 380/3166] [D loss: 0.698288] [G loss: 0.714694]\n",
      "[Epoch 0/200] [Batch 381/3166] [D loss: 0.696520] [G loss: 0.723209]\n",
      "[Epoch 0/200] [Batch 382/3166] [D loss: 0.693433] [G loss: 0.721405]\n",
      "[Epoch 0/200] [Batch 383/3166] [D loss: 0.691275] [G loss: 0.723891]\n",
      "[Epoch 0/200] [Batch 384/3166] [D loss: 0.692203] [G loss: 0.720545]\n",
      "[Epoch 0/200] [Batch 385/3166] [D loss: 0.696221] [G loss: 0.713030]\n",
      "[Epoch 0/200] [Batch 386/3166] [D loss: 0.690907] [G loss: 0.716809]\n",
      "[Epoch 0/200] [Batch 387/3166] [D loss: 0.698205] [G loss: 0.712186]\n",
      "[Epoch 0/200] [Batch 388/3166] [D loss: 0.690292] [G loss: 0.712800]\n",
      "[Epoch 0/200] [Batch 389/3166] [D loss: 0.690846] [G loss: 0.714934]\n",
      "[Epoch 0/200] [Batch 390/3166] [D loss: 0.689923] [G loss: 0.711927]\n",
      "[Epoch 0/200] [Batch 391/3166] [D loss: 0.689736] [G loss: 0.713408]\n",
      "[Epoch 0/200] [Batch 392/3166] [D loss: 0.690324] [G loss: 0.712332]\n",
      "[Epoch 0/200] [Batch 393/3166] [D loss: 0.683528] [G loss: 0.701579]\n",
      "[Epoch 0/200] [Batch 394/3166] [D loss: 0.680863] [G loss: 0.707274]\n",
      "[Epoch 0/200] [Batch 395/3166] [D loss: 0.682879] [G loss: 0.705642]\n",
      "[Epoch 0/200] [Batch 396/3166] [D loss: 0.685316] [G loss: 0.704378]\n",
      "[Epoch 0/200] [Batch 397/3166] [D loss: 0.680961] [G loss: 0.698754]\n",
      "[Epoch 0/200] [Batch 398/3166] [D loss: 0.677226] [G loss: 0.701076]\n",
      "[Epoch 0/200] [Batch 399/3166] [D loss: 0.688332] [G loss: 0.703017]\n",
      "[Epoch 0/200] [Batch 400/3166] [D loss: 0.675716] [G loss: 0.696287]\n",
      "[Epoch 0/200] [Batch 401/3166] [D loss: 0.668408] [G loss: 0.699249]\n",
      "[Epoch 0/200] [Batch 402/3166] [D loss: 0.664991] [G loss: 0.695802]\n",
      "[Epoch 0/200] [Batch 403/3166] [D loss: 0.671622] [G loss: 0.697101]\n",
      "[Epoch 0/200] [Batch 404/3166] [D loss: 0.669805] [G loss: 0.704601]\n",
      "[Epoch 0/200] [Batch 405/3166] [D loss: 0.658504] [G loss: 0.709523]\n",
      "[Epoch 0/200] [Batch 406/3166] [D loss: 0.644742] [G loss: 0.702179]\n",
      "[Epoch 0/200] [Batch 407/3166] [D loss: 0.655093] [G loss: 0.702502]\n",
      "[Epoch 0/200] [Batch 408/3166] [D loss: 0.670641] [G loss: 0.665469]\n",
      "[Epoch 0/200] [Batch 409/3166] [D loss: 0.700239] [G loss: 0.633141]\n",
      "[Epoch 0/200] [Batch 410/3166] [D loss: 0.754346] [G loss: 0.560210]\n",
      "[Epoch 0/200] [Batch 411/3166] [D loss: 0.729619] [G loss: 0.569659]\n",
      "[Epoch 0/200] [Batch 412/3166] [D loss: 0.697163] [G loss: 0.599331]\n",
      "[Epoch 0/200] [Batch 413/3166] [D loss: 0.732215] [G loss: 0.609797]\n",
      "[Epoch 0/200] [Batch 414/3166] [D loss: 0.713389] [G loss: 0.639082]\n",
      "[Epoch 0/200] [Batch 415/3166] [D loss: 0.711874] [G loss: 0.670651]\n",
      "[Epoch 0/200] [Batch 416/3166] [D loss: 0.706929] [G loss: 0.695958]\n",
      "[Epoch 0/200] [Batch 417/3166] [D loss: 0.694562] [G loss: 0.731304]\n",
      "[Epoch 0/200] [Batch 418/3166] [D loss: 0.691749] [G loss: 0.745662]\n",
      "[Epoch 0/200] [Batch 419/3166] [D loss: 0.688368] [G loss: 0.759923]\n",
      "[Epoch 0/200] [Batch 420/3166] [D loss: 0.687113] [G loss: 0.791680]\n",
      "[Epoch 0/200] [Batch 421/3166] [D loss: 0.677463] [G loss: 0.760316]\n",
      "[Epoch 0/200] [Batch 422/3166] [D loss: 0.684324] [G loss: 0.776132]\n",
      "[Epoch 0/200] [Batch 423/3166] [D loss: 0.676136] [G loss: 0.789861]\n",
      "[Epoch 0/200] [Batch 424/3166] [D loss: 0.685205] [G loss: 0.783745]\n",
      "[Epoch 0/200] [Batch 425/3166] [D loss: 0.687267] [G loss: 0.774120]\n",
      "[Epoch 0/200] [Batch 426/3166] [D loss: 0.684192] [G loss: 0.759194]\n",
      "[Epoch 0/200] [Batch 427/3166] [D loss: 0.682286] [G loss: 0.738473]\n",
      "[Epoch 0/200] [Batch 428/3166] [D loss: 0.691989] [G loss: 0.725685]\n",
      "[Epoch 0/200] [Batch 429/3166] [D loss: 0.699416] [G loss: 0.707011]\n",
      "[Epoch 0/200] [Batch 430/3166] [D loss: 0.703425] [G loss: 0.699331]\n",
      "[Epoch 0/200] [Batch 431/3166] [D loss: 0.705260] [G loss: 0.692407]\n",
      "[Epoch 0/200] [Batch 432/3166] [D loss: 0.694297] [G loss: 0.699452]\n",
      "[Epoch 0/200] [Batch 433/3166] [D loss: 0.704096] [G loss: 0.691832]\n",
      "[Epoch 0/200] [Batch 434/3166] [D loss: 0.702214] [G loss: 0.686213]\n",
      "[Epoch 0/200] [Batch 435/3166] [D loss: 0.703741] [G loss: 0.684592]\n",
      "[Epoch 0/200] [Batch 436/3166] [D loss: 0.699284] [G loss: 0.686466]\n",
      "[Epoch 0/200] [Batch 437/3166] [D loss: 0.699166] [G loss: 0.681240]\n",
      "[Epoch 0/200] [Batch 438/3166] [D loss: 0.693980] [G loss: 0.686034]\n",
      "[Epoch 0/200] [Batch 439/3166] [D loss: 0.703334] [G loss: 0.685992]\n",
      "[Epoch 0/200] [Batch 440/3166] [D loss: 0.696643] [G loss: 0.687435]\n",
      "[Epoch 0/200] [Batch 441/3166] [D loss: 0.699818] [G loss: 0.678505]\n",
      "[Epoch 0/200] [Batch 442/3166] [D loss: 0.707345] [G loss: 0.675729]\n",
      "[Epoch 0/200] [Batch 443/3166] [D loss: 0.698802] [G loss: 0.674091]\n",
      "[Epoch 0/200] [Batch 444/3166] [D loss: 0.698648] [G loss: 0.677338]\n",
      "[Epoch 0/200] [Batch 445/3166] [D loss: 0.704702] [G loss: 0.675055]\n",
      "[Epoch 0/200] [Batch 446/3166] [D loss: 0.703880] [G loss: 0.675380]\n",
      "[Epoch 0/200] [Batch 447/3166] [D loss: 0.705678] [G loss: 0.675926]\n",
      "[Epoch 0/200] [Batch 448/3166] [D loss: 0.703120] [G loss: 0.678696]\n",
      "[Epoch 0/200] [Batch 449/3166] [D loss: 0.700910] [G loss: 0.681903]\n",
      "[Epoch 0/200] [Batch 450/3166] [D loss: 0.702977] [G loss: 0.685191]\n",
      "[Epoch 0/200] [Batch 451/3166] [D loss: 0.701927] [G loss: 0.680493]\n",
      "[Epoch 0/200] [Batch 452/3166] [D loss: 0.697852] [G loss: 0.682021]\n",
      "[Epoch 0/200] [Batch 453/3166] [D loss: 0.703581] [G loss: 0.691572]\n",
      "[Epoch 0/200] [Batch 454/3166] [D loss: 0.697818] [G loss: 0.689725]\n",
      "[Epoch 0/200] [Batch 455/3166] [D loss: 0.694742] [G loss: 0.685596]\n",
      "[Epoch 0/200] [Batch 456/3166] [D loss: 0.703035] [G loss: 0.681829]\n",
      "[Epoch 0/200] [Batch 457/3166] [D loss: 0.698281] [G loss: 0.689256]\n",
      "[Epoch 0/200] [Batch 458/3166] [D loss: 0.702526] [G loss: 0.692956]\n",
      "[Epoch 0/200] [Batch 459/3166] [D loss: 0.699966] [G loss: 0.696508]\n",
      "[Epoch 0/200] [Batch 460/3166] [D loss: 0.696224] [G loss: 0.695871]\n",
      "[Epoch 0/200] [Batch 461/3166] [D loss: 0.698862] [G loss: 0.698781]\n",
      "[Epoch 0/200] [Batch 462/3166] [D loss: 0.698976] [G loss: 0.696880]\n",
      "[Epoch 0/200] [Batch 463/3166] [D loss: 0.699059] [G loss: 0.698451]\n",
      "[Epoch 0/200] [Batch 464/3166] [D loss: 0.697713] [G loss: 0.700421]\n",
      "[Epoch 0/200] [Batch 465/3166] [D loss: 0.698055] [G loss: 0.704901]\n",
      "[Epoch 0/200] [Batch 466/3166] [D loss: 0.694159] [G loss: 0.705847]\n",
      "[Epoch 0/200] [Batch 467/3166] [D loss: 0.697122] [G loss: 0.705307]\n",
      "[Epoch 0/200] [Batch 468/3166] [D loss: 0.695159] [G loss: 0.707244]\n",
      "[Epoch 0/200] [Batch 469/3166] [D loss: 0.695500] [G loss: 0.705431]\n",
      "[Epoch 0/200] [Batch 470/3166] [D loss: 0.699786] [G loss: 0.705278]\n",
      "[Epoch 0/200] [Batch 471/3166] [D loss: 0.695083] [G loss: 0.711131]\n",
      "[Epoch 0/200] [Batch 472/3166] [D loss: 0.694193] [G loss: 0.711495]\n",
      "[Epoch 0/200] [Batch 473/3166] [D loss: 0.695863] [G loss: 0.707590]\n",
      "[Epoch 0/200] [Batch 474/3166] [D loss: 0.694376] [G loss: 0.710752]\n",
      "[Epoch 0/200] [Batch 475/3166] [D loss: 0.695557] [G loss: 0.708523]\n",
      "[Epoch 0/200] [Batch 476/3166] [D loss: 0.695974] [G loss: 0.710336]\n",
      "[Epoch 0/200] [Batch 477/3166] [D loss: 0.691491] [G loss: 0.711091]\n",
      "[Epoch 0/200] [Batch 478/3166] [D loss: 0.693393] [G loss: 0.710375]\n",
      "[Epoch 0/200] [Batch 479/3166] [D loss: 0.694840] [G loss: 0.712206]\n",
      "[Epoch 0/200] [Batch 480/3166] [D loss: 0.695971] [G loss: 0.712033]\n",
      "[Epoch 0/200] [Batch 481/3166] [D loss: 0.695469] [G loss: 0.708461]\n",
      "[Epoch 0/200] [Batch 482/3166] [D loss: 0.691063] [G loss: 0.708406]\n",
      "[Epoch 0/200] [Batch 483/3166] [D loss: 0.690354] [G loss: 0.709314]\n",
      "[Epoch 0/200] [Batch 484/3166] [D loss: 0.688444] [G loss: 0.710788]\n",
      "[Epoch 0/200] [Batch 485/3166] [D loss: 0.688527] [G loss: 0.708330]\n",
      "[Epoch 0/200] [Batch 486/3166] [D loss: 0.688918] [G loss: 0.710505]\n",
      "[Epoch 0/200] [Batch 487/3166] [D loss: 0.691768] [G loss: 0.704436]\n",
      "[Epoch 0/200] [Batch 488/3166] [D loss: 0.688929] [G loss: 0.707012]\n",
      "[Epoch 0/200] [Batch 489/3166] [D loss: 0.682274] [G loss: 0.713138]\n",
      "[Epoch 0/200] [Batch 490/3166] [D loss: 0.688502] [G loss: 0.709835]\n",
      "[Epoch 0/200] [Batch 491/3166] [D loss: 0.685425] [G loss: 0.710610]\n",
      "[Epoch 0/200] [Batch 492/3166] [D loss: 0.683751] [G loss: 0.710243]\n",
      "[Epoch 0/200] [Batch 493/3166] [D loss: 0.676073] [G loss: 0.709188]\n",
      "[Epoch 0/200] [Batch 494/3166] [D loss: 0.677782] [G loss: 0.706834]\n",
      "[Epoch 0/200] [Batch 495/3166] [D loss: 0.686681] [G loss: 0.704227]\n",
      "[Epoch 0/200] [Batch 496/3166] [D loss: 0.682367] [G loss: 0.702511]\n",
      "[Epoch 0/200] [Batch 497/3166] [D loss: 0.673914] [G loss: 0.700304]\n",
      "[Epoch 0/200] [Batch 498/3166] [D loss: 0.673862] [G loss: 0.697013]\n",
      "[Epoch 0/200] [Batch 499/3166] [D loss: 0.668368] [G loss: 0.696538]\n",
      "[Epoch 0/200] [Batch 500/3166] [D loss: 0.673830] [G loss: 0.696718]\n",
      "[Epoch 0/200] [Batch 501/3166] [D loss: 0.673500] [G loss: 0.686669]\n",
      "[Epoch 0/200] [Batch 502/3166] [D loss: 0.671747] [G loss: 0.677243]\n",
      "[Epoch 0/200] [Batch 503/3166] [D loss: 0.672291] [G loss: 0.668174]\n",
      "[Epoch 0/200] [Batch 504/3166] [D loss: 0.684180] [G loss: 0.642127]\n",
      "[Epoch 0/200] [Batch 505/3166] [D loss: 0.713361] [G loss: 0.626910]\n",
      "[Epoch 0/200] [Batch 506/3166] [D loss: 0.695699] [G loss: 0.620965]\n",
      "[Epoch 0/200] [Batch 507/3166] [D loss: 0.697667] [G loss: 0.630165]\n",
      "[Epoch 0/200] [Batch 508/3166] [D loss: 0.712417] [G loss: 0.641116]\n",
      "[Epoch 0/200] [Batch 509/3166] [D loss: 0.689797] [G loss: 0.629084]\n",
      "[Epoch 0/200] [Batch 510/3166] [D loss: 0.702058] [G loss: 0.663160]\n",
      "[Epoch 0/200] [Batch 511/3166] [D loss: 0.710879] [G loss: 0.659138]\n",
      "[Epoch 0/200] [Batch 512/3166] [D loss: 0.704954] [G loss: 0.654872]\n",
      "[Epoch 0/200] [Batch 513/3166] [D loss: 0.706258] [G loss: 0.676058]\n",
      "[Epoch 0/200] [Batch 514/3166] [D loss: 0.695647] [G loss: 0.696659]\n",
      "[Epoch 0/200] [Batch 515/3166] [D loss: 0.705535] [G loss: 0.690262]\n",
      "[Epoch 0/200] [Batch 516/3166] [D loss: 0.711219] [G loss: 0.694916]\n",
      "[Epoch 0/200] [Batch 517/3166] [D loss: 0.704503] [G loss: 0.690893]\n",
      "[Epoch 0/200] [Batch 518/3166] [D loss: 0.700235] [G loss: 0.690296]\n",
      "[Epoch 0/200] [Batch 519/3166] [D loss: 0.696204] [G loss: 0.709419]\n",
      "[Epoch 0/200] [Batch 520/3166] [D loss: 0.694649] [G loss: 0.716058]\n",
      "[Epoch 0/200] [Batch 521/3166] [D loss: 0.698112] [G loss: 0.715911]\n",
      "[Epoch 0/200] [Batch 522/3166] [D loss: 0.692396] [G loss: 0.712266]\n",
      "[Epoch 0/200] [Batch 523/3166] [D loss: 0.696414] [G loss: 0.723721]\n",
      "[Epoch 0/200] [Batch 524/3166] [D loss: 0.705832] [G loss: 0.717887]\n",
      "[Epoch 0/200] [Batch 525/3166] [D loss: 0.690395] [G loss: 0.726878]\n",
      "[Epoch 0/200] [Batch 526/3166] [D loss: 0.689387] [G loss: 0.730315]\n",
      "[Epoch 0/200] [Batch 527/3166] [D loss: 0.691770] [G loss: 0.738000]\n",
      "[Epoch 0/200] [Batch 528/3166] [D loss: 0.689309] [G loss: 0.738898]\n",
      "[Epoch 0/200] [Batch 529/3166] [D loss: 0.684268] [G loss: 0.744749]\n",
      "[Epoch 0/200] [Batch 530/3166] [D loss: 0.684592] [G loss: 0.742002]\n",
      "[Epoch 0/200] [Batch 531/3166] [D loss: 0.686547] [G loss: 0.740403]\n",
      "[Epoch 0/200] [Batch 532/3166] [D loss: 0.678303] [G loss: 0.753801]\n",
      "[Epoch 0/200] [Batch 533/3166] [D loss: 0.690324] [G loss: 0.750945]\n",
      "[Epoch 0/200] [Batch 534/3166] [D loss: 0.681782] [G loss: 0.744420]\n",
      "[Epoch 0/200] [Batch 535/3166] [D loss: 0.683109] [G loss: 0.748532]\n",
      "[Epoch 0/200] [Batch 536/3166] [D loss: 0.680344] [G loss: 0.754699]\n",
      "[Epoch 0/200] [Batch 537/3166] [D loss: 0.680019] [G loss: 0.759851]\n",
      "[Epoch 0/200] [Batch 538/3166] [D loss: 0.671454] [G loss: 0.774006]\n",
      "[Epoch 0/200] [Batch 539/3166] [D loss: 0.678723] [G loss: 0.775264]\n",
      "[Epoch 0/200] [Batch 540/3166] [D loss: 0.672067] [G loss: 0.755048]\n",
      "[Epoch 0/200] [Batch 541/3166] [D loss: 0.673828] [G loss: 0.761600]\n",
      "[Epoch 0/200] [Batch 542/3166] [D loss: 0.684133] [G loss: 0.747652]\n",
      "[Epoch 0/200] [Batch 543/3166] [D loss: 0.679317] [G loss: 0.753555]\n",
      "[Epoch 0/200] [Batch 544/3166] [D loss: 0.677720] [G loss: 0.749056]\n",
      "[Epoch 0/200] [Batch 545/3166] [D loss: 0.688041] [G loss: 0.729599]\n",
      "[Epoch 0/200] [Batch 546/3166] [D loss: 0.702661] [G loss: 0.696253]\n",
      "[Epoch 0/200] [Batch 547/3166] [D loss: 0.703181] [G loss: 0.702241]\n",
      "[Epoch 0/200] [Batch 548/3166] [D loss: 0.700593] [G loss: 0.695085]\n",
      "[Epoch 0/200] [Batch 549/3166] [D loss: 0.712978] [G loss: 0.688987]\n",
      "[Epoch 0/200] [Batch 550/3166] [D loss: 0.705743] [G loss: 0.688771]\n",
      "[Epoch 0/200] [Batch 551/3166] [D loss: 0.705910] [G loss: 0.684648]\n",
      "[Epoch 0/200] [Batch 552/3166] [D loss: 0.704729] [G loss: 0.688431]\n",
      "[Epoch 0/200] [Batch 553/3166] [D loss: 0.704158] [G loss: 0.681066]\n",
      "[Epoch 0/200] [Batch 554/3166] [D loss: 0.698067] [G loss: 0.684725]\n",
      "[Epoch 0/200] [Batch 555/3166] [D loss: 0.696891] [G loss: 0.683647]\n",
      "[Epoch 0/200] [Batch 556/3166] [D loss: 0.695174] [G loss: 0.688956]\n",
      "[Epoch 0/200] [Batch 557/3166] [D loss: 0.695786] [G loss: 0.686711]\n",
      "[Epoch 0/200] [Batch 558/3166] [D loss: 0.693133] [G loss: 0.685655]\n",
      "[Epoch 0/200] [Batch 559/3166] [D loss: 0.691171] [G loss: 0.692613]\n",
      "[Epoch 0/200] [Batch 560/3166] [D loss: 0.696090] [G loss: 0.688525]\n",
      "[Epoch 0/200] [Batch 561/3166] [D loss: 0.687785] [G loss: 0.690306]\n",
      "[Epoch 0/200] [Batch 562/3166] [D loss: 0.693121] [G loss: 0.687366]\n",
      "[Epoch 0/200] [Batch 563/3166] [D loss: 0.689533] [G loss: 0.689491]\n",
      "[Epoch 0/200] [Batch 564/3166] [D loss: 0.690864] [G loss: 0.689032]\n",
      "[Epoch 0/200] [Batch 565/3166] [D loss: 0.690015] [G loss: 0.688281]\n",
      "[Epoch 0/200] [Batch 566/3166] [D loss: 0.690784] [G loss: 0.688720]\n",
      "[Epoch 0/200] [Batch 567/3166] [D loss: 0.689051] [G loss: 0.690725]\n",
      "[Epoch 0/200] [Batch 568/3166] [D loss: 0.696027] [G loss: 0.687937]\n",
      "[Epoch 0/200] [Batch 569/3166] [D loss: 0.691019] [G loss: 0.686753]\n",
      "[Epoch 0/200] [Batch 570/3166] [D loss: 0.695026] [G loss: 0.681238]\n",
      "[Epoch 0/200] [Batch 571/3166] [D loss: 0.695683] [G loss: 0.683824]\n",
      "[Epoch 0/200] [Batch 572/3166] [D loss: 0.692845] [G loss: 0.679660]\n",
      "[Epoch 0/200] [Batch 573/3166] [D loss: 0.693994] [G loss: 0.680670]\n",
      "[Epoch 0/200] [Batch 574/3166] [D loss: 0.696611] [G loss: 0.680798]\n",
      "[Epoch 0/200] [Batch 575/3166] [D loss: 0.691941] [G loss: 0.681329]\n",
      "[Epoch 0/200] [Batch 576/3166] [D loss: 0.693610] [G loss: 0.676757]\n",
      "[Epoch 0/200] [Batch 577/3166] [D loss: 0.691721] [G loss: 0.680435]\n",
      "[Epoch 0/200] [Batch 578/3166] [D loss: 0.693874] [G loss: 0.680840]\n",
      "[Epoch 0/200] [Batch 579/3166] [D loss: 0.692885] [G loss: 0.679072]\n",
      "[Epoch 0/200] [Batch 580/3166] [D loss: 0.694668] [G loss: 0.680896]\n",
      "[Epoch 0/200] [Batch 581/3166] [D loss: 0.690185] [G loss: 0.682976]\n",
      "[Epoch 0/200] [Batch 582/3166] [D loss: 0.688477] [G loss: 0.682528]\n",
      "[Epoch 0/200] [Batch 583/3166] [D loss: 0.690103] [G loss: 0.680251]\n",
      "[Epoch 0/200] [Batch 584/3166] [D loss: 0.690932] [G loss: 0.687342]\n",
      "[Epoch 0/200] [Batch 585/3166] [D loss: 0.689491] [G loss: 0.685051]\n",
      "[Epoch 0/200] [Batch 586/3166] [D loss: 0.689690] [G loss: 0.684847]\n",
      "[Epoch 0/200] [Batch 587/3166] [D loss: 0.691589] [G loss: 0.682347]\n",
      "[Epoch 0/200] [Batch 588/3166] [D loss: 0.688249] [G loss: 0.681208]\n",
      "[Epoch 0/200] [Batch 589/3166] [D loss: 0.693459] [G loss: 0.680400]\n",
      "[Epoch 0/200] [Batch 590/3166] [D loss: 0.686504] [G loss: 0.681619]\n",
      "[Epoch 0/200] [Batch 591/3166] [D loss: 0.695549] [G loss: 0.673538]\n",
      "[Epoch 0/200] [Batch 592/3166] [D loss: 0.689803] [G loss: 0.671675]\n",
      "[Epoch 0/200] [Batch 593/3166] [D loss: 0.694059] [G loss: 0.668396]\n",
      "[Epoch 0/200] [Batch 594/3166] [D loss: 0.694800] [G loss: 0.670312]\n",
      "[Epoch 0/200] [Batch 595/3166] [D loss: 0.698772] [G loss: 0.664851]\n",
      "[Epoch 0/200] [Batch 596/3166] [D loss: 0.697326] [G loss: 0.673622]\n",
      "[Epoch 0/200] [Batch 597/3166] [D loss: 0.695580] [G loss: 0.671378]\n",
      "[Epoch 0/200] [Batch 598/3166] [D loss: 0.699803] [G loss: 0.668347]\n",
      "[Epoch 0/200] [Batch 599/3166] [D loss: 0.694169] [G loss: 0.674640]\n",
      "[Epoch 0/200] [Batch 600/3166] [D loss: 0.697170] [G loss: 0.675571]\n",
      "[Epoch 0/200] [Batch 601/3166] [D loss: 0.697645] [G loss: 0.679011]\n",
      "[Epoch 0/200] [Batch 602/3166] [D loss: 0.697803] [G loss: 0.681326]\n",
      "[Epoch 0/200] [Batch 603/3166] [D loss: 0.695787] [G loss: 0.682948]\n",
      "[Epoch 0/200] [Batch 604/3166] [D loss: 0.697782] [G loss: 0.687267]\n",
      "[Epoch 0/200] [Batch 605/3166] [D loss: 0.694843] [G loss: 0.693442]\n",
      "[Epoch 0/200] [Batch 606/3166] [D loss: 0.694639] [G loss: 0.691114]\n",
      "[Epoch 0/200] [Batch 607/3166] [D loss: 0.691859] [G loss: 0.692610]\n",
      "[Epoch 0/200] [Batch 608/3166] [D loss: 0.691802] [G loss: 0.696570]\n",
      "[Epoch 0/200] [Batch 609/3166] [D loss: 0.691728] [G loss: 0.700824]\n",
      "[Epoch 0/200] [Batch 610/3166] [D loss: 0.690323] [G loss: 0.703890]\n",
      "[Epoch 0/200] [Batch 611/3166] [D loss: 0.689504] [G loss: 0.703496]\n",
      "[Epoch 0/200] [Batch 612/3166] [D loss: 0.685137] [G loss: 0.710460]\n",
      "[Epoch 0/200] [Batch 613/3166] [D loss: 0.689713] [G loss: 0.707582]\n",
      "[Epoch 0/200] [Batch 614/3166] [D loss: 0.690417] [G loss: 0.702398]\n",
      "[Epoch 0/200] [Batch 615/3166] [D loss: 0.690830] [G loss: 0.710944]\n",
      "[Epoch 0/200] [Batch 616/3166] [D loss: 0.684602] [G loss: 0.716810]\n",
      "[Epoch 0/200] [Batch 617/3166] [D loss: 0.685918] [G loss: 0.704083]\n",
      "[Epoch 0/200] [Batch 618/3166] [D loss: 0.684562] [G loss: 0.714738]\n",
      "[Epoch 0/200] [Batch 619/3166] [D loss: 0.691556] [G loss: 0.704484]\n",
      "[Epoch 0/200] [Batch 620/3166] [D loss: 0.691592] [G loss: 0.712672]\n",
      "[Epoch 0/200] [Batch 621/3166] [D loss: 0.690390] [G loss: 0.707149]\n",
      "[Epoch 0/200] [Batch 622/3166] [D loss: 0.691790] [G loss: 0.705421]\n",
      "[Epoch 0/200] [Batch 623/3166] [D loss: 0.691808] [G loss: 0.704535]\n",
      "[Epoch 0/200] [Batch 624/3166] [D loss: 0.693573] [G loss: 0.695889]\n",
      "[Epoch 0/200] [Batch 625/3166] [D loss: 0.691781] [G loss: 0.705830]\n",
      "[Epoch 0/200] [Batch 626/3166] [D loss: 0.694035] [G loss: 0.693159]\n",
      "[Epoch 0/200] [Batch 627/3166] [D loss: 0.698320] [G loss: 0.687807]\n",
      "[Epoch 0/200] [Batch 628/3166] [D loss: 0.699021] [G loss: 0.687437]\n",
      "[Epoch 0/200] [Batch 629/3166] [D loss: 0.698192] [G loss: 0.691885]\n",
      "[Epoch 0/200] [Batch 630/3166] [D loss: 0.697192] [G loss: 0.689628]\n",
      "[Epoch 0/200] [Batch 631/3166] [D loss: 0.697082] [G loss: 0.685662]\n",
      "[Epoch 0/200] [Batch 632/3166] [D loss: 0.693475] [G loss: 0.690282]\n",
      "[Epoch 0/200] [Batch 633/3166] [D loss: 0.699000] [G loss: 0.686411]\n",
      "[Epoch 0/200] [Batch 634/3166] [D loss: 0.698697] [G loss: 0.688764]\n",
      "[Epoch 0/200] [Batch 635/3166] [D loss: 0.696615] [G loss: 0.686283]\n",
      "[Epoch 0/200] [Batch 636/3166] [D loss: 0.697595] [G loss: 0.685406]\n",
      "[Epoch 0/200] [Batch 637/3166] [D loss: 0.693381] [G loss: 0.687449]\n",
      "[Epoch 0/200] [Batch 638/3166] [D loss: 0.694624] [G loss: 0.688570]\n",
      "[Epoch 0/200] [Batch 639/3166] [D loss: 0.694202] [G loss: 0.688193]\n",
      "[Epoch 0/200] [Batch 640/3166] [D loss: 0.694263] [G loss: 0.689044]\n",
      "[Epoch 0/200] [Batch 641/3166] [D loss: 0.692253] [G loss: 0.691199]\n",
      "[Epoch 0/200] [Batch 642/3166] [D loss: 0.693368] [G loss: 0.690424]\n",
      "[Epoch 0/200] [Batch 643/3166] [D loss: 0.690015] [G loss: 0.692428]\n",
      "[Epoch 0/200] [Batch 644/3166] [D loss: 0.690933] [G loss: 0.692860]\n",
      "[Epoch 0/200] [Batch 645/3166] [D loss: 0.691040] [G loss: 0.689991]\n",
      "[Epoch 0/200] [Batch 646/3166] [D loss: 0.691256] [G loss: 0.692919]\n",
      "[Epoch 0/200] [Batch 647/3166] [D loss: 0.688296] [G loss: 0.695132]\n",
      "[Epoch 0/200] [Batch 648/3166] [D loss: 0.686160] [G loss: 0.692658]\n",
      "[Epoch 0/200] [Batch 649/3166] [D loss: 0.685087] [G loss: 0.693721]\n",
      "[Epoch 0/200] [Batch 650/3166] [D loss: 0.688327] [G loss: 0.692280]\n",
      "[Epoch 0/200] [Batch 651/3166] [D loss: 0.684899] [G loss: 0.693005]\n",
      "[Epoch 0/200] [Batch 652/3166] [D loss: 0.682405] [G loss: 0.694509]\n",
      "[Epoch 0/200] [Batch 653/3166] [D loss: 0.688686] [G loss: 0.691490]\n",
      "[Epoch 0/200] [Batch 654/3166] [D loss: 0.684974] [G loss: 0.692085]\n",
      "[Epoch 0/200] [Batch 655/3166] [D loss: 0.678719] [G loss: 0.693154]\n",
      "[Epoch 0/200] [Batch 656/3166] [D loss: 0.682216] [G loss: 0.692754]\n",
      "[Epoch 0/200] [Batch 657/3166] [D loss: 0.681336] [G loss: 0.696613]\n",
      "[Epoch 0/200] [Batch 658/3166] [D loss: 0.681017] [G loss: 0.696711]\n",
      "[Epoch 0/200] [Batch 659/3166] [D loss: 0.673412] [G loss: 0.695352]\n",
      "[Epoch 0/200] [Batch 660/3166] [D loss: 0.683311] [G loss: 0.694305]\n",
      "[Epoch 0/200] [Batch 661/3166] [D loss: 0.678736] [G loss: 0.694847]\n",
      "[Epoch 0/200] [Batch 662/3166] [D loss: 0.671814] [G loss: 0.692695]\n",
      "[Epoch 0/200] [Batch 663/3166] [D loss: 0.665548] [G loss: 0.692233]\n",
      "[Epoch 0/200] [Batch 664/3166] [D loss: 0.672068] [G loss: 0.691422]\n",
      "[Epoch 0/200] [Batch 665/3166] [D loss: 0.680539] [G loss: 0.689864]\n",
      "[Epoch 0/200] [Batch 666/3166] [D loss: 0.666631] [G loss: 0.686924]\n",
      "[Epoch 0/200] [Batch 667/3166] [D loss: 0.694098] [G loss: 0.667470]\n",
      "[Epoch 0/200] [Batch 668/3166] [D loss: 0.686645] [G loss: 0.663411]\n",
      "[Epoch 0/200] [Batch 669/3166] [D loss: 0.716156] [G loss: 0.624512]\n",
      "[Epoch 0/200] [Batch 670/3166] [D loss: 0.717007] [G loss: 0.610804]\n",
      "[Epoch 0/200] [Batch 671/3166] [D loss: 0.718911] [G loss: 0.609952]\n",
      "[Epoch 0/200] [Batch 672/3166] [D loss: 0.727595] [G loss: 0.619651]\n",
      "[Epoch 0/200] [Batch 673/3166] [D loss: 0.719402] [G loss: 0.626616]\n",
      "[Epoch 0/200] [Batch 674/3166] [D loss: 0.731411] [G loss: 0.632787]\n",
      "[Epoch 0/200] [Batch 675/3166] [D loss: 0.695203] [G loss: 0.674838]\n",
      "[Epoch 0/200] [Batch 676/3166] [D loss: 0.707199] [G loss: 0.687760]\n",
      "[Epoch 0/200] [Batch 677/3166] [D loss: 0.700816] [G loss: 0.694694]\n",
      "[Epoch 0/200] [Batch 678/3166] [D loss: 0.699652] [G loss: 0.700807]\n",
      "[Epoch 0/200] [Batch 679/3166] [D loss: 0.695731] [G loss: 0.714704]\n",
      "[Epoch 0/200] [Batch 680/3166] [D loss: 0.694167] [G loss: 0.718270]\n",
      "[Epoch 0/200] [Batch 681/3166] [D loss: 0.690882] [G loss: 0.726004]\n",
      "[Epoch 0/200] [Batch 682/3166] [D loss: 0.689771] [G loss: 0.733924]\n",
      "[Epoch 0/200] [Batch 683/3166] [D loss: 0.692084] [G loss: 0.726964]\n",
      "[Epoch 0/200] [Batch 684/3166] [D loss: 0.682951] [G loss: 0.750322]\n",
      "[Epoch 0/200] [Batch 685/3166] [D loss: 0.679769] [G loss: 0.751293]\n",
      "[Epoch 0/200] [Batch 686/3166] [D loss: 0.670858] [G loss: 0.779008]\n",
      "[Epoch 0/200] [Batch 687/3166] [D loss: 0.676451] [G loss: 0.761282]\n",
      "[Epoch 0/200] [Batch 688/3166] [D loss: 0.679198] [G loss: 0.761379]\n",
      "[Epoch 0/200] [Batch 689/3166] [D loss: 0.700086] [G loss: 0.737348]\n",
      "[Epoch 0/200] [Batch 690/3166] [D loss: 0.677023] [G loss: 0.775309]\n",
      "[Epoch 0/200] [Batch 691/3166] [D loss: 0.685647] [G loss: 0.757985]\n",
      "[Epoch 0/200] [Batch 692/3166] [D loss: 0.674716] [G loss: 0.767531]\n",
      "[Epoch 0/200] [Batch 693/3166] [D loss: 0.668485] [G loss: 0.775686]\n",
      "[Epoch 0/200] [Batch 694/3166] [D loss: 0.688216] [G loss: 0.760097]\n",
      "[Epoch 0/200] [Batch 695/3166] [D loss: 0.690791] [G loss: 0.731801]\n",
      "[Epoch 0/200] [Batch 696/3166] [D loss: 0.696571] [G loss: 0.737853]\n",
      "[Epoch 0/200] [Batch 697/3166] [D loss: 0.691336] [G loss: 0.733062]\n",
      "[Epoch 0/200] [Batch 698/3166] [D loss: 0.710358] [G loss: 0.715542]\n",
      "[Epoch 0/200] [Batch 699/3166] [D loss: 0.706834] [G loss: 0.699628]\n",
      "[Epoch 0/200] [Batch 700/3166] [D loss: 0.712260] [G loss: 0.688591]\n",
      "[Epoch 0/200] [Batch 701/3166] [D loss: 0.702348] [G loss: 0.680477]\n",
      "[Epoch 0/200] [Batch 702/3166] [D loss: 0.708344] [G loss: 0.681790]\n",
      "[Epoch 0/200] [Batch 703/3166] [D loss: 0.703812] [G loss: 0.684323]\n",
      "[Epoch 0/200] [Batch 704/3166] [D loss: 0.703216] [G loss: 0.681824]\n",
      "[Epoch 0/200] [Batch 705/3166] [D loss: 0.697093] [G loss: 0.683549]\n",
      "[Epoch 0/200] [Batch 706/3166] [D loss: 0.697399] [G loss: 0.681520]\n",
      "[Epoch 0/200] [Batch 707/3166] [D loss: 0.694628] [G loss: 0.688041]\n",
      "[Epoch 0/200] [Batch 708/3166] [D loss: 0.689230] [G loss: 0.683115]\n",
      "[Epoch 0/200] [Batch 709/3166] [D loss: 0.688419] [G loss: 0.687712]\n",
      "[Epoch 0/200] [Batch 710/3166] [D loss: 0.687773] [G loss: 0.691870]\n",
      "[Epoch 0/200] [Batch 711/3166] [D loss: 0.687119] [G loss: 0.688740]\n",
      "[Epoch 0/200] [Batch 712/3166] [D loss: 0.680588] [G loss: 0.690158]\n",
      "[Epoch 0/200] [Batch 713/3166] [D loss: 0.679342] [G loss: 0.687611]\n",
      "[Epoch 0/200] [Batch 714/3166] [D loss: 0.679114] [G loss: 0.692337]\n",
      "[Epoch 0/200] [Batch 715/3166] [D loss: 0.678967] [G loss: 0.688774]\n",
      "[Epoch 0/200] [Batch 716/3166] [D loss: 0.681010] [G loss: 0.683793]\n",
      "[Epoch 0/200] [Batch 717/3166] [D loss: 0.669242] [G loss: 0.687822]\n",
      "[Epoch 0/200] [Batch 718/3166] [D loss: 0.676209] [G loss: 0.682871]\n",
      "[Epoch 0/200] [Batch 719/3166] [D loss: 0.677664] [G loss: 0.679770]\n",
      "[Epoch 0/200] [Batch 720/3166] [D loss: 0.680212] [G loss: 0.675454]\n",
      "[Epoch 0/200] [Batch 721/3166] [D loss: 0.676332] [G loss: 0.674268]\n",
      "[Epoch 0/200] [Batch 722/3166] [D loss: 0.683375] [G loss: 0.669864]\n",
      "[Epoch 0/200] [Batch 723/3166] [D loss: 0.668728] [G loss: 0.655387]\n",
      "[Epoch 0/200] [Batch 724/3166] [D loss: 0.681934] [G loss: 0.649280]\n",
      "[Epoch 0/200] [Batch 725/3166] [D loss: 0.679037] [G loss: 0.648146]\n",
      "[Epoch 0/200] [Batch 726/3166] [D loss: 0.685799] [G loss: 0.638041]\n",
      "[Epoch 0/200] [Batch 727/3166] [D loss: 0.705230] [G loss: 0.631321]\n",
      "[Epoch 0/200] [Batch 728/3166] [D loss: 0.707296] [G loss: 0.649681]\n",
      "[Epoch 0/200] [Batch 729/3166] [D loss: 0.701234] [G loss: 0.644417]\n",
      "[Epoch 0/200] [Batch 730/3166] [D loss: 0.706369] [G loss: 0.662064]\n",
      "[Epoch 0/200] [Batch 731/3166] [D loss: 0.705141] [G loss: 0.674909]\n",
      "[Epoch 0/200] [Batch 732/3166] [D loss: 0.696020] [G loss: 0.689045]\n",
      "[Epoch 0/200] [Batch 733/3166] [D loss: 0.693702] [G loss: 0.691290]\n",
      "[Epoch 0/200] [Batch 734/3166] [D loss: 0.698209] [G loss: 0.687602]\n",
      "[Epoch 0/200] [Batch 735/3166] [D loss: 0.700018] [G loss: 0.677836]\n",
      "[Epoch 0/200] [Batch 736/3166] [D loss: 0.712038] [G loss: 0.687168]\n",
      "[Epoch 0/200] [Batch 737/3166] [D loss: 0.702410] [G loss: 0.689248]\n",
      "[Epoch 0/200] [Batch 738/3166] [D loss: 0.699625] [G loss: 0.688975]\n",
      "[Epoch 0/200] [Batch 739/3166] [D loss: 0.700973] [G loss: 0.689562]\n",
      "[Epoch 0/200] [Batch 740/3166] [D loss: 0.694878] [G loss: 0.697408]\n",
      "[Epoch 0/200] [Batch 741/3166] [D loss: 0.692774] [G loss: 0.702851]\n",
      "[Epoch 0/200] [Batch 742/3166] [D loss: 0.685845] [G loss: 0.721954]\n",
      "[Epoch 0/200] [Batch 743/3166] [D loss: 0.686396] [G loss: 0.722766]\n",
      "[Epoch 0/200] [Batch 744/3166] [D loss: 0.680200] [G loss: 0.739710]\n",
      "[Epoch 0/200] [Batch 745/3166] [D loss: 0.678896] [G loss: 0.728626]\n",
      "[Epoch 0/200] [Batch 746/3166] [D loss: 0.687094] [G loss: 0.721541]\n",
      "[Epoch 0/200] [Batch 747/3166] [D loss: 0.686967] [G loss: 0.719596]\n",
      "[Epoch 0/200] [Batch 748/3166] [D loss: 0.694727] [G loss: 0.704770]\n",
      "[Epoch 0/200] [Batch 749/3166] [D loss: 0.694282] [G loss: 0.720065]\n",
      "[Epoch 0/200] [Batch 750/3166] [D loss: 0.695813] [G loss: 0.712413]\n",
      "[Epoch 0/200] [Batch 751/3166] [D loss: 0.695320] [G loss: 0.715259]\n",
      "[Epoch 0/200] [Batch 752/3166] [D loss: 0.694956] [G loss: 0.716020]\n",
      "[Epoch 0/200] [Batch 753/3166] [D loss: 0.697430] [G loss: 0.715698]\n",
      "[Epoch 0/200] [Batch 754/3166] [D loss: 0.692234] [G loss: 0.718976]\n",
      "[Epoch 0/200] [Batch 755/3166] [D loss: 0.690109] [G loss: 0.720270]\n",
      "[Epoch 0/200] [Batch 756/3166] [D loss: 0.694078] [G loss: 0.722401]\n",
      "[Epoch 0/200] [Batch 757/3166] [D loss: 0.688459] [G loss: 0.725486]\n",
      "[Epoch 0/200] [Batch 758/3166] [D loss: 0.690057] [G loss: 0.721198]\n",
      "[Epoch 0/200] [Batch 759/3166] [D loss: 0.688173] [G loss: 0.722200]\n",
      "[Epoch 0/200] [Batch 760/3166] [D loss: 0.689205] [G loss: 0.722185]\n",
      "[Epoch 0/200] [Batch 761/3166] [D loss: 0.687730] [G loss: 0.713427]\n",
      "[Epoch 0/200] [Batch 762/3166] [D loss: 0.685895] [G loss: 0.718919]\n",
      "[Epoch 0/200] [Batch 763/3166] [D loss: 0.689388] [G loss: 0.709533]\n",
      "[Epoch 0/200] [Batch 764/3166] [D loss: 0.688620] [G loss: 0.719448]\n",
      "[Epoch 0/200] [Batch 765/3166] [D loss: 0.688944] [G loss: 0.718062]\n",
      "[Epoch 0/200] [Batch 766/3166] [D loss: 0.685009] [G loss: 0.714519]\n",
      "[Epoch 0/200] [Batch 767/3166] [D loss: 0.682461] [G loss: 0.715312]\n",
      "[Epoch 0/200] [Batch 768/3166] [D loss: 0.684110] [G loss: 0.716335]\n",
      "[Epoch 0/200] [Batch 769/3166] [D loss: 0.684854] [G loss: 0.707633]\n",
      "[Epoch 0/200] [Batch 770/3166] [D loss: 0.679587] [G loss: 0.711351]\n",
      "[Epoch 0/200] [Batch 771/3166] [D loss: 0.673943] [G loss: 0.710802]\n",
      "[Epoch 0/200] [Batch 772/3166] [D loss: 0.669932] [G loss: 0.713301]\n",
      "[Epoch 0/200] [Batch 773/3166] [D loss: 0.668696] [G loss: 0.705742]\n",
      "[Epoch 0/200] [Batch 774/3166] [D loss: 0.661239] [G loss: 0.705938]\n",
      "[Epoch 0/200] [Batch 775/3166] [D loss: 0.666062] [G loss: 0.705803]\n",
      "[Epoch 0/200] [Batch 776/3166] [D loss: 0.665960] [G loss: 0.686830]\n",
      "[Epoch 0/200] [Batch 777/3166] [D loss: 0.663034] [G loss: 0.697238]\n",
      "[Epoch 0/200] [Batch 778/3166] [D loss: 0.659679] [G loss: 0.686495]\n",
      "[Epoch 0/200] [Batch 779/3166] [D loss: 0.668208] [G loss: 0.659880]\n",
      "[Epoch 0/200] [Batch 780/3166] [D loss: 0.681942] [G loss: 0.641627]\n",
      "[Epoch 0/200] [Batch 781/3166] [D loss: 0.672505] [G loss: 0.668606]\n",
      "[Epoch 0/200] [Batch 782/3166] [D loss: 0.680076] [G loss: 0.655715]\n",
      "[Epoch 0/200] [Batch 783/3166] [D loss: 0.683140] [G loss: 0.657823]\n",
      "[Epoch 0/200] [Batch 784/3166] [D loss: 0.689467] [G loss: 0.651441]\n",
      "[Epoch 0/200] [Batch 785/3166] [D loss: 0.677916] [G loss: 0.683279]\n",
      "[Epoch 0/200] [Batch 786/3166] [D loss: 0.705362] [G loss: 0.670913]\n",
      "[Epoch 0/200] [Batch 787/3166] [D loss: 0.677945] [G loss: 0.681074]\n",
      "[Epoch 0/200] [Batch 788/3166] [D loss: 0.684670] [G loss: 0.677979]\n",
      "[Epoch 0/200] [Batch 789/3166] [D loss: 0.681415] [G loss: 0.728651]\n",
      "[Epoch 0/200] [Batch 790/3166] [D loss: 0.686565] [G loss: 0.737437]\n",
      "[Epoch 0/200] [Batch 791/3166] [D loss: 0.681798] [G loss: 0.791929]\n",
      "[Epoch 0/200] [Batch 792/3166] [D loss: 0.672475] [G loss: 0.781301]\n",
      "[Epoch 0/200] [Batch 793/3166] [D loss: 0.675690] [G loss: 0.805423]\n",
      "[Epoch 0/200] [Batch 794/3166] [D loss: 0.668889] [G loss: 0.807788]\n",
      "[Epoch 0/200] [Batch 795/3166] [D loss: 0.676942] [G loss: 0.850335]\n",
      "[Epoch 0/200] [Batch 796/3166] [D loss: 0.667713] [G loss: 0.839173]\n",
      "[Epoch 0/200] [Batch 797/3166] [D loss: 0.652785] [G loss: 0.857122]\n",
      "[Epoch 0/200] [Batch 798/3166] [D loss: 0.648014] [G loss: 0.869464]\n",
      "[Epoch 0/200] [Batch 799/3166] [D loss: 0.694072] [G loss: 0.830600]\n",
      "[Epoch 0/200] [Batch 800/3166] [D loss: 0.731239] [G loss: 0.770426]\n",
      "[Epoch 0/200] [Batch 801/3166] [D loss: 0.718679] [G loss: 0.763853]\n",
      "[Epoch 0/200] [Batch 802/3166] [D loss: 0.729071] [G loss: 0.735144]\n",
      "[Epoch 0/200] [Batch 803/3166] [D loss: 0.716011] [G loss: 0.729028]\n",
      "[Epoch 0/200] [Batch 804/3166] [D loss: 0.731037] [G loss: 0.657463]\n",
      "[Epoch 0/200] [Batch 805/3166] [D loss: 0.716336] [G loss: 0.670671]\n",
      "[Epoch 0/200] [Batch 806/3166] [D loss: 0.700270] [G loss: 0.685285]\n",
      "[Epoch 0/200] [Batch 807/3166] [D loss: 0.725927] [G loss: 0.647698]\n",
      "[Epoch 0/200] [Batch 808/3166] [D loss: 0.708793] [G loss: 0.667546]\n",
      "[Epoch 0/200] [Batch 809/3166] [D loss: 0.701804] [G loss: 0.670291]\n",
      "[Epoch 0/200] [Batch 810/3166] [D loss: 0.706422] [G loss: 0.666744]\n",
      "[Epoch 0/200] [Batch 811/3166] [D loss: 0.700619] [G loss: 0.675131]\n",
      "[Epoch 0/200] [Batch 812/3166] [D loss: 0.702573] [G loss: 0.676072]\n",
      "[Epoch 0/200] [Batch 813/3166] [D loss: 0.686609] [G loss: 0.687207]\n",
      "[Epoch 0/200] [Batch 814/3166] [D loss: 0.689329] [G loss: 0.681184]\n",
      "[Epoch 0/200] [Batch 815/3166] [D loss: 0.682488] [G loss: 0.695652]\n",
      "[Epoch 0/200] [Batch 816/3166] [D loss: 0.681749] [G loss: 0.694423]\n",
      "[Epoch 0/200] [Batch 817/3166] [D loss: 0.689403] [G loss: 0.694044]\n",
      "[Epoch 0/200] [Batch 818/3166] [D loss: 0.683307] [G loss: 0.692757]\n",
      "[Epoch 0/200] [Batch 819/3166] [D loss: 0.673163] [G loss: 0.709416]\n",
      "[Epoch 0/200] [Batch 820/3166] [D loss: 0.679277] [G loss: 0.682577]\n",
      "[Epoch 0/200] [Batch 821/3166] [D loss: 0.685871] [G loss: 0.696784]\n",
      "[Epoch 0/200] [Batch 822/3166] [D loss: 0.695637] [G loss: 0.658673]\n",
      "[Epoch 0/200] [Batch 823/3166] [D loss: 0.688846] [G loss: 0.668292]\n",
      "[Epoch 0/200] [Batch 824/3166] [D loss: 0.703008] [G loss: 0.646687]\n",
      "[Epoch 0/200] [Batch 825/3166] [D loss: 0.680087] [G loss: 0.687314]\n",
      "[Epoch 0/200] [Batch 826/3166] [D loss: 0.684476] [G loss: 0.684723]\n",
      "[Epoch 0/200] [Batch 827/3166] [D loss: 0.692694] [G loss: 0.670445]\n",
      "[Epoch 0/200] [Batch 828/3166] [D loss: 0.699608] [G loss: 0.685578]\n",
      "[Epoch 0/200] [Batch 829/3166] [D loss: 0.689785] [G loss: 0.690732]\n",
      "[Epoch 0/200] [Batch 830/3166] [D loss: 0.690660] [G loss: 0.670454]\n",
      "[Epoch 0/200] [Batch 831/3166] [D loss: 0.706767] [G loss: 0.657220]\n",
      "[Epoch 0/200] [Batch 832/3166] [D loss: 0.710146] [G loss: 0.667913]\n",
      "[Epoch 0/200] [Batch 833/3166] [D loss: 0.700012] [G loss: 0.675363]\n",
      "[Epoch 0/200] [Batch 834/3166] [D loss: 0.700216] [G loss: 0.681072]\n",
      "[Epoch 0/200] [Batch 835/3166] [D loss: 0.696885] [G loss: 0.689727]\n",
      "[Epoch 0/200] [Batch 836/3166] [D loss: 0.700562] [G loss: 0.684499]\n",
      "[Epoch 0/200] [Batch 837/3166] [D loss: 0.697143] [G loss: 0.691059]\n",
      "[Epoch 0/200] [Batch 838/3166] [D loss: 0.700362] [G loss: 0.688778]\n",
      "[Epoch 0/200] [Batch 839/3166] [D loss: 0.695521] [G loss: 0.689854]\n",
      "[Epoch 0/200] [Batch 840/3166] [D loss: 0.698743] [G loss: 0.684820]\n",
      "[Epoch 0/200] [Batch 841/3166] [D loss: 0.697021] [G loss: 0.687179]\n",
      "[Epoch 0/200] [Batch 842/3166] [D loss: 0.689417] [G loss: 0.705202]\n",
      "[Epoch 0/200] [Batch 843/3166] [D loss: 0.698397] [G loss: 0.690244]\n",
      "[Epoch 0/200] [Batch 844/3166] [D loss: 0.694186] [G loss: 0.701377]\n",
      "[Epoch 0/200] [Batch 845/3166] [D loss: 0.695117] [G loss: 0.698980]\n",
      "[Epoch 0/200] [Batch 846/3166] [D loss: 0.693399] [G loss: 0.697812]\n",
      "[Epoch 0/200] [Batch 847/3166] [D loss: 0.697618] [G loss: 0.698037]\n",
      "[Epoch 0/200] [Batch 848/3166] [D loss: 0.694298] [G loss: 0.696781]\n",
      "[Epoch 0/200] [Batch 849/3166] [D loss: 0.691709] [G loss: 0.703934]\n",
      "[Epoch 0/200] [Batch 850/3166] [D loss: 0.688968] [G loss: 0.698601]\n",
      "[Epoch 0/200] [Batch 851/3166] [D loss: 0.694535] [G loss: 0.694027]\n",
      "[Epoch 0/200] [Batch 852/3166] [D loss: 0.688600] [G loss: 0.711581]\n",
      "[Epoch 0/200] [Batch 853/3166] [D loss: 0.684856] [G loss: 0.715221]\n",
      "[Epoch 0/200] [Batch 854/3166] [D loss: 0.683302] [G loss: 0.704011]\n",
      "[Epoch 0/200] [Batch 855/3166] [D loss: 0.685136] [G loss: 0.714958]\n",
      "[Epoch 0/200] [Batch 856/3166] [D loss: 0.690896] [G loss: 0.705543]\n",
      "[Epoch 0/200] [Batch 857/3166] [D loss: 0.692736] [G loss: 0.708214]\n",
      "[Epoch 0/200] [Batch 858/3166] [D loss: 0.686061] [G loss: 0.710196]\n",
      "[Epoch 0/200] [Batch 859/3166] [D loss: 0.688455] [G loss: 0.702860]\n",
      "[Epoch 0/200] [Batch 860/3166] [D loss: 0.684389] [G loss: 0.701419]\n",
      "[Epoch 0/200] [Batch 861/3166] [D loss: 0.680786] [G loss: 0.700683]\n",
      "[Epoch 0/200] [Batch 862/3166] [D loss: 0.688564] [G loss: 0.694641]\n",
      "[Epoch 0/200] [Batch 863/3166] [D loss: 0.682934] [G loss: 0.700803]\n",
      "[Epoch 0/200] [Batch 864/3166] [D loss: 0.685281] [G loss: 0.704028]\n",
      "[Epoch 0/200] [Batch 865/3166] [D loss: 0.685897] [G loss: 0.696925]\n",
      "[Epoch 0/200] [Batch 866/3166] [D loss: 0.684277] [G loss: 0.697043]\n",
      "[Epoch 0/200] [Batch 867/3166] [D loss: 0.683227] [G loss: 0.693609]\n",
      "[Epoch 0/200] [Batch 868/3166] [D loss: 0.688528] [G loss: 0.692968]\n",
      "[Epoch 0/200] [Batch 869/3166] [D loss: 0.686130] [G loss: 0.696379]\n",
      "[Epoch 0/200] [Batch 870/3166] [D loss: 0.686890] [G loss: 0.693753]\n",
      "[Epoch 0/200] [Batch 871/3166] [D loss: 0.680204] [G loss: 0.699836]\n",
      "[Epoch 0/200] [Batch 872/3166] [D loss: 0.680990] [G loss: 0.697065]\n",
      "[Epoch 0/200] [Batch 873/3166] [D loss: 0.681965] [G loss: 0.691482]\n",
      "[Epoch 0/200] [Batch 874/3166] [D loss: 0.679753] [G loss: 0.690848]\n",
      "[Epoch 0/200] [Batch 875/3166] [D loss: 0.680270] [G loss: 0.691761]\n",
      "[Epoch 0/200] [Batch 876/3166] [D loss: 0.684298] [G loss: 0.694425]\n",
      "[Epoch 0/200] [Batch 877/3166] [D loss: 0.676132] [G loss: 0.679363]\n",
      "[Epoch 0/200] [Batch 878/3166] [D loss: 0.691370] [G loss: 0.672470]\n",
      "[Epoch 0/200] [Batch 879/3166] [D loss: 0.699527] [G loss: 0.657443]\n",
      "[Epoch 0/200] [Batch 880/3166] [D loss: 0.688154] [G loss: 0.667572]\n",
      "[Epoch 0/200] [Batch 881/3166] [D loss: 0.693094] [G loss: 0.665310]\n",
      "[Epoch 0/200] [Batch 882/3166] [D loss: 0.695885] [G loss: 0.670456]\n",
      "[Epoch 0/200] [Batch 883/3166] [D loss: 0.700861] [G loss: 0.661526]\n",
      "[Epoch 0/200] [Batch 884/3166] [D loss: 0.700637] [G loss: 0.670841]\n",
      "[Epoch 0/200] [Batch 885/3166] [D loss: 0.689911] [G loss: 0.666787]\n",
      "[Epoch 0/200] [Batch 886/3166] [D loss: 0.691048] [G loss: 0.670768]\n",
      "[Epoch 0/200] [Batch 887/3166] [D loss: 0.704607] [G loss: 0.675573]\n",
      "[Epoch 0/200] [Batch 888/3166] [D loss: 0.694689] [G loss: 0.677438]\n",
      "[Epoch 0/200] [Batch 889/3166] [D loss: 0.702508] [G loss: 0.668691]\n",
      "[Epoch 0/200] [Batch 890/3166] [D loss: 0.697926] [G loss: 0.673775]\n",
      "[Epoch 0/200] [Batch 891/3166] [D loss: 0.692548] [G loss: 0.680829]\n",
      "[Epoch 0/200] [Batch 892/3166] [D loss: 0.688409] [G loss: 0.689766]\n",
      "[Epoch 0/200] [Batch 893/3166] [D loss: 0.690373] [G loss: 0.685372]\n",
      "[Epoch 0/200] [Batch 894/3166] [D loss: 0.689994] [G loss: 0.688278]\n",
      "[Epoch 0/200] [Batch 895/3166] [D loss: 0.686443] [G loss: 0.695667]\n",
      "[Epoch 0/200] [Batch 896/3166] [D loss: 0.683872] [G loss: 0.687174]\n",
      "[Epoch 0/200] [Batch 897/3166] [D loss: 0.677371] [G loss: 0.688871]\n",
      "[Epoch 0/200] [Batch 898/3166] [D loss: 0.682408] [G loss: 0.690904]\n",
      "[Epoch 0/200] [Batch 899/3166] [D loss: 0.681121] [G loss: 0.689752]\n",
      "[Epoch 0/200] [Batch 900/3166] [D loss: 0.680681] [G loss: 0.683886]\n",
      "[Epoch 0/200] [Batch 901/3166] [D loss: 0.669424] [G loss: 0.692444]\n",
      "[Epoch 0/200] [Batch 902/3166] [D loss: 0.677876] [G loss: 0.684812]\n",
      "[Epoch 0/200] [Batch 903/3166] [D loss: 0.672175] [G loss: 0.692609]\n",
      "[Epoch 0/200] [Batch 904/3166] [D loss: 0.675111] [G loss: 0.687024]\n",
      "[Epoch 0/200] [Batch 905/3166] [D loss: 0.683532] [G loss: 0.673719]\n",
      "[Epoch 0/200] [Batch 906/3166] [D loss: 0.673563] [G loss: 0.683652]\n",
      "[Epoch 0/200] [Batch 907/3166] [D loss: 0.666696] [G loss: 0.680635]\n",
      "[Epoch 0/200] [Batch 908/3166] [D loss: 0.674669] [G loss: 0.667391]\n",
      "[Epoch 0/200] [Batch 909/3166] [D loss: 0.683512] [G loss: 0.656441]\n",
      "[Epoch 0/200] [Batch 910/3166] [D loss: 0.707209] [G loss: 0.636312]\n",
      "[Epoch 0/200] [Batch 911/3166] [D loss: 0.689733] [G loss: 0.624256]\n",
      "[Epoch 0/200] [Batch 912/3166] [D loss: 0.743118] [G loss: 0.588221]\n",
      "[Epoch 0/200] [Batch 913/3166] [D loss: 0.717879] [G loss: 0.600998]\n",
      "[Epoch 0/200] [Batch 914/3166] [D loss: 0.744708] [G loss: 0.599080]\n",
      "[Epoch 0/200] [Batch 915/3166] [D loss: 0.711858] [G loss: 0.653571]\n",
      "[Epoch 0/200] [Batch 916/3166] [D loss: 0.708356] [G loss: 0.678873]\n",
      "[Epoch 0/200] [Batch 917/3166] [D loss: 0.689978] [G loss: 0.702575]\n",
      "[Epoch 0/200] [Batch 918/3166] [D loss: 0.677908] [G loss: 0.742419]\n",
      "[Epoch 0/200] [Batch 919/3166] [D loss: 0.686827] [G loss: 0.742508]\n",
      "[Epoch 0/200] [Batch 920/3166] [D loss: 0.680131] [G loss: 0.785099]\n",
      "[Epoch 0/200] [Batch 921/3166] [D loss: 0.662105] [G loss: 0.809841]\n",
      "[Epoch 0/200] [Batch 922/3166] [D loss: 0.667383] [G loss: 0.812157]\n",
      "[Epoch 0/200] [Batch 923/3166] [D loss: 0.708260] [G loss: 0.798228]\n",
      "[Epoch 0/200] [Batch 924/3166] [D loss: 0.682730] [G loss: 0.778359]\n",
      "[Epoch 0/200] [Batch 925/3166] [D loss: 0.696629] [G loss: 0.727979]\n",
      "[Epoch 0/200] [Batch 926/3166] [D loss: 0.690080] [G loss: 0.736645]\n",
      "[Epoch 0/200] [Batch 927/3166] [D loss: 0.704584] [G loss: 0.736598]\n",
      "[Epoch 0/200] [Batch 928/3166] [D loss: 0.694511] [G loss: 0.739615]\n",
      "[Epoch 0/200] [Batch 929/3166] [D loss: 0.700280] [G loss: 0.716653]\n",
      "[Epoch 0/200] [Batch 930/3166] [D loss: 0.697113] [G loss: 0.720331]\n",
      "[Epoch 0/200] [Batch 931/3166] [D loss: 0.692272] [G loss: 0.710270]\n",
      "[Epoch 0/200] [Batch 932/3166] [D loss: 0.692416] [G loss: 0.713581]\n",
      "[Epoch 0/200] [Batch 933/3166] [D loss: 0.684534] [G loss: 0.713218]\n",
      "[Epoch 0/200] [Batch 934/3166] [D loss: 0.679438] [G loss: 0.715034]\n",
      "[Epoch 0/200] [Batch 935/3166] [D loss: 0.678177] [G loss: 0.709146]\n",
      "[Epoch 0/200] [Batch 936/3166] [D loss: 0.677809] [G loss: 0.701555]\n",
      "[Epoch 0/200] [Batch 937/3166] [D loss: 0.678227] [G loss: 0.705677]\n",
      "[Epoch 0/200] [Batch 938/3166] [D loss: 0.695681] [G loss: 0.701479]\n",
      "[Epoch 0/200] [Batch 939/3166] [D loss: 0.685499] [G loss: 0.686572]\n",
      "[Epoch 0/200] [Batch 940/3166] [D loss: 0.695091] [G loss: 0.680446]\n",
      "[Epoch 0/200] [Batch 941/3166] [D loss: 0.699413] [G loss: 0.673678]\n",
      "[Epoch 0/200] [Batch 942/3166] [D loss: 0.705321] [G loss: 0.667969]\n",
      "[Epoch 0/200] [Batch 943/3166] [D loss: 0.696673] [G loss: 0.685130]\n",
      "[Epoch 0/200] [Batch 944/3166] [D loss: 0.696861] [G loss: 0.694315]\n",
      "[Epoch 0/200] [Batch 945/3166] [D loss: 0.684539] [G loss: 0.699851]\n",
      "[Epoch 0/200] [Batch 946/3166] [D loss: 0.691950] [G loss: 0.705325]\n",
      "[Epoch 0/200] [Batch 947/3166] [D loss: 0.683936] [G loss: 0.721969]\n",
      "[Epoch 0/200] [Batch 948/3166] [D loss: 0.677652] [G loss: 0.730443]\n",
      "[Epoch 0/200] [Batch 949/3166] [D loss: 0.693236] [G loss: 0.727274]\n",
      "[Epoch 0/200] [Batch 950/3166] [D loss: 0.682812] [G loss: 0.726531]\n",
      "[Epoch 0/200] [Batch 951/3166] [D loss: 0.688720] [G loss: 0.714666]\n",
      "[Epoch 0/200] [Batch 952/3166] [D loss: 0.694515] [G loss: 0.708528]\n",
      "[Epoch 0/200] [Batch 953/3166] [D loss: 0.691028] [G loss: 0.715590]\n",
      "[Epoch 0/200] [Batch 954/3166] [D loss: 0.678390] [G loss: 0.714156]\n",
      "[Epoch 0/200] [Batch 955/3166] [D loss: 0.683672] [G loss: 0.706500]\n",
      "[Epoch 0/200] [Batch 956/3166] [D loss: 0.689713] [G loss: 0.698628]\n",
      "[Epoch 0/200] [Batch 957/3166] [D loss: 0.681032] [G loss: 0.711384]\n",
      "[Epoch 0/200] [Batch 958/3166] [D loss: 0.686092] [G loss: 0.699167]\n",
      "[Epoch 0/200] [Batch 959/3166] [D loss: 0.687875] [G loss: 0.691564]\n",
      "[Epoch 0/200] [Batch 960/3166] [D loss: 0.693596] [G loss: 0.693235]\n",
      "[Epoch 0/200] [Batch 961/3166] [D loss: 0.693839] [G loss: 0.700736]\n",
      "[Epoch 0/200] [Batch 962/3166] [D loss: 0.691644] [G loss: 0.696425]\n",
      "[Epoch 0/200] [Batch 963/3166] [D loss: 0.690084] [G loss: 0.704105]\n",
      "[Epoch 0/200] [Batch 964/3166] [D loss: 0.687783] [G loss: 0.709526]\n",
      "[Epoch 0/200] [Batch 965/3166] [D loss: 0.683127] [G loss: 0.720532]\n",
      "[Epoch 0/200] [Batch 966/3166] [D loss: 0.683974] [G loss: 0.734190]\n",
      "[Epoch 0/200] [Batch 967/3166] [D loss: 0.694156] [G loss: 0.732548]\n",
      "[Epoch 0/200] [Batch 968/3166] [D loss: 0.681134] [G loss: 0.724876]\n",
      "[Epoch 0/200] [Batch 969/3166] [D loss: 0.687898] [G loss: 0.727788]\n",
      "[Epoch 0/200] [Batch 970/3166] [D loss: 0.695459] [G loss: 0.723365]\n",
      "[Epoch 0/200] [Batch 971/3166] [D loss: 0.688513] [G loss: 0.730315]\n",
      "[Epoch 0/200] [Batch 972/3166] [D loss: 0.681707] [G loss: 0.720380]\n",
      "[Epoch 0/200] [Batch 973/3166] [D loss: 0.684443] [G loss: 0.735231]\n",
      "[Epoch 0/200] [Batch 974/3166] [D loss: 0.692418] [G loss: 0.719909]\n",
      "[Epoch 0/200] [Batch 975/3166] [D loss: 0.698218] [G loss: 0.732598]\n",
      "[Epoch 0/200] [Batch 976/3166] [D loss: 0.688867] [G loss: 0.719835]\n",
      "[Epoch 0/200] [Batch 977/3166] [D loss: 0.696725] [G loss: 0.711872]\n",
      "[Epoch 0/200] [Batch 978/3166] [D loss: 0.689342] [G loss: 0.712036]\n",
      "[Epoch 0/200] [Batch 979/3166] [D loss: 0.693179] [G loss: 0.700019]\n",
      "[Epoch 0/200] [Batch 980/3166] [D loss: 0.696994] [G loss: 0.696644]\n",
      "[Epoch 0/200] [Batch 981/3166] [D loss: 0.705652] [G loss: 0.684121]\n",
      "[Epoch 0/200] [Batch 982/3166] [D loss: 0.693417] [G loss: 0.686537]\n",
      "[Epoch 0/200] [Batch 983/3166] [D loss: 0.706621] [G loss: 0.676270]\n",
      "[Epoch 0/200] [Batch 984/3166] [D loss: 0.704459] [G loss: 0.668079]\n",
      "[Epoch 0/200] [Batch 985/3166] [D loss: 0.703894] [G loss: 0.679366]\n",
      "[Epoch 0/200] [Batch 986/3166] [D loss: 0.705540] [G loss: 0.658522]\n",
      "[Epoch 0/200] [Batch 987/3166] [D loss: 0.707474] [G loss: 0.665303]\n",
      "[Epoch 0/200] [Batch 988/3166] [D loss: 0.700910] [G loss: 0.673966]\n",
      "[Epoch 0/200] [Batch 989/3166] [D loss: 0.696351] [G loss: 0.678804]\n",
      "[Epoch 0/200] [Batch 990/3166] [D loss: 0.702388] [G loss: 0.683484]\n",
      "[Epoch 0/200] [Batch 991/3166] [D loss: 0.693390] [G loss: 0.695119]\n",
      "[Epoch 0/200] [Batch 992/3166] [D loss: 0.700520] [G loss: 0.685510]\n",
      "[Epoch 0/200] [Batch 993/3166] [D loss: 0.688910] [G loss: 0.683811]\n",
      "[Epoch 0/200] [Batch 994/3166] [D loss: 0.701820] [G loss: 0.676745]\n",
      "[Epoch 0/200] [Batch 995/3166] [D loss: 0.697754] [G loss: 0.679959]\n",
      "[Epoch 0/200] [Batch 996/3166] [D loss: 0.690914] [G loss: 0.692710]\n",
      "[Epoch 0/200] [Batch 997/3166] [D loss: 0.691920] [G loss: 0.687602]\n",
      "[Epoch 0/200] [Batch 998/3166] [D loss: 0.691827] [G loss: 0.685359]\n",
      "[Epoch 0/200] [Batch 999/3166] [D loss: 0.683797] [G loss: 0.694496]\n",
      "[Epoch 0/200] [Batch 1000/3166] [D loss: 0.695358] [G loss: 0.682330]\n",
      "[Epoch 0/200] [Batch 1001/3166] [D loss: 0.696104] [G loss: 0.677753]\n",
      "[Epoch 0/200] [Batch 1002/3166] [D loss: 0.696367] [G loss: 0.680526]\n",
      "[Epoch 0/200] [Batch 1003/3166] [D loss: 0.693232] [G loss: 0.679913]\n",
      "[Epoch 0/200] [Batch 1004/3166] [D loss: 0.689836] [G loss: 0.688647]\n",
      "[Epoch 0/200] [Batch 1005/3166] [D loss: 0.684471] [G loss: 0.692677]\n",
      "[Epoch 0/200] [Batch 1006/3166] [D loss: 0.694744] [G loss: 0.685084]\n",
      "[Epoch 0/200] [Batch 1007/3166] [D loss: 0.695477] [G loss: 0.687051]\n",
      "[Epoch 0/200] [Batch 1008/3166] [D loss: 0.694383] [G loss: 0.689875]\n",
      "[Epoch 0/200] [Batch 1009/3166] [D loss: 0.690170] [G loss: 0.706207]\n",
      "[Epoch 0/200] [Batch 1010/3166] [D loss: 0.680502] [G loss: 0.711683]\n",
      "[Epoch 0/200] [Batch 1011/3166] [D loss: 0.692567] [G loss: 0.695627]\n",
      "[Epoch 0/200] [Batch 1012/3166] [D loss: 0.693265] [G loss: 0.690263]\n",
      "[Epoch 0/200] [Batch 1013/3166] [D loss: 0.693160] [G loss: 0.698466]\n",
      "[Epoch 0/200] [Batch 1014/3166] [D loss: 0.693480] [G loss: 0.734384]\n",
      "[Epoch 0/200] [Batch 1015/3166] [D loss: 0.711502] [G loss: 0.677562]\n",
      "[Epoch 0/200] [Batch 1016/3166] [D loss: 0.722404] [G loss: 0.664045]\n",
      "[Epoch 0/200] [Batch 1017/3166] [D loss: 0.718854] [G loss: 0.658232]\n",
      "[Epoch 0/200] [Batch 1018/3166] [D loss: 0.736206] [G loss: 0.648745]\n",
      "[Epoch 0/200] [Batch 1019/3166] [D loss: 0.732380] [G loss: 0.648296]\n",
      "[Epoch 0/200] [Batch 1020/3166] [D loss: 0.715497] [G loss: 0.657098]\n",
      "[Epoch 0/200] [Batch 1021/3166] [D loss: 0.716988] [G loss: 0.643998]\n",
      "[Epoch 0/200] [Batch 1022/3166] [D loss: 0.707420] [G loss: 0.662769]\n",
      "[Epoch 0/200] [Batch 1023/3166] [D loss: 0.705008] [G loss: 0.670120]\n",
      "[Epoch 0/200] [Batch 1024/3166] [D loss: 0.700375] [G loss: 0.683140]\n",
      "[Epoch 0/200] [Batch 1025/3166] [D loss: 0.692652] [G loss: 0.699195]\n",
      "[Epoch 0/200] [Batch 1026/3166] [D loss: 0.693796] [G loss: 0.694610]\n",
      "[Epoch 0/200] [Batch 1027/3166] [D loss: 0.678035] [G loss: 0.709500]\n",
      "[Epoch 0/200] [Batch 1028/3166] [D loss: 0.676581] [G loss: 0.709323]\n",
      "[Epoch 0/200] [Batch 1029/3166] [D loss: 0.678555] [G loss: 0.715984]\n",
      "[Epoch 0/200] [Batch 1030/3166] [D loss: 0.674637] [G loss: 0.719985]\n",
      "[Epoch 0/200] [Batch 1031/3166] [D loss: 0.672474] [G loss: 0.710619]\n",
      "[Epoch 0/200] [Batch 1032/3166] [D loss: 0.677142] [G loss: 0.703364]\n",
      "[Epoch 0/200] [Batch 1033/3166] [D loss: 0.674666] [G loss: 0.692177]\n",
      "[Epoch 0/200] [Batch 1034/3166] [D loss: 0.696405] [G loss: 0.669409]\n",
      "[Epoch 0/200] [Batch 1035/3166] [D loss: 0.702609] [G loss: 0.658476]\n",
      "[Epoch 0/200] [Batch 1036/3166] [D loss: 0.691354] [G loss: 0.664972]\n",
      "[Epoch 0/200] [Batch 1037/3166] [D loss: 0.693187] [G loss: 0.649593]\n",
      "[Epoch 0/200] [Batch 1038/3166] [D loss: 0.696229] [G loss: 0.663534]\n",
      "[Epoch 0/200] [Batch 1039/3166] [D loss: 0.717122] [G loss: 0.643205]\n",
      "[Epoch 0/200] [Batch 1040/3166] [D loss: 0.697872] [G loss: 0.655613]\n",
      "[Epoch 0/200] [Batch 1041/3166] [D loss: 0.719069] [G loss: 0.651372]\n",
      "[Epoch 0/200] [Batch 1042/3166] [D loss: 0.700665] [G loss: 0.636739]\n",
      "[Epoch 0/200] [Batch 1043/3166] [D loss: 0.707138] [G loss: 0.653579]\n",
      "[Epoch 0/200] [Batch 1044/3166] [D loss: 0.715975] [G loss: 0.659594]\n",
      "[Epoch 0/200] [Batch 1045/3166] [D loss: 0.700893] [G loss: 0.669649]\n",
      "[Epoch 0/200] [Batch 1046/3166] [D loss: 0.695699] [G loss: 0.692951]\n",
      "[Epoch 0/200] [Batch 1047/3166] [D loss: 0.710545] [G loss: 0.681549]\n",
      "[Epoch 0/200] [Batch 1048/3166] [D loss: 0.696678] [G loss: 0.695920]\n",
      "[Epoch 0/200] [Batch 1049/3166] [D loss: 0.705733] [G loss: 0.688391]\n",
      "[Epoch 0/200] [Batch 1050/3166] [D loss: 0.701155] [G loss: 0.706557]\n",
      "[Epoch 0/200] [Batch 1051/3166] [D loss: 0.693876] [G loss: 0.699345]\n",
      "[Epoch 0/200] [Batch 1052/3166] [D loss: 0.696859] [G loss: 0.698358]\n",
      "[Epoch 0/200] [Batch 1053/3166] [D loss: 0.696599] [G loss: 0.707364]\n",
      "[Epoch 0/200] [Batch 1054/3166] [D loss: 0.693729] [G loss: 0.711107]\n",
      "[Epoch 0/200] [Batch 1055/3166] [D loss: 0.695165] [G loss: 0.707302]\n",
      "[Epoch 0/200] [Batch 1056/3166] [D loss: 0.698707] [G loss: 0.706157]\n",
      "[Epoch 0/200] [Batch 1057/3166] [D loss: 0.698290] [G loss: 0.707530]\n",
      "[Epoch 0/200] [Batch 1058/3166] [D loss: 0.696945] [G loss: 0.719489]\n",
      "[Epoch 0/200] [Batch 1059/3166] [D loss: 0.695058] [G loss: 0.711208]\n",
      "[Epoch 0/200] [Batch 1060/3166] [D loss: 0.696861] [G loss: 0.708188]\n",
      "[Epoch 0/200] [Batch 1061/3166] [D loss: 0.692269] [G loss: 0.715856]\n",
      "[Epoch 0/200] [Batch 1062/3166] [D loss: 0.695575] [G loss: 0.713801]\n",
      "[Epoch 0/200] [Batch 1063/3166] [D loss: 0.700266] [G loss: 0.715062]\n",
      "[Epoch 0/200] [Batch 1064/3166] [D loss: 0.697964] [G loss: 0.714277]\n",
      "[Epoch 0/200] [Batch 1065/3166] [D loss: 0.692380] [G loss: 0.704139]\n",
      "[Epoch 0/200] [Batch 1066/3166] [D loss: 0.691231] [G loss: 0.711991]\n",
      "[Epoch 0/200] [Batch 1067/3166] [D loss: 0.689551] [G loss: 0.713868]\n",
      "[Epoch 0/200] [Batch 1068/3166] [D loss: 0.689388] [G loss: 0.710737]\n",
      "[Epoch 0/200] [Batch 1069/3166] [D loss: 0.689995] [G loss: 0.719357]\n",
      "[Epoch 0/200] [Batch 1070/3166] [D loss: 0.692186] [G loss: 0.713618]\n",
      "[Epoch 0/200] [Batch 1071/3166] [D loss: 0.691333] [G loss: 0.717227]\n",
      "[Epoch 0/200] [Batch 1072/3166] [D loss: 0.692106] [G loss: 0.708315]\n",
      "[Epoch 0/200] [Batch 1073/3166] [D loss: 0.686693] [G loss: 0.715349]\n",
      "[Epoch 0/200] [Batch 1074/3166] [D loss: 0.688697] [G loss: 0.710510]\n",
      "[Epoch 0/200] [Batch 1075/3166] [D loss: 0.692609] [G loss: 0.702107]\n",
      "[Epoch 0/200] [Batch 1076/3166] [D loss: 0.694780] [G loss: 0.700388]\n",
      "[Epoch 0/200] [Batch 1077/3166] [D loss: 0.693079] [G loss: 0.697543]\n",
      "[Epoch 0/200] [Batch 1078/3166] [D loss: 0.690827] [G loss: 0.711060]\n",
      "[Epoch 0/200] [Batch 1079/3166] [D loss: 0.697536] [G loss: 0.705397]\n",
      "[Epoch 0/200] [Batch 1080/3166] [D loss: 0.692083] [G loss: 0.699941]\n",
      "[Epoch 0/200] [Batch 1081/3166] [D loss: 0.702082] [G loss: 0.701524]\n",
      "[Epoch 0/200] [Batch 1082/3166] [D loss: 0.690206] [G loss: 0.701235]\n",
      "[Epoch 0/200] [Batch 1083/3166] [D loss: 0.699062] [G loss: 0.703580]\n",
      "[Epoch 0/200] [Batch 1084/3166] [D loss: 0.697564] [G loss: 0.695411]\n",
      "[Epoch 0/200] [Batch 1085/3166] [D loss: 0.690440] [G loss: 0.706285]\n",
      "[Epoch 0/200] [Batch 1086/3166] [D loss: 0.694109] [G loss: 0.694652]\n",
      "[Epoch 0/200] [Batch 1087/3166] [D loss: 0.696277] [G loss: 0.697670]\n",
      "[Epoch 0/200] [Batch 1088/3166] [D loss: 0.694128] [G loss: 0.701865]\n",
      "[Epoch 0/200] [Batch 1089/3166] [D loss: 0.693306] [G loss: 0.704235]\n",
      "[Epoch 0/200] [Batch 1090/3166] [D loss: 0.697007] [G loss: 0.692498]\n",
      "[Epoch 0/200] [Batch 1091/3166] [D loss: 0.695672] [G loss: 0.694668]\n",
      "[Epoch 0/200] [Batch 1092/3166] [D loss: 0.695489] [G loss: 0.691448]\n",
      "[Epoch 0/200] [Batch 1093/3166] [D loss: 0.697634] [G loss: 0.690157]\n",
      "[Epoch 0/200] [Batch 1094/3166] [D loss: 0.695248] [G loss: 0.687660]\n",
      "[Epoch 0/200] [Batch 1095/3166] [D loss: 0.695166] [G loss: 0.699621]\n",
      "[Epoch 0/200] [Batch 1096/3166] [D loss: 0.692358] [G loss: 0.694482]\n",
      "[Epoch 0/200] [Batch 1097/3166] [D loss: 0.695258] [G loss: 0.691521]\n",
      "[Epoch 0/200] [Batch 1098/3166] [D loss: 0.687615] [G loss: 0.696373]\n",
      "[Epoch 0/200] [Batch 1099/3166] [D loss: 0.694709] [G loss: 0.690448]\n",
      "[Epoch 0/200] [Batch 1100/3166] [D loss: 0.697140] [G loss: 0.688826]\n",
      "[Epoch 0/200] [Batch 1101/3166] [D loss: 0.689073] [G loss: 0.691646]\n",
      "[Epoch 0/200] [Batch 1102/3166] [D loss: 0.688849] [G loss: 0.695326]\n",
      "[Epoch 0/200] [Batch 1103/3166] [D loss: 0.694826] [G loss: 0.690647]\n",
      "[Epoch 0/200] [Batch 1104/3166] [D loss: 0.694337] [G loss: 0.685036]\n",
      "[Epoch 0/200] [Batch 1105/3166] [D loss: 0.691598] [G loss: 0.690907]\n",
      "[Epoch 0/200] [Batch 1106/3166] [D loss: 0.685781] [G loss: 0.699968]\n",
      "[Epoch 0/200] [Batch 1107/3166] [D loss: 0.688692] [G loss: 0.689837]\n",
      "[Epoch 0/200] [Batch 1108/3166] [D loss: 0.692639] [G loss: 0.688989]\n",
      "[Epoch 0/200] [Batch 1109/3166] [D loss: 0.694476] [G loss: 0.685878]\n",
      "[Epoch 0/200] [Batch 1110/3166] [D loss: 0.691145] [G loss: 0.688334]\n",
      "[Epoch 0/200] [Batch 1111/3166] [D loss: 0.691897] [G loss: 0.690340]\n",
      "[Epoch 0/200] [Batch 1112/3166] [D loss: 0.693456] [G loss: 0.681902]\n",
      "[Epoch 0/200] [Batch 1113/3166] [D loss: 0.696089] [G loss: 0.688368]\n",
      "[Epoch 0/200] [Batch 1114/3166] [D loss: 0.692645] [G loss: 0.684142]\n",
      "[Epoch 0/200] [Batch 1115/3166] [D loss: 0.696510] [G loss: 0.673612]\n",
      "[Epoch 0/200] [Batch 1116/3166] [D loss: 0.688915] [G loss: 0.687919]\n",
      "[Epoch 0/200] [Batch 1117/3166] [D loss: 0.699899] [G loss: 0.678110]\n",
      "[Epoch 0/200] [Batch 1118/3166] [D loss: 0.691085] [G loss: 0.694317]\n",
      "[Epoch 0/200] [Batch 1119/3166] [D loss: 0.692135] [G loss: 0.692138]\n",
      "[Epoch 0/200] [Batch 1120/3166] [D loss: 0.686716] [G loss: 0.690339]\n",
      "[Epoch 0/200] [Batch 1121/3166] [D loss: 0.692457] [G loss: 0.697999]\n",
      "[Epoch 0/200] [Batch 1122/3166] [D loss: 0.686021] [G loss: 0.703483]\n",
      "[Epoch 0/200] [Batch 1123/3166] [D loss: 0.695683] [G loss: 0.690029]\n",
      "[Epoch 0/200] [Batch 1124/3166] [D loss: 0.684142] [G loss: 0.701390]\n",
      "[Epoch 0/200] [Batch 1125/3166] [D loss: 0.693909] [G loss: 0.694586]\n",
      "[Epoch 0/200] [Batch 1126/3166] [D loss: 0.690210] [G loss: 0.704204]\n",
      "[Epoch 0/200] [Batch 1127/3166] [D loss: 0.692702] [G loss: 0.697231]\n",
      "[Epoch 0/200] [Batch 1128/3166] [D loss: 0.689716] [G loss: 0.695646]\n",
      "[Epoch 0/200] [Batch 1129/3166] [D loss: 0.691590] [G loss: 0.699254]\n",
      "[Epoch 0/200] [Batch 1130/3166] [D loss: 0.696784] [G loss: 0.692343]\n",
      "[Epoch 0/200] [Batch 1131/3166] [D loss: 0.691667] [G loss: 0.704727]\n",
      "[Epoch 0/200] [Batch 1132/3166] [D loss: 0.691240] [G loss: 0.701365]\n",
      "[Epoch 0/200] [Batch 1133/3166] [D loss: 0.691925] [G loss: 0.700620]\n",
      "[Epoch 0/200] [Batch 1134/3166] [D loss: 0.693705] [G loss: 0.700683]\n",
      "[Epoch 0/200] [Batch 1135/3166] [D loss: 0.690852] [G loss: 0.703071]\n",
      "[Epoch 0/200] [Batch 1136/3166] [D loss: 0.691357] [G loss: 0.701619]\n",
      "[Epoch 0/200] [Batch 1137/3166] [D loss: 0.692020] [G loss: 0.701744]\n",
      "[Epoch 0/200] [Batch 1138/3166] [D loss: 0.698685] [G loss: 0.696235]\n",
      "[Epoch 0/200] [Batch 1139/3166] [D loss: 0.693824] [G loss: 0.697918]\n",
      "[Epoch 0/200] [Batch 1140/3166] [D loss: 0.692304] [G loss: 0.697746]\n",
      "[Epoch 0/200] [Batch 1141/3166] [D loss: 0.696128] [G loss: 0.691346]\n",
      "[Epoch 0/200] [Batch 1142/3166] [D loss: 0.694954] [G loss: 0.694241]\n",
      "[Epoch 0/200] [Batch 1143/3166] [D loss: 0.695619] [G loss: 0.692837]\n",
      "[Epoch 0/200] [Batch 1144/3166] [D loss: 0.693982] [G loss: 0.690138]\n",
      "[Epoch 0/200] [Batch 1145/3166] [D loss: 0.697127] [G loss: 0.691065]\n",
      "[Epoch 0/200] [Batch 1146/3166] [D loss: 0.698004] [G loss: 0.694623]\n",
      "[Epoch 0/200] [Batch 1147/3166] [D loss: 0.700426] [G loss: 0.695448]\n",
      "[Epoch 0/200] [Batch 1148/3166] [D loss: 0.701203] [G loss: 0.691333]\n",
      "[Epoch 0/200] [Batch 1149/3166] [D loss: 0.698193] [G loss: 0.690900]\n",
      "[Epoch 0/200] [Batch 1150/3166] [D loss: 0.694547] [G loss: 0.693194]\n",
      "[Epoch 0/200] [Batch 1151/3166] [D loss: 0.694886] [G loss: 0.702160]\n",
      "[Epoch 0/200] [Batch 1152/3166] [D loss: 0.693365] [G loss: 0.695003]\n",
      "[Epoch 0/200] [Batch 1153/3166] [D loss: 0.694414] [G loss: 0.691700]\n",
      "[Epoch 0/200] [Batch 1154/3166] [D loss: 0.695575] [G loss: 0.694777]\n",
      "[Epoch 0/200] [Batch 1155/3166] [D loss: 0.693821] [G loss: 0.700099]\n",
      "[Epoch 0/200] [Batch 1156/3166] [D loss: 0.695425] [G loss: 0.691231]\n",
      "[Epoch 0/200] [Batch 1157/3166] [D loss: 0.693995] [G loss: 0.694081]\n",
      "[Epoch 0/200] [Batch 1158/3166] [D loss: 0.693591] [G loss: 0.697442]\n",
      "[Epoch 0/200] [Batch 1159/3166] [D loss: 0.694346] [G loss: 0.702109]\n",
      "[Epoch 0/200] [Batch 1160/3166] [D loss: 0.693472] [G loss: 0.696376]\n",
      "[Epoch 0/200] [Batch 1161/3166] [D loss: 0.691196] [G loss: 0.692511]\n",
      "[Epoch 0/200] [Batch 1162/3166] [D loss: 0.693361] [G loss: 0.694329]\n",
      "[Epoch 0/200] [Batch 1163/3166] [D loss: 0.693223] [G loss: 0.696393]\n",
      "[Epoch 0/200] [Batch 1164/3166] [D loss: 0.697883] [G loss: 0.692039]\n",
      "[Epoch 0/200] [Batch 1165/3166] [D loss: 0.695477] [G loss: 0.698346]\n",
      "[Epoch 0/200] [Batch 1166/3166] [D loss: 0.698000] [G loss: 0.683659]\n",
      "[Epoch 0/200] [Batch 1167/3166] [D loss: 0.699613] [G loss: 0.689003]\n",
      "[Epoch 0/200] [Batch 1168/3166] [D loss: 0.694557] [G loss: 0.698896]\n",
      "[Epoch 0/200] [Batch 1169/3166] [D loss: 0.693795] [G loss: 0.689211]\n",
      "[Epoch 0/200] [Batch 1170/3166] [D loss: 0.696378] [G loss: 0.691719]\n",
      "[Epoch 0/200] [Batch 1171/3166] [D loss: 0.695650] [G loss: 0.692377]\n",
      "[Epoch 0/200] [Batch 1172/3166] [D loss: 0.696849] [G loss: 0.695486]\n",
      "[Epoch 0/200] [Batch 1173/3166] [D loss: 0.696262] [G loss: 0.694019]\n",
      "[Epoch 0/200] [Batch 1174/3166] [D loss: 0.702852] [G loss: 0.685820]\n",
      "[Epoch 0/200] [Batch 1175/3166] [D loss: 0.698100] [G loss: 0.693838]\n",
      "[Epoch 0/200] [Batch 1176/3166] [D loss: 0.696369] [G loss: 0.695527]\n",
      "[Epoch 0/200] [Batch 1177/3166] [D loss: 0.694032] [G loss: 0.700000]\n",
      "[Epoch 0/200] [Batch 1178/3166] [D loss: 0.696790] [G loss: 0.697612]\n",
      "[Epoch 0/200] [Batch 1179/3166] [D loss: 0.694108] [G loss: 0.698669]\n",
      "[Epoch 0/200] [Batch 1180/3166] [D loss: 0.695917] [G loss: 0.699354]\n",
      "[Epoch 0/200] [Batch 1181/3166] [D loss: 0.692556] [G loss: 0.700806]\n",
      "[Epoch 0/200] [Batch 1182/3166] [D loss: 0.693213] [G loss: 0.702272]\n",
      "[Epoch 0/200] [Batch 1183/3166] [D loss: 0.693697] [G loss: 0.706365]\n",
      "[Epoch 0/200] [Batch 1184/3166] [D loss: 0.690905] [G loss: 0.702385]\n",
      "[Epoch 0/200] [Batch 1185/3166] [D loss: 0.686556] [G loss: 0.709091]\n",
      "[Epoch 0/200] [Batch 1186/3166] [D loss: 0.695411] [G loss: 0.701748]\n",
      "[Epoch 0/200] [Batch 1187/3166] [D loss: 0.690031] [G loss: 0.705910]\n",
      "[Epoch 0/200] [Batch 1188/3166] [D loss: 0.689774] [G loss: 0.702697]\n",
      "[Epoch 0/200] [Batch 1189/3166] [D loss: 0.691644] [G loss: 0.704975]\n",
      "[Epoch 0/200] [Batch 1190/3166] [D loss: 0.691343] [G loss: 0.702692]\n",
      "[Epoch 0/200] [Batch 1191/3166] [D loss: 0.691865] [G loss: 0.698206]\n",
      "[Epoch 0/200] [Batch 1192/3166] [D loss: 0.691814] [G loss: 0.698350]\n",
      "[Epoch 0/200] [Batch 1193/3166] [D loss: 0.692975] [G loss: 0.698060]\n",
      "[Epoch 0/200] [Batch 1194/3166] [D loss: 0.688905] [G loss: 0.702532]\n",
      "[Epoch 0/200] [Batch 1195/3166] [D loss: 0.691762] [G loss: 0.702279]\n",
      "[Epoch 0/200] [Batch 1196/3166] [D loss: 0.693117] [G loss: 0.696977]\n",
      "[Epoch 0/200] [Batch 1197/3166] [D loss: 0.686932] [G loss: 0.696975]\n",
      "[Epoch 0/200] [Batch 1198/3166] [D loss: 0.690243] [G loss: 0.695239]\n",
      "[Epoch 0/200] [Batch 1199/3166] [D loss: 0.688044] [G loss: 0.693078]\n",
      "[Epoch 0/200] [Batch 1200/3166] [D loss: 0.690371] [G loss: 0.697982]\n",
      "[Epoch 0/200] [Batch 1201/3166] [D loss: 0.685458] [G loss: 0.694251]\n",
      "[Epoch 0/200] [Batch 1202/3166] [D loss: 0.693367] [G loss: 0.690516]\n",
      "[Epoch 0/200] [Batch 1203/3166] [D loss: 0.692640] [G loss: 0.690124]\n",
      "[Epoch 0/200] [Batch 1204/3166] [D loss: 0.690931] [G loss: 0.689322]\n",
      "[Epoch 0/200] [Batch 1205/3166] [D loss: 0.689221] [G loss: 0.688124]\n",
      "[Epoch 0/200] [Batch 1206/3166] [D loss: 0.690542] [G loss: 0.689773]\n",
      "[Epoch 0/200] [Batch 1207/3166] [D loss: 0.694520] [G loss: 0.678535]\n",
      "[Epoch 0/200] [Batch 1208/3166] [D loss: 0.695790] [G loss: 0.681193]\n",
      "[Epoch 0/200] [Batch 1209/3166] [D loss: 0.695087] [G loss: 0.684944]\n",
      "[Epoch 0/200] [Batch 1210/3166] [D loss: 0.693522] [G loss: 0.686670]\n",
      "[Epoch 0/200] [Batch 1211/3166] [D loss: 0.694092] [G loss: 0.687083]\n",
      "[Epoch 0/200] [Batch 1212/3166] [D loss: 0.697870] [G loss: 0.685114]\n",
      "[Epoch 0/200] [Batch 1213/3166] [D loss: 0.697152] [G loss: 0.682599]\n",
      "[Epoch 0/200] [Batch 1214/3166] [D loss: 0.694470] [G loss: 0.683051]\n",
      "[Epoch 0/200] [Batch 1215/3166] [D loss: 0.692599] [G loss: 0.682227]\n",
      "[Epoch 0/200] [Batch 1216/3166] [D loss: 0.697927] [G loss: 0.688364]\n",
      "[Epoch 0/200] [Batch 1217/3166] [D loss: 0.697651] [G loss: 0.688660]\n",
      "[Epoch 0/200] [Batch 1218/3166] [D loss: 0.699515] [G loss: 0.694152]\n",
      "[Epoch 0/200] [Batch 1219/3166] [D loss: 0.692668] [G loss: 0.692205]\n",
      "[Epoch 0/200] [Batch 1220/3166] [D loss: 0.697265] [G loss: 0.688697]\n",
      "[Epoch 0/200] [Batch 1221/3166] [D loss: 0.693994] [G loss: 0.697409]\n",
      "[Epoch 0/200] [Batch 1222/3166] [D loss: 0.692942] [G loss: 0.695928]\n",
      "[Epoch 0/200] [Batch 1223/3166] [D loss: 0.694092] [G loss: 0.700725]\n",
      "[Epoch 0/200] [Batch 1224/3166] [D loss: 0.696659] [G loss: 0.698224]\n",
      "[Epoch 0/200] [Batch 1225/3166] [D loss: 0.691059] [G loss: 0.704673]\n",
      "[Epoch 0/200] [Batch 1226/3166] [D loss: 0.690501] [G loss: 0.702858]\n",
      "[Epoch 0/200] [Batch 1227/3166] [D loss: 0.690479] [G loss: 0.702808]\n",
      "[Epoch 0/200] [Batch 1228/3166] [D loss: 0.692017] [G loss: 0.700597]\n",
      "[Epoch 0/200] [Batch 1229/3166] [D loss: 0.690052] [G loss: 0.709065]\n",
      "[Epoch 0/200] [Batch 1230/3166] [D loss: 0.692660] [G loss: 0.707483]\n",
      "[Epoch 0/200] [Batch 1231/3166] [D loss: 0.688303] [G loss: 0.707849]\n",
      "[Epoch 0/200] [Batch 1232/3166] [D loss: 0.690372] [G loss: 0.708474]\n",
      "[Epoch 0/200] [Batch 1233/3166] [D loss: 0.694618] [G loss: 0.696973]\n",
      "[Epoch 0/200] [Batch 1234/3166] [D loss: 0.690476] [G loss: 0.705896]\n",
      "[Epoch 0/200] [Batch 1235/3166] [D loss: 0.687801] [G loss: 0.705840]\n",
      "[Epoch 0/200] [Batch 1236/3166] [D loss: 0.690617] [G loss: 0.701865]\n",
      "[Epoch 0/200] [Batch 1237/3166] [D loss: 0.691266] [G loss: 0.699244]\n",
      "[Epoch 0/200] [Batch 1238/3166] [D loss: 0.690787] [G loss: 0.705113]\n",
      "[Epoch 0/200] [Batch 1239/3166] [D loss: 0.690811] [G loss: 0.697908]\n",
      "[Epoch 0/200] [Batch 1240/3166] [D loss: 0.688498] [G loss: 0.698264]\n",
      "[Epoch 0/200] [Batch 1241/3166] [D loss: 0.693494] [G loss: 0.701657]\n",
      "[Epoch 0/200] [Batch 1242/3166] [D loss: 0.691662] [G loss: 0.696127]\n",
      "[Epoch 0/200] [Batch 1243/3166] [D loss: 0.695108] [G loss: 0.697416]\n",
      "[Epoch 0/200] [Batch 1244/3166] [D loss: 0.695725] [G loss: 0.694925]\n",
      "[Epoch 0/200] [Batch 1245/3166] [D loss: 0.691408] [G loss: 0.694866]\n",
      "[Epoch 0/200] [Batch 1246/3166] [D loss: 0.702465] [G loss: 0.684427]\n",
      "[Epoch 0/200] [Batch 1247/3166] [D loss: 0.694363] [G loss: 0.699371]\n",
      "[Epoch 0/200] [Batch 1248/3166] [D loss: 0.696095] [G loss: 0.689902]\n",
      "[Epoch 0/200] [Batch 1249/3166] [D loss: 0.693984] [G loss: 0.689861]\n",
      "[Epoch 0/200] [Batch 1250/3166] [D loss: 0.696002] [G loss: 0.691683]\n",
      "[Epoch 0/200] [Batch 1251/3166] [D loss: 0.695298] [G loss: 0.690398]\n",
      "[Epoch 0/200] [Batch 1252/3166] [D loss: 0.691769] [G loss: 0.692662]\n",
      "[Epoch 0/200] [Batch 1253/3166] [D loss: 0.695009] [G loss: 0.695587]\n",
      "[Epoch 0/200] [Batch 1254/3166] [D loss: 0.696857] [G loss: 0.688533]\n",
      "[Epoch 0/200] [Batch 1255/3166] [D loss: 0.695052] [G loss: 0.688076]\n",
      "[Epoch 0/200] [Batch 1256/3166] [D loss: 0.691702] [G loss: 0.690458]\n",
      "[Epoch 0/200] [Batch 1257/3166] [D loss: 0.694491] [G loss: 0.690078]\n",
      "[Epoch 0/200] [Batch 1258/3166] [D loss: 0.689659] [G loss: 0.691675]\n",
      "[Epoch 0/200] [Batch 1259/3166] [D loss: 0.693289] [G loss: 0.686898]\n",
      "[Epoch 0/200] [Batch 1260/3166] [D loss: 0.694079] [G loss: 0.695274]\n",
      "[Epoch 0/200] [Batch 1261/3166] [D loss: 0.693096] [G loss: 0.692209]\n",
      "[Epoch 0/200] [Batch 1262/3166] [D loss: 0.693391] [G loss: 0.692775]\n",
      "[Epoch 0/200] [Batch 1263/3166] [D loss: 0.689820] [G loss: 0.697401]\n",
      "[Epoch 0/200] [Batch 1264/3166] [D loss: 0.688823] [G loss: 0.696936]\n",
      "[Epoch 0/200] [Batch 1265/3166] [D loss: 0.688528] [G loss: 0.695889]\n",
      "[Epoch 0/200] [Batch 1266/3166] [D loss: 0.691320] [G loss: 0.694660]\n",
      "[Epoch 0/200] [Batch 1267/3166] [D loss: 0.691080] [G loss: 0.696143]\n",
      "[Epoch 0/200] [Batch 1268/3166] [D loss: 0.693732] [G loss: 0.693292]\n",
      "[Epoch 0/200] [Batch 1269/3166] [D loss: 0.688628] [G loss: 0.694153]\n",
      "[Epoch 0/200] [Batch 1270/3166] [D loss: 0.689189] [G loss: 0.696287]\n",
      "[Epoch 0/200] [Batch 1271/3166] [D loss: 0.690324] [G loss: 0.700487]\n",
      "[Epoch 0/200] [Batch 1272/3166] [D loss: 0.690139] [G loss: 0.696843]\n",
      "[Epoch 0/200] [Batch 1273/3166] [D loss: 0.688553] [G loss: 0.699337]\n",
      "[Epoch 0/200] [Batch 1274/3166] [D loss: 0.687482] [G loss: 0.695999]\n",
      "[Epoch 0/200] [Batch 1275/3166] [D loss: 0.689859] [G loss: 0.698987]\n",
      "[Epoch 0/200] [Batch 1276/3166] [D loss: 0.689965] [G loss: 0.699880]\n",
      "[Epoch 0/200] [Batch 1277/3166] [D loss: 0.688846] [G loss: 0.701807]\n",
      "[Epoch 0/200] [Batch 1278/3166] [D loss: 0.686352] [G loss: 0.698337]\n",
      "[Epoch 0/200] [Batch 1279/3166] [D loss: 0.689459] [G loss: 0.699280]\n",
      "[Epoch 0/200] [Batch 1280/3166] [D loss: 0.687951] [G loss: 0.699478]\n",
      "[Epoch 0/200] [Batch 1281/3166] [D loss: 0.684495] [G loss: 0.698197]\n",
      "[Epoch 0/200] [Batch 1282/3166] [D loss: 0.693277] [G loss: 0.698628]\n",
      "[Epoch 0/200] [Batch 1283/3166] [D loss: 0.685514] [G loss: 0.697945]\n",
      "[Epoch 0/200] [Batch 1284/3166] [D loss: 0.692054] [G loss: 0.694621]\n",
      "[Epoch 0/200] [Batch 1285/3166] [D loss: 0.691343] [G loss: 0.687067]\n",
      "[Epoch 0/200] [Batch 1286/3166] [D loss: 0.694284] [G loss: 0.692705]\n",
      "[Epoch 0/200] [Batch 1287/3166] [D loss: 0.693290] [G loss: 0.683915]\n",
      "[Epoch 0/200] [Batch 1288/3166] [D loss: 0.692505] [G loss: 0.692739]\n",
      "[Epoch 0/200] [Batch 1289/3166] [D loss: 0.697041] [G loss: 0.685614]\n",
      "[Epoch 0/200] [Batch 1290/3166] [D loss: 0.700042] [G loss: 0.678868]\n",
      "[Epoch 0/200] [Batch 1291/3166] [D loss: 0.696282] [G loss: 0.688737]\n",
      "[Epoch 0/200] [Batch 1292/3166] [D loss: 0.694554] [G loss: 0.690172]\n",
      "[Epoch 0/200] [Batch 1293/3166] [D loss: 0.706970] [G loss: 0.680936]\n",
      "[Epoch 0/200] [Batch 1294/3166] [D loss: 0.693367] [G loss: 0.695618]\n",
      "[Epoch 0/200] [Batch 1295/3166] [D loss: 0.700324] [G loss: 0.685148]\n",
      "[Epoch 0/200] [Batch 1296/3166] [D loss: 0.695773] [G loss: 0.681536]\n",
      "[Epoch 0/200] [Batch 1297/3166] [D loss: 0.696539] [G loss: 0.684265]\n",
      "[Epoch 0/200] [Batch 1298/3166] [D loss: 0.695322] [G loss: 0.690229]\n",
      "[Epoch 0/200] [Batch 1299/3166] [D loss: 0.704659] [G loss: 0.674034]\n",
      "[Epoch 0/200] [Batch 1300/3166] [D loss: 0.697786] [G loss: 0.686579]\n",
      "[Epoch 0/200] [Batch 1301/3166] [D loss: 0.696777] [G loss: 0.696907]\n",
      "[Epoch 0/200] [Batch 1302/3166] [D loss: 0.696282] [G loss: 0.693118]\n",
      "[Epoch 0/200] [Batch 1303/3166] [D loss: 0.697879] [G loss: 0.692440]\n",
      "[Epoch 0/200] [Batch 1304/3166] [D loss: 0.696221] [G loss: 0.692509]\n",
      "[Epoch 0/200] [Batch 1305/3166] [D loss: 0.698868] [G loss: 0.689904]\n",
      "[Epoch 0/200] [Batch 1306/3166] [D loss: 0.696862] [G loss: 0.695776]\n",
      "[Epoch 0/200] [Batch 1307/3166] [D loss: 0.695477] [G loss: 0.693433]\n",
      "[Epoch 0/200] [Batch 1308/3166] [D loss: 0.698124] [G loss: 0.696428]\n",
      "[Epoch 0/200] [Batch 1309/3166] [D loss: 0.692765] [G loss: 0.703445]\n",
      "[Epoch 0/200] [Batch 1310/3166] [D loss: 0.692461] [G loss: 0.701242]\n",
      "[Epoch 0/200] [Batch 1311/3166] [D loss: 0.695143] [G loss: 0.700572]\n",
      "[Epoch 0/200] [Batch 1312/3166] [D loss: 0.692969] [G loss: 0.698819]\n",
      "[Epoch 0/200] [Batch 1313/3166] [D loss: 0.691637] [G loss: 0.703609]\n",
      "[Epoch 0/200] [Batch 1314/3166] [D loss: 0.690724] [G loss: 0.704848]\n",
      "[Epoch 0/200] [Batch 1315/3166] [D loss: 0.689954] [G loss: 0.706654]\n",
      "[Epoch 0/200] [Batch 1316/3166] [D loss: 0.692247] [G loss: 0.704348]\n",
      "[Epoch 0/200] [Batch 1317/3166] [D loss: 0.693374] [G loss: 0.700581]\n",
      "[Epoch 0/200] [Batch 1318/3166] [D loss: 0.691348] [G loss: 0.708659]\n",
      "[Epoch 0/200] [Batch 1319/3166] [D loss: 0.689494] [G loss: 0.708221]\n",
      "[Epoch 0/200] [Batch 1320/3166] [D loss: 0.688700] [G loss: 0.703638]\n",
      "[Epoch 0/200] [Batch 1321/3166] [D loss: 0.688604] [G loss: 0.710301]\n",
      "[Epoch 0/200] [Batch 1322/3166] [D loss: 0.690830] [G loss: 0.706309]\n",
      "[Epoch 0/200] [Batch 1323/3166] [D loss: 0.687159] [G loss: 0.704011]\n",
      "[Epoch 0/200] [Batch 1324/3166] [D loss: 0.691430] [G loss: 0.701889]\n",
      "[Epoch 0/200] [Batch 1325/3166] [D loss: 0.696752] [G loss: 0.697677]\n",
      "[Epoch 0/200] [Batch 1326/3166] [D loss: 0.693309] [G loss: 0.691535]\n",
      "[Epoch 0/200] [Batch 1327/3166] [D loss: 0.693302] [G loss: 0.699213]\n",
      "[Epoch 0/200] [Batch 1328/3166] [D loss: 0.695852] [G loss: 0.693536]\n",
      "[Epoch 0/200] [Batch 1329/3166] [D loss: 0.688994] [G loss: 0.690191]\n",
      "[Epoch 0/200] [Batch 1330/3166] [D loss: 0.694855] [G loss: 0.693328]\n",
      "[Epoch 0/200] [Batch 1331/3166] [D loss: 0.693750] [G loss: 0.697586]\n",
      "[Epoch 0/200] [Batch 1332/3166] [D loss: 0.691848] [G loss: 0.692924]\n",
      "[Epoch 0/200] [Batch 1333/3166] [D loss: 0.691984] [G loss: 0.691702]\n",
      "[Epoch 0/200] [Batch 1334/3166] [D loss: 0.692095] [G loss: 0.690148]\n",
      "[Epoch 0/200] [Batch 1335/3166] [D loss: 0.689888] [G loss: 0.693369]\n",
      "[Epoch 0/200] [Batch 1336/3166] [D loss: 0.692225] [G loss: 0.691104]\n",
      "[Epoch 0/200] [Batch 1337/3166] [D loss: 0.694715] [G loss: 0.685120]\n",
      "[Epoch 0/200] [Batch 1338/3166] [D loss: 0.692122] [G loss: 0.690816]\n",
      "[Epoch 0/200] [Batch 1339/3166] [D loss: 0.692208] [G loss: 0.685977]\n",
      "[Epoch 0/200] [Batch 1340/3166] [D loss: 0.691730] [G loss: 0.690510]\n",
      "[Epoch 0/200] [Batch 1341/3166] [D loss: 0.696504] [G loss: 0.688423]\n",
      "[Epoch 0/200] [Batch 1342/3166] [D loss: 0.691611] [G loss: 0.694992]\n",
      "[Epoch 0/200] [Batch 1343/3166] [D loss: 0.695349] [G loss: 0.686550]\n",
      "[Epoch 0/200] [Batch 1344/3166] [D loss: 0.691531] [G loss: 0.685740]\n",
      "[Epoch 0/200] [Batch 1345/3166] [D loss: 0.693158] [G loss: 0.687971]\n",
      "[Epoch 0/200] [Batch 1346/3166] [D loss: 0.690039] [G loss: 0.689908]\n",
      "[Epoch 0/200] [Batch 1347/3166] [D loss: 0.692920] [G loss: 0.684516]\n",
      "[Epoch 0/200] [Batch 1348/3166] [D loss: 0.689815] [G loss: 0.686509]\n",
      "[Epoch 0/200] [Batch 1349/3166] [D loss: 0.690503] [G loss: 0.684674]\n",
      "[Epoch 0/200] [Batch 1350/3166] [D loss: 0.692706] [G loss: 0.684720]\n",
      "[Epoch 0/200] [Batch 1351/3166] [D loss: 0.693443] [G loss: 0.690534]\n",
      "[Epoch 0/200] [Batch 1352/3166] [D loss: 0.690963] [G loss: 0.690767]\n",
      "[Epoch 0/200] [Batch 1353/3166] [D loss: 0.689404] [G loss: 0.692194]\n",
      "[Epoch 0/200] [Batch 1354/3166] [D loss: 0.688667] [G loss: 0.686490]\n",
      "[Epoch 0/200] [Batch 1355/3166] [D loss: 0.689259] [G loss: 0.690378]\n",
      "[Epoch 0/200] [Batch 1356/3166] [D loss: 0.688099] [G loss: 0.690404]\n",
      "[Epoch 0/200] [Batch 1357/3166] [D loss: 0.685284] [G loss: 0.694535]\n",
      "[Epoch 0/200] [Batch 1358/3166] [D loss: 0.686813] [G loss: 0.688274]\n",
      "[Epoch 0/200] [Batch 1359/3166] [D loss: 0.681574] [G loss: 0.699139]\n",
      "[Epoch 0/200] [Batch 1360/3166] [D loss: 0.685897] [G loss: 0.695985]\n",
      "[Epoch 0/200] [Batch 1361/3166] [D loss: 0.686727] [G loss: 0.694472]\n",
      "[Epoch 0/200] [Batch 1362/3166] [D loss: 0.689333] [G loss: 0.683856]\n",
      "[Epoch 0/200] [Batch 1363/3166] [D loss: 0.686117] [G loss: 0.694998]\n",
      "[Epoch 0/200] [Batch 1364/3166] [D loss: 0.686513] [G loss: 0.693972]\n",
      "[Epoch 0/200] [Batch 1365/3166] [D loss: 0.685328] [G loss: 0.684517]\n",
      "[Epoch 0/200] [Batch 1366/3166] [D loss: 0.695764] [G loss: 0.681074]\n",
      "[Epoch 0/200] [Batch 1367/3166] [D loss: 0.689777] [G loss: 0.689406]\n",
      "[Epoch 0/200] [Batch 1368/3166] [D loss: 0.687356] [G loss: 0.692454]\n",
      "[Epoch 0/200] [Batch 1369/3166] [D loss: 0.687280] [G loss: 0.691294]\n",
      "[Epoch 0/200] [Batch 1370/3166] [D loss: 0.690129] [G loss: 0.692366]\n",
      "[Epoch 0/200] [Batch 1371/3166] [D loss: 0.686246] [G loss: 0.692503]\n",
      "[Epoch 0/200] [Batch 1372/3166] [D loss: 0.689555] [G loss: 0.690739]\n",
      "[Epoch 0/200] [Batch 1373/3166] [D loss: 0.690619] [G loss: 0.690439]\n",
      "[Epoch 0/200] [Batch 1374/3166] [D loss: 0.684353] [G loss: 0.688297]\n",
      "[Epoch 0/200] [Batch 1375/3166] [D loss: 0.688618] [G loss: 0.690845]\n",
      "[Epoch 0/200] [Batch 1376/3166] [D loss: 0.692080] [G loss: 0.678866]\n",
      "[Epoch 0/200] [Batch 1377/3166] [D loss: 0.700342] [G loss: 0.677908]\n",
      "[Epoch 0/200] [Batch 1378/3166] [D loss: 0.694254] [G loss: 0.687729]\n",
      "[Epoch 0/200] [Batch 1379/3166] [D loss: 0.691200] [G loss: 0.692371]\n",
      "[Epoch 0/200] [Batch 1380/3166] [D loss: 0.690729] [G loss: 0.691300]\n",
      "[Epoch 0/200] [Batch 1381/3166] [D loss: 0.686211] [G loss: 0.693292]\n",
      "[Epoch 0/200] [Batch 1382/3166] [D loss: 0.688130] [G loss: 0.689652]\n",
      "[Epoch 0/200] [Batch 1383/3166] [D loss: 0.690925] [G loss: 0.690533]\n",
      "[Epoch 0/200] [Batch 1384/3166] [D loss: 0.690906] [G loss: 0.686710]\n",
      "[Epoch 0/200] [Batch 1385/3166] [D loss: 0.684420] [G loss: 0.688660]\n",
      "[Epoch 0/200] [Batch 1386/3166] [D loss: 0.693081] [G loss: 0.697192]\n",
      "[Epoch 0/200] [Batch 1387/3166] [D loss: 0.690917] [G loss: 0.685821]\n",
      "[Epoch 0/200] [Batch 1388/3166] [D loss: 0.688237] [G loss: 0.700363]\n",
      "[Epoch 0/200] [Batch 1389/3166] [D loss: 0.685538] [G loss: 0.693892]\n",
      "[Epoch 0/200] [Batch 1390/3166] [D loss: 0.685471] [G loss: 0.691077]\n",
      "[Epoch 0/200] [Batch 1391/3166] [D loss: 0.694163] [G loss: 0.692672]\n",
      "[Epoch 0/200] [Batch 1392/3166] [D loss: 0.693612] [G loss: 0.693810]\n",
      "[Epoch 0/200] [Batch 1393/3166] [D loss: 0.693254] [G loss: 0.699881]\n",
      "[Epoch 0/200] [Batch 1394/3166] [D loss: 0.682544] [G loss: 0.704816]\n",
      "[Epoch 0/200] [Batch 1395/3166] [D loss: 0.693094] [G loss: 0.703948]\n",
      "[Epoch 0/200] [Batch 1396/3166] [D loss: 0.687914] [G loss: 0.711042]\n",
      "[Epoch 0/200] [Batch 1397/3166] [D loss: 0.684152] [G loss: 0.722718]\n",
      "[Epoch 0/200] [Batch 1398/3166] [D loss: 0.693089] [G loss: 0.709072]\n",
      "[Epoch 0/200] [Batch 1399/3166] [D loss: 0.688773] [G loss: 0.704842]\n",
      "[Epoch 0/200] [Batch 1400/3166] [D loss: 0.691840] [G loss: 0.711171]\n",
      "[Epoch 0/200] [Batch 1401/3166] [D loss: 0.691725] [G loss: 0.691954]\n",
      "[Epoch 0/200] [Batch 1402/3166] [D loss: 0.688684] [G loss: 0.701840]\n",
      "[Epoch 0/200] [Batch 1403/3166] [D loss: 0.688181] [G loss: 0.703050]\n",
      "[Epoch 0/200] [Batch 1404/3166] [D loss: 0.692887] [G loss: 0.698909]\n",
      "[Epoch 0/200] [Batch 1405/3166] [D loss: 0.689181] [G loss: 0.687569]\n",
      "[Epoch 0/200] [Batch 1406/3166] [D loss: 0.698172] [G loss: 0.687035]\n",
      "[Epoch 0/200] [Batch 1407/3166] [D loss: 0.694147] [G loss: 0.690680]\n",
      "[Epoch 0/200] [Batch 1408/3166] [D loss: 0.698584] [G loss: 0.678316]\n",
      "[Epoch 0/200] [Batch 1409/3166] [D loss: 0.691818] [G loss: 0.679869]\n",
      "[Epoch 0/200] [Batch 1410/3166] [D loss: 0.694366] [G loss: 0.691011]\n",
      "[Epoch 0/200] [Batch 1411/3166] [D loss: 0.700494] [G loss: 0.670782]\n",
      "[Epoch 0/200] [Batch 1412/3166] [D loss: 0.699098] [G loss: 0.677513]\n",
      "[Epoch 0/200] [Batch 1413/3166] [D loss: 0.702982] [G loss: 0.668679]\n",
      "[Epoch 0/200] [Batch 1414/3166] [D loss: 0.697558] [G loss: 0.683416]\n",
      "[Epoch 0/200] [Batch 1415/3166] [D loss: 0.701110] [G loss: 0.682828]\n",
      "[Epoch 0/200] [Batch 1416/3166] [D loss: 0.691127] [G loss: 0.692790]\n",
      "[Epoch 0/200] [Batch 1417/3166] [D loss: 0.696472] [G loss: 0.685006]\n",
      "[Epoch 0/200] [Batch 1418/3166] [D loss: 0.693483] [G loss: 0.692823]\n",
      "[Epoch 0/200] [Batch 1419/3166] [D loss: 0.698492] [G loss: 0.682494]\n",
      "[Epoch 0/200] [Batch 1420/3166] [D loss: 0.691972] [G loss: 0.692000]\n",
      "[Epoch 0/200] [Batch 1421/3166] [D loss: 0.698219] [G loss: 0.684324]\n",
      "[Epoch 0/200] [Batch 1422/3166] [D loss: 0.693910] [G loss: 0.699341]\n",
      "[Epoch 0/200] [Batch 1423/3166] [D loss: 0.694625] [G loss: 0.694882]\n",
      "[Epoch 0/200] [Batch 1424/3166] [D loss: 0.697009] [G loss: 0.693271]\n",
      "[Epoch 0/200] [Batch 1425/3166] [D loss: 0.692396] [G loss: 0.697168]\n",
      "[Epoch 0/200] [Batch 1426/3166] [D loss: 0.693641] [G loss: 0.694299]\n",
      "[Epoch 0/200] [Batch 1427/3166] [D loss: 0.693272] [G loss: 0.694977]\n",
      "[Epoch 0/200] [Batch 1428/3166] [D loss: 0.693135] [G loss: 0.695770]\n",
      "[Epoch 0/200] [Batch 1429/3166] [D loss: 0.691061] [G loss: 0.695103]\n",
      "[Epoch 0/200] [Batch 1430/3166] [D loss: 0.693176] [G loss: 0.700536]\n",
      "[Epoch 0/200] [Batch 1431/3166] [D loss: 0.693135] [G loss: 0.696864]\n",
      "[Epoch 0/200] [Batch 1432/3166] [D loss: 0.692240] [G loss: 0.695299]\n",
      "[Epoch 0/200] [Batch 1433/3166] [D loss: 0.693943] [G loss: 0.698424]\n",
      "[Epoch 0/200] [Batch 1434/3166] [D loss: 0.696811] [G loss: 0.696261]\n",
      "[Epoch 0/200] [Batch 1435/3166] [D loss: 0.692677] [G loss: 0.701499]\n",
      "[Epoch 0/200] [Batch 1436/3166] [D loss: 0.695650] [G loss: 0.694569]\n",
      "[Epoch 0/200] [Batch 1437/3166] [D loss: 0.690429] [G loss: 0.707909]\n",
      "[Epoch 0/200] [Batch 1438/3166] [D loss: 0.694139] [G loss: 0.698109]\n",
      "[Epoch 0/200] [Batch 1439/3166] [D loss: 0.692713] [G loss: 0.702770]\n",
      "[Epoch 0/200] [Batch 1440/3166] [D loss: 0.692990] [G loss: 0.708072]\n",
      "[Epoch 0/200] [Batch 1441/3166] [D loss: 0.691338] [G loss: 0.703443]\n",
      "[Epoch 0/200] [Batch 1442/3166] [D loss: 0.694705] [G loss: 0.701677]\n",
      "[Epoch 0/200] [Batch 1443/3166] [D loss: 0.691709] [G loss: 0.708864]\n",
      "[Epoch 0/200] [Batch 1444/3166] [D loss: 0.692213] [G loss: 0.708512]\n",
      "[Epoch 0/200] [Batch 1445/3166] [D loss: 0.694007] [G loss: 0.709993]\n",
      "[Epoch 0/200] [Batch 1446/3166] [D loss: 0.694865] [G loss: 0.705159]\n",
      "[Epoch 0/200] [Batch 1447/3166] [D loss: 0.690636] [G loss: 0.700430]\n",
      "[Epoch 0/200] [Batch 1448/3166] [D loss: 0.693147] [G loss: 0.702296]\n",
      "[Epoch 0/200] [Batch 1449/3166] [D loss: 0.690515] [G loss: 0.703616]\n",
      "[Epoch 0/200] [Batch 1450/3166] [D loss: 0.690483] [G loss: 0.709066]\n",
      "[Epoch 0/200] [Batch 1451/3166] [D loss: 0.695944] [G loss: 0.701443]\n",
      "[Epoch 0/200] [Batch 1452/3166] [D loss: 0.688989] [G loss: 0.707168]\n",
      "[Epoch 0/200] [Batch 1453/3166] [D loss: 0.689370] [G loss: 0.703299]\n",
      "[Epoch 0/200] [Batch 1454/3166] [D loss: 0.690867] [G loss: 0.706836]\n",
      "[Epoch 0/200] [Batch 1455/3166] [D loss: 0.688679] [G loss: 0.710991]\n",
      "[Epoch 0/200] [Batch 1456/3166] [D loss: 0.687380] [G loss: 0.710736]\n",
      "[Epoch 0/200] [Batch 1457/3166] [D loss: 0.688198] [G loss: 0.707306]\n",
      "[Epoch 0/200] [Batch 1458/3166] [D loss: 0.690175] [G loss: 0.706156]\n",
      "[Epoch 0/200] [Batch 1459/3166] [D loss: 0.688996] [G loss: 0.706539]\n",
      "[Epoch 0/200] [Batch 1460/3166] [D loss: 0.689544] [G loss: 0.708123]\n",
      "[Epoch 0/200] [Batch 1461/3166] [D loss: 0.688691] [G loss: 0.705599]\n",
      "[Epoch 0/200] [Batch 1462/3166] [D loss: 0.692858] [G loss: 0.697342]\n",
      "[Epoch 0/200] [Batch 1463/3166] [D loss: 0.693017] [G loss: 0.700335]\n",
      "[Epoch 0/200] [Batch 1464/3166] [D loss: 0.689135] [G loss: 0.704605]\n",
      "[Epoch 0/200] [Batch 1465/3166] [D loss: 0.695926] [G loss: 0.695524]\n",
      "[Epoch 0/200] [Batch 1466/3166] [D loss: 0.691916] [G loss: 0.698380]\n",
      "[Epoch 0/200] [Batch 1467/3166] [D loss: 0.692110] [G loss: 0.699654]\n",
      "[Epoch 0/200] [Batch 1468/3166] [D loss: 0.699023] [G loss: 0.695179]\n",
      "[Epoch 0/200] [Batch 1469/3166] [D loss: 0.692687] [G loss: 0.694716]\n",
      "[Epoch 0/200] [Batch 1470/3166] [D loss: 0.694646] [G loss: 0.689684]\n",
      "[Epoch 0/200] [Batch 1471/3166] [D loss: 0.691461] [G loss: 0.697900]\n",
      "[Epoch 0/200] [Batch 1472/3166] [D loss: 0.696504] [G loss: 0.692891]\n",
      "[Epoch 0/200] [Batch 1473/3166] [D loss: 0.698725] [G loss: 0.688454]\n",
      "[Epoch 0/200] [Batch 1474/3166] [D loss: 0.697690] [G loss: 0.688148]\n",
      "[Epoch 0/200] [Batch 1475/3166] [D loss: 0.697690] [G loss: 0.683427]\n",
      "[Epoch 0/200] [Batch 1476/3166] [D loss: 0.697192] [G loss: 0.682996]\n",
      "[Epoch 0/200] [Batch 1477/3166] [D loss: 0.697238] [G loss: 0.682595]\n",
      "[Epoch 0/200] [Batch 1478/3166] [D loss: 0.698096] [G loss: 0.687390]\n",
      "[Epoch 0/200] [Batch 1479/3166] [D loss: 0.696822] [G loss: 0.681823]\n",
      "[Epoch 0/200] [Batch 1480/3166] [D loss: 0.695184] [G loss: 0.679690]\n",
      "[Epoch 0/200] [Batch 1481/3166] [D loss: 0.694725] [G loss: 0.679024]\n",
      "[Epoch 0/200] [Batch 1482/3166] [D loss: 0.695452] [G loss: 0.681922]\n",
      "[Epoch 0/200] [Batch 1483/3166] [D loss: 0.694000] [G loss: 0.682131]\n",
      "[Epoch 0/200] [Batch 1484/3166] [D loss: 0.695666] [G loss: 0.683453]\n",
      "[Epoch 0/200] [Batch 1485/3166] [D loss: 0.693857] [G loss: 0.681872]\n",
      "[Epoch 0/200] [Batch 1486/3166] [D loss: 0.695189] [G loss: 0.686299]\n",
      "[Epoch 0/200] [Batch 1487/3166] [D loss: 0.693473] [G loss: 0.688274]\n",
      "[Epoch 0/200] [Batch 1488/3166] [D loss: 0.690609] [G loss: 0.685707]\n",
      "[Epoch 0/200] [Batch 1489/3166] [D loss: 0.690746] [G loss: 0.684253]\n",
      "[Epoch 0/200] [Batch 1490/3166] [D loss: 0.687677] [G loss: 0.687398]\n",
      "[Epoch 0/200] [Batch 1491/3166] [D loss: 0.696349] [G loss: 0.683453]\n",
      "[Epoch 0/200] [Batch 1492/3166] [D loss: 0.691856] [G loss: 0.692216]\n",
      "[Epoch 0/200] [Batch 1493/3166] [D loss: 0.692882] [G loss: 0.678479]\n",
      "[Epoch 0/200] [Batch 1494/3166] [D loss: 0.691672] [G loss: 0.684266]\n",
      "[Epoch 0/200] [Batch 1495/3166] [D loss: 0.690463] [G loss: 0.682712]\n",
      "[Epoch 0/200] [Batch 1496/3166] [D loss: 0.696281] [G loss: 0.683227]\n",
      "[Epoch 0/200] [Batch 1497/3166] [D loss: 0.690963] [G loss: 0.688442]\n",
      "[Epoch 0/200] [Batch 1498/3166] [D loss: 0.692496] [G loss: 0.684011]\n",
      "[Epoch 0/200] [Batch 1499/3166] [D loss: 0.689224] [G loss: 0.684105]\n",
      "[Epoch 0/200] [Batch 1500/3166] [D loss: 0.693973] [G loss: 0.680976]\n",
      "[Epoch 0/200] [Batch 1501/3166] [D loss: 0.694116] [G loss: 0.684218]\n",
      "[Epoch 0/200] [Batch 1502/3166] [D loss: 0.694806] [G loss: 0.680884]\n",
      "[Epoch 0/200] [Batch 1503/3166] [D loss: 0.691434] [G loss: 0.682632]\n",
      "[Epoch 0/200] [Batch 1504/3166] [D loss: 0.690364] [G loss: 0.688971]\n",
      "[Epoch 0/200] [Batch 1505/3166] [D loss: 0.690528] [G loss: 0.684874]\n",
      "[Epoch 0/200] [Batch 1506/3166] [D loss: 0.694459] [G loss: 0.689718]\n",
      "[Epoch 0/200] [Batch 1507/3166] [D loss: 0.695201] [G loss: 0.678518]\n",
      "[Epoch 0/200] [Batch 1508/3166] [D loss: 0.693508] [G loss: 0.692305]\n",
      "[Epoch 0/200] [Batch 1509/3166] [D loss: 0.700493] [G loss: 0.684099]\n",
      "[Epoch 0/200] [Batch 1510/3166] [D loss: 0.693789] [G loss: 0.685112]\n",
      "[Epoch 0/200] [Batch 1511/3166] [D loss: 0.697439] [G loss: 0.687857]\n",
      "[Epoch 0/200] [Batch 1512/3166] [D loss: 0.695643] [G loss: 0.697589]\n",
      "[Epoch 0/200] [Batch 1513/3166] [D loss: 0.698268] [G loss: 0.690758]\n",
      "[Epoch 0/200] [Batch 1514/3166] [D loss: 0.695894] [G loss: 0.693941]\n",
      "[Epoch 0/200] [Batch 1515/3166] [D loss: 0.696660] [G loss: 0.694643]\n",
      "[Epoch 0/200] [Batch 1516/3166] [D loss: 0.695807] [G loss: 0.694377]\n",
      "[Epoch 0/200] [Batch 1517/3166] [D loss: 0.697084] [G loss: 0.696592]\n",
      "[Epoch 0/200] [Batch 1518/3166] [D loss: 0.694488] [G loss: 0.700066]\n",
      "[Epoch 0/200] [Batch 1519/3166] [D loss: 0.695856] [G loss: 0.697479]\n",
      "[Epoch 0/200] [Batch 1520/3166] [D loss: 0.692454] [G loss: 0.702993]\n",
      "[Epoch 0/200] [Batch 1521/3166] [D loss: 0.698289] [G loss: 0.702829]\n",
      "[Epoch 0/200] [Batch 1522/3166] [D loss: 0.695528] [G loss: 0.706085]\n",
      "[Epoch 0/200] [Batch 1523/3166] [D loss: 0.692103] [G loss: 0.707792]\n",
      "[Epoch 0/200] [Batch 1524/3166] [D loss: 0.691184] [G loss: 0.707416]\n",
      "[Epoch 0/200] [Batch 1525/3166] [D loss: 0.693007] [G loss: 0.709485]\n",
      "[Epoch 0/200] [Batch 1526/3166] [D loss: 0.692630] [G loss: 0.712418]\n",
      "[Epoch 0/200] [Batch 1527/3166] [D loss: 0.688001] [G loss: 0.709844]\n",
      "[Epoch 0/200] [Batch 1528/3166] [D loss: 0.689294] [G loss: 0.707830]\n",
      "[Epoch 0/200] [Batch 1529/3166] [D loss: 0.691056] [G loss: 0.706448]\n",
      "[Epoch 0/200] [Batch 1530/3166] [D loss: 0.691933] [G loss: 0.703551]\n",
      "[Epoch 0/200] [Batch 1531/3166] [D loss: 0.691248] [G loss: 0.708886]\n",
      "[Epoch 0/200] [Batch 1532/3166] [D loss: 0.690468] [G loss: 0.707242]\n",
      "[Epoch 0/200] [Batch 1533/3166] [D loss: 0.691105] [G loss: 0.708726]\n",
      "[Epoch 0/200] [Batch 1534/3166] [D loss: 0.693860] [G loss: 0.706547]\n",
      "[Epoch 0/200] [Batch 1535/3166] [D loss: 0.692033] [G loss: 0.706345]\n",
      "[Epoch 0/200] [Batch 1536/3166] [D loss: 0.692299] [G loss: 0.708739]\n",
      "[Epoch 0/200] [Batch 1537/3166] [D loss: 0.694109] [G loss: 0.712262]\n",
      "[Epoch 0/200] [Batch 1538/3166] [D loss: 0.694808] [G loss: 0.704195]\n",
      "[Epoch 0/200] [Batch 1539/3166] [D loss: 0.691941] [G loss: 0.702784]\n",
      "[Epoch 0/200] [Batch 1540/3166] [D loss: 0.689603] [G loss: 0.711258]\n",
      "[Epoch 0/200] [Batch 1541/3166] [D loss: 0.690930] [G loss: 0.707229]\n",
      "[Epoch 0/200] [Batch 1542/3166] [D loss: 0.691868] [G loss: 0.706409]\n",
      "[Epoch 0/200] [Batch 1543/3166] [D loss: 0.693762] [G loss: 0.708541]\n",
      "[Epoch 0/200] [Batch 1544/3166] [D loss: 0.691097] [G loss: 0.704251]\n",
      "[Epoch 0/200] [Batch 1545/3166] [D loss: 0.692982] [G loss: 0.703437]\n",
      "[Epoch 0/200] [Batch 1546/3166] [D loss: 0.693382] [G loss: 0.703287]\n",
      "[Epoch 0/200] [Batch 1547/3166] [D loss: 0.691714] [G loss: 0.706727]\n",
      "[Epoch 0/200] [Batch 1548/3166] [D loss: 0.690938] [G loss: 0.699166]\n",
      "[Epoch 0/200] [Batch 1549/3166] [D loss: 0.693262] [G loss: 0.701163]\n",
      "[Epoch 0/200] [Batch 1550/3166] [D loss: 0.689169] [G loss: 0.701925]\n",
      "[Epoch 0/200] [Batch 1551/3166] [D loss: 0.689144] [G loss: 0.700842]\n",
      "[Epoch 0/200] [Batch 1552/3166] [D loss: 0.692112] [G loss: 0.697035]\n",
      "[Epoch 0/200] [Batch 1553/3166] [D loss: 0.690677] [G loss: 0.697150]\n",
      "[Epoch 0/200] [Batch 1554/3166] [D loss: 0.691763] [G loss: 0.693185]\n",
      "[Epoch 0/200] [Batch 1555/3166] [D loss: 0.692600] [G loss: 0.691348]\n",
      "[Epoch 0/200] [Batch 1556/3166] [D loss: 0.691947] [G loss: 0.695292]\n",
      "[Epoch 0/200] [Batch 1557/3166] [D loss: 0.689556] [G loss: 0.690391]\n",
      "[Epoch 0/200] [Batch 1558/3166] [D loss: 0.693791] [G loss: 0.691127]\n",
      "[Epoch 0/200] [Batch 1559/3166] [D loss: 0.691561] [G loss: 0.691799]\n",
      "[Epoch 0/200] [Batch 1560/3166] [D loss: 0.693472] [G loss: 0.690839]\n",
      "[Epoch 0/200] [Batch 1561/3166] [D loss: 0.690116] [G loss: 0.691932]\n",
      "[Epoch 0/200] [Batch 1562/3166] [D loss: 0.688464] [G loss: 0.696449]\n",
      "[Epoch 0/200] [Batch 1563/3166] [D loss: 0.690890] [G loss: 0.693662]\n",
      "[Epoch 0/200] [Batch 1564/3166] [D loss: 0.691985] [G loss: 0.687632]\n",
      "[Epoch 0/200] [Batch 1565/3166] [D loss: 0.696201] [G loss: 0.688528]\n",
      "[Epoch 0/200] [Batch 1566/3166] [D loss: 0.692653] [G loss: 0.683578]\n",
      "[Epoch 0/200] [Batch 1567/3166] [D loss: 0.697802] [G loss: 0.683249]\n",
      "[Epoch 0/200] [Batch 1568/3166] [D loss: 0.699176] [G loss: 0.687313]\n",
      "[Epoch 0/200] [Batch 1569/3166] [D loss: 0.695963] [G loss: 0.689043]\n",
      "[Epoch 0/200] [Batch 1570/3166] [D loss: 0.693672] [G loss: 0.686830]\n",
      "[Epoch 0/200] [Batch 1571/3166] [D loss: 0.694234] [G loss: 0.688440]\n",
      "[Epoch 0/200] [Batch 1572/3166] [D loss: 0.697838] [G loss: 0.684470]\n",
      "[Epoch 0/200] [Batch 1573/3166] [D loss: 0.696303] [G loss: 0.682847]\n",
      "[Epoch 0/200] [Batch 1574/3166] [D loss: 0.697168] [G loss: 0.684330]\n",
      "[Epoch 0/200] [Batch 1575/3166] [D loss: 0.694664] [G loss: 0.687367]\n",
      "[Epoch 0/200] [Batch 1576/3166] [D loss: 0.696097] [G loss: 0.693994]\n",
      "[Epoch 0/200] [Batch 1577/3166] [D loss: 0.693714] [G loss: 0.693404]\n",
      "[Epoch 0/200] [Batch 1578/3166] [D loss: 0.693864] [G loss: 0.692272]\n",
      "[Epoch 0/200] [Batch 1579/3166] [D loss: 0.693307] [G loss: 0.694199]\n",
      "[Epoch 0/200] [Batch 1580/3166] [D loss: 0.694218] [G loss: 0.694524]\n",
      "[Epoch 0/200] [Batch 1581/3166] [D loss: 0.693756] [G loss: 0.695253]\n",
      "[Epoch 0/200] [Batch 1582/3166] [D loss: 0.696758] [G loss: 0.697748]\n",
      "[Epoch 0/200] [Batch 1583/3166] [D loss: 0.692708] [G loss: 0.699923]\n",
      "[Epoch 0/200] [Batch 1584/3166] [D loss: 0.693390] [G loss: 0.695170]\n",
      "[Epoch 0/200] [Batch 1585/3166] [D loss: 0.691932] [G loss: 0.700746]\n",
      "[Epoch 0/200] [Batch 1586/3166] [D loss: 0.692823] [G loss: 0.695443]\n",
      "[Epoch 0/200] [Batch 1587/3166] [D loss: 0.693201] [G loss: 0.700066]\n",
      "[Epoch 0/200] [Batch 1588/3166] [D loss: 0.687384] [G loss: 0.706319]\n",
      "[Epoch 0/200] [Batch 1589/3166] [D loss: 0.691583] [G loss: 0.695326]\n",
      "[Epoch 0/200] [Batch 1590/3166] [D loss: 0.696602] [G loss: 0.696948]\n",
      "[Epoch 0/200] [Batch 1591/3166] [D loss: 0.689207] [G loss: 0.696305]\n",
      "[Epoch 0/200] [Batch 1592/3166] [D loss: 0.689254] [G loss: 0.698211]\n",
      "[Epoch 0/200] [Batch 1593/3166] [D loss: 0.693220] [G loss: 0.699842]\n",
      "[Epoch 0/200] [Batch 1594/3166] [D loss: 0.691429] [G loss: 0.703496]\n",
      "[Epoch 0/200] [Batch 1595/3166] [D loss: 0.690145] [G loss: 0.701195]\n",
      "[Epoch 0/200] [Batch 1596/3166] [D loss: 0.693199] [G loss: 0.700254]\n",
      "[Epoch 0/200] [Batch 1597/3166] [D loss: 0.690737] [G loss: 0.692565]\n",
      "[Epoch 0/200] [Batch 1598/3166] [D loss: 0.691701] [G loss: 0.696196]\n",
      "[Epoch 0/200] [Batch 1599/3166] [D loss: 0.694961] [G loss: 0.700705]\n",
      "[Epoch 0/200] [Batch 1600/3166] [D loss: 0.696424] [G loss: 0.696355]\n",
      "[Epoch 0/200] [Batch 1601/3166] [D loss: 0.692824] [G loss: 0.699156]\n",
      "[Epoch 0/200] [Batch 1602/3166] [D loss: 0.691751] [G loss: 0.696554]\n",
      "[Epoch 0/200] [Batch 1603/3166] [D loss: 0.689355] [G loss: 0.702258]\n",
      "[Epoch 0/200] [Batch 1604/3166] [D loss: 0.689777] [G loss: 0.702145]\n",
      "[Epoch 0/200] [Batch 1605/3166] [D loss: 0.692308] [G loss: 0.697581]\n",
      "[Epoch 0/200] [Batch 1606/3166] [D loss: 0.693720] [G loss: 0.692580]\n",
      "[Epoch 0/200] [Batch 1607/3166] [D loss: 0.694575] [G loss: 0.690460]\n",
      "[Epoch 0/200] [Batch 1608/3166] [D loss: 0.696595] [G loss: 0.688024]\n",
      "[Epoch 0/200] [Batch 1609/3166] [D loss: 0.693649] [G loss: 0.692643]\n",
      "[Epoch 0/200] [Batch 1610/3166] [D loss: 0.694715] [G loss: 0.685404]\n",
      "[Epoch 0/200] [Batch 1611/3166] [D loss: 0.692330] [G loss: 0.690819]\n",
      "[Epoch 0/200] [Batch 1612/3166] [D loss: 0.693920] [G loss: 0.691518]\n",
      "[Epoch 0/200] [Batch 1613/3166] [D loss: 0.697831] [G loss: 0.685355]\n",
      "[Epoch 0/200] [Batch 1614/3166] [D loss: 0.693957] [G loss: 0.689102]\n",
      "[Epoch 0/200] [Batch 1615/3166] [D loss: 0.695993] [G loss: 0.688325]\n",
      "[Epoch 0/200] [Batch 1616/3166] [D loss: 0.696884] [G loss: 0.693761]\n",
      "[Epoch 0/200] [Batch 1617/3166] [D loss: 0.698064] [G loss: 0.691716]\n",
      "[Epoch 0/200] [Batch 1618/3166] [D loss: 0.693846] [G loss: 0.693889]\n",
      "[Epoch 0/200] [Batch 1619/3166] [D loss: 0.689442] [G loss: 0.703314]\n",
      "[Epoch 0/200] [Batch 1620/3166] [D loss: 0.691945] [G loss: 0.694997]\n",
      "[Epoch 0/200] [Batch 1621/3166] [D loss: 0.692218] [G loss: 0.699967]\n",
      "[Epoch 0/200] [Batch 1622/3166] [D loss: 0.691953] [G loss: 0.702308]\n",
      "[Epoch 0/200] [Batch 1623/3166] [D loss: 0.693691] [G loss: 0.703965]\n",
      "[Epoch 0/200] [Batch 1624/3166] [D loss: 0.690749] [G loss: 0.702107]\n",
      "[Epoch 0/200] [Batch 1625/3166] [D loss: 0.690579] [G loss: 0.706605]\n",
      "[Epoch 0/200] [Batch 1626/3166] [D loss: 0.691219] [G loss: 0.708694]\n",
      "[Epoch 0/200] [Batch 1627/3166] [D loss: 0.688843] [G loss: 0.706914]\n",
      "[Epoch 0/200] [Batch 1628/3166] [D loss: 0.693254] [G loss: 0.702959]\n",
      "[Epoch 0/200] [Batch 1629/3166] [D loss: 0.690858] [G loss: 0.705466]\n",
      "[Epoch 0/200] [Batch 1630/3166] [D loss: 0.693620] [G loss: 0.704450]\n",
      "[Epoch 0/200] [Batch 1631/3166] [D loss: 0.688474] [G loss: 0.705344]\n",
      "[Epoch 0/200] [Batch 1632/3166] [D loss: 0.691456] [G loss: 0.703346]\n",
      "[Epoch 0/200] [Batch 1633/3166] [D loss: 0.688447] [G loss: 0.703607]\n",
      "[Epoch 0/200] [Batch 1634/3166] [D loss: 0.694152] [G loss: 0.696966]\n",
      "[Epoch 0/200] [Batch 1635/3166] [D loss: 0.690745] [G loss: 0.698668]\n",
      "[Epoch 0/200] [Batch 1636/3166] [D loss: 0.692207] [G loss: 0.698180]\n",
      "[Epoch 0/200] [Batch 1637/3166] [D loss: 0.691176] [G loss: 0.695480]\n",
      "[Epoch 0/200] [Batch 1638/3166] [D loss: 0.690667] [G loss: 0.690575]\n",
      "[Epoch 0/200] [Batch 1639/3166] [D loss: 0.699653] [G loss: 0.694337]\n",
      "[Epoch 0/200] [Batch 1640/3166] [D loss: 0.693338] [G loss: 0.693234]\n",
      "[Epoch 0/200] [Batch 1641/3166] [D loss: 0.699853] [G loss: 0.685210]\n",
      "[Epoch 0/200] [Batch 1642/3166] [D loss: 0.698653] [G loss: 0.683816]\n",
      "[Epoch 0/200] [Batch 1643/3166] [D loss: 0.694245] [G loss: 0.684287]\n",
      "[Epoch 0/200] [Batch 1644/3166] [D loss: 0.699044] [G loss: 0.682041]\n",
      "[Epoch 0/200] [Batch 1645/3166] [D loss: 0.700421] [G loss: 0.682894]\n",
      "[Epoch 0/200] [Batch 1646/3166] [D loss: 0.696210] [G loss: 0.686862]\n",
      "[Epoch 0/200] [Batch 1647/3166] [D loss: 0.696638] [G loss: 0.680215]\n",
      "[Epoch 0/200] [Batch 1648/3166] [D loss: 0.695812] [G loss: 0.687634]\n",
      "[Epoch 0/200] [Batch 1649/3166] [D loss: 0.695830] [G loss: 0.687107]\n",
      "[Epoch 0/200] [Batch 1650/3166] [D loss: 0.687271] [G loss: 0.698335]\n",
      "[Epoch 0/200] [Batch 1651/3166] [D loss: 0.693264] [G loss: 0.694807]\n",
      "[Epoch 0/200] [Batch 1652/3166] [D loss: 0.690629] [G loss: 0.699157]\n",
      "[Epoch 0/200] [Batch 1653/3166] [D loss: 0.693019] [G loss: 0.693215]\n",
      "[Epoch 0/200] [Batch 1654/3166] [D loss: 0.689680] [G loss: 0.697985]\n",
      "[Epoch 0/200] [Batch 1655/3166] [D loss: 0.691441] [G loss: 0.690591]\n",
      "[Epoch 0/200] [Batch 1656/3166] [D loss: 0.694048] [G loss: 0.694662]\n",
      "[Epoch 0/200] [Batch 1657/3166] [D loss: 0.691110] [G loss: 0.693542]\n",
      "[Epoch 0/200] [Batch 1658/3166] [D loss: 0.695677] [G loss: 0.690631]\n",
      "[Epoch 0/200] [Batch 1659/3166] [D loss: 0.690675] [G loss: 0.692901]\n",
      "[Epoch 0/200] [Batch 1660/3166] [D loss: 0.697419] [G loss: 0.684559]\n",
      "[Epoch 0/200] [Batch 1661/3166] [D loss: 0.689009] [G loss: 0.690500]\n",
      "[Epoch 0/200] [Batch 1662/3166] [D loss: 0.691981] [G loss: 0.689942]\n",
      "[Epoch 0/200] [Batch 1663/3166] [D loss: 0.689616] [G loss: 0.696487]\n",
      "[Epoch 0/200] [Batch 1664/3166] [D loss: 0.684343] [G loss: 0.721424]\n",
      "[Epoch 0/200] [Batch 1665/3166] [D loss: 0.686041] [G loss: 0.705708]\n",
      "[Epoch 0/200] [Batch 1666/3166] [D loss: 0.688219] [G loss: 0.715273]\n",
      "[Epoch 0/200] [Batch 1667/3166] [D loss: 0.692574] [G loss: 0.701676]\n",
      "[Epoch 0/200] [Batch 1668/3166] [D loss: 0.712382] [G loss: 0.693169]\n",
      "[Epoch 0/200] [Batch 1669/3166] [D loss: 0.706231] [G loss: 0.683391]\n",
      "[Epoch 0/200] [Batch 1670/3166] [D loss: 0.698151] [G loss: 0.681713]\n",
      "[Epoch 0/200] [Batch 1671/3166] [D loss: 0.703617] [G loss: 0.675951]\n",
      "[Epoch 0/200] [Batch 1672/3166] [D loss: 0.698569] [G loss: 0.676235]\n",
      "[Epoch 0/200] [Batch 1673/3166] [D loss: 0.700591] [G loss: 0.678861]\n",
      "[Epoch 0/200] [Batch 1674/3166] [D loss: 0.696738] [G loss: 0.677447]\n",
      "[Epoch 0/200] [Batch 1675/3166] [D loss: 0.695430] [G loss: 0.683024]\n",
      "[Epoch 0/200] [Batch 1676/3166] [D loss: 0.694123] [G loss: 0.681436]\n",
      "[Epoch 0/200] [Batch 1677/3166] [D loss: 0.692454] [G loss: 0.684819]\n",
      "[Epoch 0/200] [Batch 1678/3166] [D loss: 0.693014] [G loss: 0.688423]\n",
      "[Epoch 0/200] [Batch 1679/3166] [D loss: 0.691488] [G loss: 0.688298]\n",
      "[Epoch 0/200] [Batch 1680/3166] [D loss: 0.687787] [G loss: 0.691767]\n",
      "[Epoch 0/200] [Batch 1681/3166] [D loss: 0.688292] [G loss: 0.692142]\n",
      "[Epoch 0/200] [Batch 1682/3166] [D loss: 0.689648] [G loss: 0.694267]\n",
      "[Epoch 0/200] [Batch 1683/3166] [D loss: 0.690629] [G loss: 0.692543]\n",
      "[Epoch 0/200] [Batch 1684/3166] [D loss: 0.687923] [G loss: 0.695673]\n",
      "[Epoch 0/200] [Batch 1685/3166] [D loss: 0.686872] [G loss: 0.693090]\n",
      "[Epoch 0/200] [Batch 1686/3166] [D loss: 0.687537] [G loss: 0.695931]\n",
      "[Epoch 0/200] [Batch 1687/3166] [D loss: 0.685727] [G loss: 0.697549]\n",
      "[Epoch 0/200] [Batch 1688/3166] [D loss: 0.683893] [G loss: 0.698805]\n",
      "[Epoch 0/200] [Batch 1689/3166] [D loss: 0.687695] [G loss: 0.696066]\n",
      "[Epoch 0/200] [Batch 1690/3166] [D loss: 0.687062] [G loss: 0.692322]\n",
      "[Epoch 0/200] [Batch 1691/3166] [D loss: 0.686629] [G loss: 0.695688]\n",
      "[Epoch 0/200] [Batch 1692/3166] [D loss: 0.687578] [G loss: 0.689810]\n",
      "[Epoch 0/200] [Batch 1693/3166] [D loss: 0.685376] [G loss: 0.694868]\n",
      "[Epoch 0/200] [Batch 1694/3166] [D loss: 0.684736] [G loss: 0.687330]\n",
      "[Epoch 0/200] [Batch 1695/3166] [D loss: 0.691652] [G loss: 0.688242]\n",
      "[Epoch 0/200] [Batch 1696/3166] [D loss: 0.692968] [G loss: 0.685889]\n",
      "[Epoch 0/200] [Batch 1697/3166] [D loss: 0.694465] [G loss: 0.680577]\n",
      "[Epoch 0/200] [Batch 1698/3166] [D loss: 0.692139] [G loss: 0.676489]\n",
      "[Epoch 0/200] [Batch 1699/3166] [D loss: 0.701100] [G loss: 0.667145]\n",
      "[Epoch 0/200] [Batch 1700/3166] [D loss: 0.686462] [G loss: 0.692873]\n",
      "[Epoch 0/200] [Batch 1701/3166] [D loss: 0.702674] [G loss: 0.670166]\n",
      "[Epoch 0/200] [Batch 1702/3166] [D loss: 0.701203] [G loss: 0.678344]\n",
      "[Epoch 0/200] [Batch 1703/3166] [D loss: 0.695242] [G loss: 0.681265]\n",
      "[Epoch 0/200] [Batch 1704/3166] [D loss: 0.703233] [G loss: 0.677888]\n",
      "[Epoch 0/200] [Batch 1705/3166] [D loss: 0.697400] [G loss: 0.694585]\n",
      "[Epoch 0/200] [Batch 1706/3166] [D loss: 0.696424] [G loss: 0.686416]\n",
      "[Epoch 0/200] [Batch 1707/3166] [D loss: 0.696287] [G loss: 0.685648]\n",
      "[Epoch 0/200] [Batch 1708/3166] [D loss: 0.695865] [G loss: 0.690283]\n",
      "[Epoch 0/200] [Batch 1709/3166] [D loss: 0.696737] [G loss: 0.691598]\n",
      "[Epoch 0/200] [Batch 1710/3166] [D loss: 0.692943] [G loss: 0.704171]\n",
      "[Epoch 0/200] [Batch 1711/3166] [D loss: 0.694628] [G loss: 0.702863]\n",
      "[Epoch 0/200] [Batch 1712/3166] [D loss: 0.692140] [G loss: 0.713067]\n",
      "[Epoch 0/200] [Batch 1713/3166] [D loss: 0.693692] [G loss: 0.710105]\n",
      "[Epoch 0/200] [Batch 1714/3166] [D loss: 0.685864] [G loss: 0.720882]\n",
      "[Epoch 0/200] [Batch 1715/3166] [D loss: 0.688295] [G loss: 0.724210]\n",
      "[Epoch 0/200] [Batch 1716/3166] [D loss: 0.686631] [G loss: 0.722668]\n",
      "[Epoch 0/200] [Batch 1717/3166] [D loss: 0.691002] [G loss: 0.723144]\n",
      "[Epoch 0/200] [Batch 1718/3166] [D loss: 0.690820] [G loss: 0.721014]\n",
      "[Epoch 0/200] [Batch 1719/3166] [D loss: 0.691869] [G loss: 0.714246]\n",
      "[Epoch 0/200] [Batch 1720/3166] [D loss: 0.687719] [G loss: 0.721244]\n",
      "[Epoch 0/200] [Batch 1721/3166] [D loss: 0.683622] [G loss: 0.724220]\n",
      "[Epoch 0/200] [Batch 1722/3166] [D loss: 0.687266] [G loss: 0.723344]\n",
      "[Epoch 0/200] [Batch 1723/3166] [D loss: 0.686366] [G loss: 0.713411]\n",
      "[Epoch 0/200] [Batch 1724/3166] [D loss: 0.684488] [G loss: 0.715041]\n",
      "[Epoch 0/200] [Batch 1725/3166] [D loss: 0.684185] [G loss: 0.713052]\n",
      "[Epoch 0/200] [Batch 1726/3166] [D loss: 0.684090] [G loss: 0.712000]\n",
      "[Epoch 0/200] [Batch 1727/3166] [D loss: 0.682747] [G loss: 0.714907]\n",
      "[Epoch 0/200] [Batch 1728/3166] [D loss: 0.681532] [G loss: 0.714799]\n",
      "[Epoch 0/200] [Batch 1729/3166] [D loss: 0.689563] [G loss: 0.697711]\n",
      "[Epoch 0/200] [Batch 1730/3166] [D loss: 0.685539] [G loss: 0.704612]\n",
      "[Epoch 0/200] [Batch 1731/3166] [D loss: 0.683927] [G loss: 0.706927]\n",
      "[Epoch 0/200] [Batch 1732/3166] [D loss: 0.692467] [G loss: 0.697107]\n",
      "[Epoch 0/200] [Batch 1733/3166] [D loss: 0.688427] [G loss: 0.696626]\n",
      "[Epoch 0/200] [Batch 1734/3166] [D loss: 0.690228] [G loss: 0.696127]\n",
      "[Epoch 0/200] [Batch 1735/3166] [D loss: 0.688299] [G loss: 0.699784]\n",
      "[Epoch 0/200] [Batch 1736/3166] [D loss: 0.687773] [G loss: 0.695964]\n",
      "[Epoch 0/200] [Batch 1737/3166] [D loss: 0.691537] [G loss: 0.691129]\n",
      "[Epoch 0/200] [Batch 1738/3166] [D loss: 0.690709] [G loss: 0.692549]\n",
      "[Epoch 0/200] [Batch 1739/3166] [D loss: 0.700242] [G loss: 0.680320]\n",
      "[Epoch 0/200] [Batch 1740/3166] [D loss: 0.697585] [G loss: 0.678168]\n",
      "[Epoch 0/200] [Batch 1741/3166] [D loss: 0.697131] [G loss: 0.674712]\n",
      "[Epoch 0/200] [Batch 1742/3166] [D loss: 0.697991] [G loss: 0.682537]\n",
      "[Epoch 0/200] [Batch 1743/3166] [D loss: 0.699301] [G loss: 0.683102]\n",
      "[Epoch 0/200] [Batch 1744/3166] [D loss: 0.697163] [G loss: 0.678688]\n",
      "[Epoch 0/200] [Batch 1745/3166] [D loss: 0.703599] [G loss: 0.679323]\n",
      "[Epoch 0/200] [Batch 1746/3166] [D loss: 0.700407] [G loss: 0.678746]\n",
      "[Epoch 0/200] [Batch 1747/3166] [D loss: 0.697247] [G loss: 0.687950]\n",
      "[Epoch 0/200] [Batch 1748/3166] [D loss: 0.697636] [G loss: 0.686009]\n",
      "[Epoch 0/200] [Batch 1749/3166] [D loss: 0.695934] [G loss: 0.695665]\n",
      "[Epoch 0/200] [Batch 1750/3166] [D loss: 0.695578] [G loss: 0.699364]\n",
      "[Epoch 0/200] [Batch 1751/3166] [D loss: 0.690268] [G loss: 0.703172]\n",
      "[Epoch 0/200] [Batch 1752/3166] [D loss: 0.687022] [G loss: 0.720477]\n",
      "[Epoch 0/200] [Batch 1753/3166] [D loss: 0.684046] [G loss: 0.714745]\n",
      "[Epoch 0/200] [Batch 1754/3166] [D loss: 0.687622] [G loss: 0.724158]\n",
      "[Epoch 0/200] [Batch 1755/3166] [D loss: 0.686002] [G loss: 0.715802]\n",
      "[Epoch 0/200] [Batch 1756/3166] [D loss: 0.683808] [G loss: 0.726597]\n",
      "[Epoch 0/200] [Batch 1757/3166] [D loss: 0.693640] [G loss: 0.708723]\n",
      "[Epoch 0/200] [Batch 1758/3166] [D loss: 0.685403] [G loss: 0.708791]\n",
      "[Epoch 0/200] [Batch 1759/3166] [D loss: 0.702444] [G loss: 0.697148]\n",
      "[Epoch 0/200] [Batch 1760/3166] [D loss: 0.692426] [G loss: 0.690623]\n",
      "[Epoch 0/200] [Batch 1761/3166] [D loss: 0.695690] [G loss: 0.692700]\n",
      "[Epoch 0/200] [Batch 1762/3166] [D loss: 0.691772] [G loss: 0.683220]\n",
      "[Epoch 0/200] [Batch 1763/3166] [D loss: 0.704953] [G loss: 0.683667]\n",
      "[Epoch 0/200] [Batch 1764/3166] [D loss: 0.698359] [G loss: 0.679638]\n",
      "[Epoch 0/200] [Batch 1765/3166] [D loss: 0.694536] [G loss: 0.677200]\n",
      "[Epoch 0/200] [Batch 1766/3166] [D loss: 0.699768] [G loss: 0.672567]\n",
      "[Epoch 0/200] [Batch 1767/3166] [D loss: 0.697662] [G loss: 0.674113]\n",
      "[Epoch 0/200] [Batch 1768/3166] [D loss: 0.702900] [G loss: 0.664870]\n",
      "[Epoch 0/200] [Batch 1769/3166] [D loss: 0.698487] [G loss: 0.669227]\n",
      "[Epoch 0/200] [Batch 1770/3166] [D loss: 0.702280] [G loss: 0.662753]\n",
      "[Epoch 0/200] [Batch 1771/3166] [D loss: 0.702952] [G loss: 0.664304]\n",
      "[Epoch 0/200] [Batch 1772/3166] [D loss: 0.706321] [G loss: 0.659866]\n",
      "[Epoch 0/200] [Batch 1773/3166] [D loss: 0.705753] [G loss: 0.662096]\n",
      "[Epoch 0/200] [Batch 1774/3166] [D loss: 0.697664] [G loss: 0.671838]\n",
      "[Epoch 0/200] [Batch 1775/3166] [D loss: 0.700892] [G loss: 0.674593]\n",
      "[Epoch 0/200] [Batch 1776/3166] [D loss: 0.699268] [G loss: 0.676745]\n",
      "[Epoch 0/200] [Batch 1777/3166] [D loss: 0.699024] [G loss: 0.681020]\n",
      "[Epoch 0/200] [Batch 1778/3166] [D loss: 0.698303] [G loss: 0.681645]\n",
      "[Epoch 0/200] [Batch 1779/3166] [D loss: 0.696649] [G loss: 0.683632]\n",
      "[Epoch 0/200] [Batch 1780/3166] [D loss: 0.694536] [G loss: 0.691571]\n",
      "[Epoch 0/200] [Batch 1781/3166] [D loss: 0.694900] [G loss: 0.697541]\n",
      "[Epoch 0/200] [Batch 1782/3166] [D loss: 0.693273] [G loss: 0.689455]\n",
      "[Epoch 0/200] [Batch 1783/3166] [D loss: 0.688842] [G loss: 0.693076]\n",
      "[Epoch 0/200] [Batch 1784/3166] [D loss: 0.693288] [G loss: 0.697951]\n",
      "[Epoch 0/200] [Batch 1785/3166] [D loss: 0.687986] [G loss: 0.710808]\n",
      "[Epoch 0/200] [Batch 1786/3166] [D loss: 0.687595] [G loss: 0.710369]\n",
      "[Epoch 0/200] [Batch 1787/3166] [D loss: 0.683996] [G loss: 0.705419]\n",
      "[Epoch 0/200] [Batch 1788/3166] [D loss: 0.683307] [G loss: 0.720062]\n",
      "[Epoch 0/200] [Batch 1789/3166] [D loss: 0.690412] [G loss: 0.702094]\n",
      "[Epoch 0/200] [Batch 1790/3166] [D loss: 0.688652] [G loss: 0.704791]\n",
      "[Epoch 0/200] [Batch 1791/3166] [D loss: 0.691181] [G loss: 0.695166]\n",
      "[Epoch 0/200] [Batch 1792/3166] [D loss: 0.694714] [G loss: 0.693374]\n",
      "[Epoch 0/200] [Batch 1793/3166] [D loss: 0.689128] [G loss: 0.695386]\n",
      "[Epoch 0/200] [Batch 1794/3166] [D loss: 0.684985] [G loss: 0.694209]\n",
      "[Epoch 0/200] [Batch 1795/3166] [D loss: 0.690303] [G loss: 0.692011]\n",
      "[Epoch 0/200] [Batch 1796/3166] [D loss: 0.687682] [G loss: 0.693734]\n",
      "[Epoch 0/200] [Batch 1797/3166] [D loss: 0.687872] [G loss: 0.694027]\n",
      "[Epoch 0/200] [Batch 1798/3166] [D loss: 0.685013] [G loss: 0.693411]\n",
      "[Epoch 0/200] [Batch 1799/3166] [D loss: 0.686267] [G loss: 0.695118]\n",
      "[Epoch 0/200] [Batch 1800/3166] [D loss: 0.687616] [G loss: 0.691229]\n",
      "[Epoch 0/200] [Batch 1801/3166] [D loss: 0.686366] [G loss: 0.688520]\n",
      "[Epoch 0/200] [Batch 1802/3166] [D loss: 0.685493] [G loss: 0.686789]\n",
      "[Epoch 0/200] [Batch 1803/3166] [D loss: 0.690258] [G loss: 0.689651]\n",
      "[Epoch 0/200] [Batch 1804/3166] [D loss: 0.690525] [G loss: 0.683168]\n",
      "[Epoch 0/200] [Batch 1805/3166] [D loss: 0.694671] [G loss: 0.677348]\n",
      "[Epoch 0/200] [Batch 1806/3166] [D loss: 0.688671] [G loss: 0.681714]\n",
      "[Epoch 0/200] [Batch 1807/3166] [D loss: 0.694953] [G loss: 0.671499]\n",
      "[Epoch 0/200] [Batch 1808/3166] [D loss: 0.698029] [G loss: 0.678463]\n",
      "[Epoch 0/200] [Batch 1809/3166] [D loss: 0.693215] [G loss: 0.681763]\n",
      "[Epoch 0/200] [Batch 1810/3166] [D loss: 0.689430] [G loss: 0.672444]\n",
      "[Epoch 0/200] [Batch 1811/3166] [D loss: 0.692252] [G loss: 0.672790]\n",
      "[Epoch 0/200] [Batch 1812/3166] [D loss: 0.691226] [G loss: 0.673931]\n",
      "[Epoch 0/200] [Batch 1813/3166] [D loss: 0.695408] [G loss: 0.678432]\n",
      "[Epoch 0/200] [Batch 1814/3166] [D loss: 0.703452] [G loss: 0.672967]\n",
      "[Epoch 0/200] [Batch 1815/3166] [D loss: 0.696436] [G loss: 0.676860]\n",
      "[Epoch 0/200] [Batch 1816/3166] [D loss: 0.698725] [G loss: 0.686005]\n",
      "[Epoch 0/200] [Batch 1817/3166] [D loss: 0.698176] [G loss: 0.673410]\n",
      "[Epoch 0/200] [Batch 1818/3166] [D loss: 0.693604] [G loss: 0.680750]\n",
      "[Epoch 0/200] [Batch 1819/3166] [D loss: 0.698337] [G loss: 0.683844]\n",
      "[Epoch 0/200] [Batch 1820/3166] [D loss: 0.699429] [G loss: 0.678632]\n",
      "[Epoch 0/200] [Batch 1821/3166] [D loss: 0.700269] [G loss: 0.683564]\n",
      "[Epoch 0/200] [Batch 1822/3166] [D loss: 0.691673] [G loss: 0.692356]\n",
      "[Epoch 0/200] [Batch 1823/3166] [D loss: 0.697625] [G loss: 0.693213]\n",
      "[Epoch 0/200] [Batch 1824/3166] [D loss: 0.698564] [G loss: 0.693733]\n",
      "[Epoch 0/200] [Batch 1825/3166] [D loss: 0.694528] [G loss: 0.693921]\n",
      "[Epoch 0/200] [Batch 1826/3166] [D loss: 0.697005] [G loss: 0.694959]\n",
      "[Epoch 0/200] [Batch 1827/3166] [D loss: 0.697628] [G loss: 0.700784]\n",
      "[Epoch 0/200] [Batch 1828/3166] [D loss: 0.689278] [G loss: 0.705038]\n",
      "[Epoch 0/200] [Batch 1829/3166] [D loss: 0.690008] [G loss: 0.708956]\n",
      "[Epoch 0/200] [Batch 1830/3166] [D loss: 0.690587] [G loss: 0.703289]\n",
      "[Epoch 0/200] [Batch 1831/3166] [D loss: 0.687497] [G loss: 0.709044]\n",
      "[Epoch 0/200] [Batch 1832/3166] [D loss: 0.692042] [G loss: 0.713373]\n",
      "[Epoch 0/200] [Batch 1833/3166] [D loss: 0.687507] [G loss: 0.709029]\n",
      "[Epoch 0/200] [Batch 1834/3166] [D loss: 0.689539] [G loss: 0.707689]\n",
      "[Epoch 0/200] [Batch 1835/3166] [D loss: 0.687789] [G loss: 0.712979]\n",
      "[Epoch 0/200] [Batch 1836/3166] [D loss: 0.687958] [G loss: 0.713318]\n",
      "[Epoch 0/200] [Batch 1837/3166] [D loss: 0.689333] [G loss: 0.703763]\n",
      "[Epoch 0/200] [Batch 1838/3166] [D loss: 0.688549] [G loss: 0.710383]\n",
      "[Epoch 0/200] [Batch 1839/3166] [D loss: 0.689485] [G loss: 0.701226]\n",
      "[Epoch 0/200] [Batch 1840/3166] [D loss: 0.688668] [G loss: 0.703165]\n",
      "[Epoch 0/200] [Batch 1841/3166] [D loss: 0.689970] [G loss: 0.707279]\n",
      "[Epoch 0/200] [Batch 1842/3166] [D loss: 0.690425] [G loss: 0.703933]\n",
      "[Epoch 0/200] [Batch 1843/3166] [D loss: 0.690460] [G loss: 0.707058]\n",
      "[Epoch 0/200] [Batch 1844/3166] [D loss: 0.687636] [G loss: 0.705110]\n",
      "[Epoch 0/200] [Batch 1845/3166] [D loss: 0.685609] [G loss: 0.705390]\n",
      "[Epoch 0/200] [Batch 1846/3166] [D loss: 0.687736] [G loss: 0.693603]\n",
      "[Epoch 0/200] [Batch 1847/3166] [D loss: 0.693691] [G loss: 0.698056]\n",
      "[Epoch 0/200] [Batch 1848/3166] [D loss: 0.685876] [G loss: 0.697492]\n",
      "[Epoch 0/200] [Batch 1849/3166] [D loss: 0.695209] [G loss: 0.691955]\n",
      "[Epoch 0/200] [Batch 1850/3166] [D loss: 0.687283] [G loss: 0.688522]\n",
      "[Epoch 0/200] [Batch 1851/3166] [D loss: 0.693525] [G loss: 0.685642]\n",
      "[Epoch 0/200] [Batch 1852/3166] [D loss: 0.692857] [G loss: 0.683771]\n",
      "[Epoch 0/200] [Batch 1853/3166] [D loss: 0.690166] [G loss: 0.687512]\n",
      "[Epoch 0/200] [Batch 1854/3166] [D loss: 0.692713] [G loss: 0.684583]\n",
      "[Epoch 0/200] [Batch 1855/3166] [D loss: 0.689912] [G loss: 0.693310]\n",
      "[Epoch 0/200] [Batch 1856/3166] [D loss: 0.690529] [G loss: 0.692012]\n",
      "[Epoch 0/200] [Batch 1857/3166] [D loss: 0.691021] [G loss: 0.688100]\n",
      "[Epoch 0/200] [Batch 1858/3166] [D loss: 0.693853] [G loss: 0.691642]\n",
      "[Epoch 0/200] [Batch 1859/3166] [D loss: 0.692910] [G loss: 0.691956]\n",
      "[Epoch 0/200] [Batch 1860/3166] [D loss: 0.690554] [G loss: 0.696788]\n",
      "[Epoch 0/200] [Batch 1861/3166] [D loss: 0.689397] [G loss: 0.695732]\n",
      "[Epoch 0/200] [Batch 1862/3166] [D loss: 0.686797] [G loss: 0.702819]\n",
      "[Epoch 0/200] [Batch 1863/3166] [D loss: 0.683530] [G loss: 0.703252]\n",
      "[Epoch 0/200] [Batch 1864/3166] [D loss: 0.683106] [G loss: 0.711037]\n",
      "[Epoch 0/200] [Batch 1865/3166] [D loss: 0.680402] [G loss: 0.713921]\n",
      "[Epoch 0/200] [Batch 1866/3166] [D loss: 0.681877] [G loss: 0.707198]\n",
      "[Epoch 0/200] [Batch 1867/3166] [D loss: 0.680672] [G loss: 0.710816]\n",
      "[Epoch 0/200] [Batch 1868/3166] [D loss: 0.678203] [G loss: 0.711271]\n",
      "[Epoch 0/200] [Batch 1869/3166] [D loss: 0.669956] [G loss: 0.710495]\n",
      "[Epoch 0/200] [Batch 1870/3166] [D loss: 0.678989] [G loss: 0.709871]\n",
      "[Epoch 0/200] [Batch 1871/3166] [D loss: 0.669575] [G loss: 0.718097]\n",
      "[Epoch 0/200] [Batch 1872/3166] [D loss: 0.672253] [G loss: 0.718611]\n",
      "[Epoch 0/200] [Batch 1873/3166] [D loss: 0.663616] [G loss: 0.721708]\n",
      "[Epoch 0/200] [Batch 1874/3166] [D loss: 0.666889] [G loss: 0.715010]\n",
      "[Epoch 0/200] [Batch 1875/3166] [D loss: 0.676949] [G loss: 0.709578]\n",
      "[Epoch 0/200] [Batch 1876/3166] [D loss: 0.678098] [G loss: 0.714596]\n",
      "[Epoch 0/200] [Batch 1877/3166] [D loss: 0.672522] [G loss: 0.688336]\n",
      "[Epoch 0/200] [Batch 1878/3166] [D loss: 0.699278] [G loss: 0.640944]\n",
      "[Epoch 0/200] [Batch 1879/3166] [D loss: 0.715610] [G loss: 0.627163]\n",
      "[Epoch 0/200] [Batch 1880/3166] [D loss: 0.706540] [G loss: 0.626012]\n",
      "[Epoch 0/200] [Batch 1881/3166] [D loss: 0.722579] [G loss: 0.619458]\n",
      "[Epoch 0/200] [Batch 1882/3166] [D loss: 0.711065] [G loss: 0.634731]\n",
      "[Epoch 0/200] [Batch 1883/3166] [D loss: 0.707431] [G loss: 0.664106]\n",
      "[Epoch 0/200] [Batch 1884/3166] [D loss: 0.696220] [G loss: 0.686849]\n",
      "[Epoch 0/200] [Batch 1885/3166] [D loss: 0.693332] [G loss: 0.692896]\n",
      "[Epoch 0/200] [Batch 1886/3166] [D loss: 0.689574] [G loss: 0.690835]\n",
      "[Epoch 0/200] [Batch 1887/3166] [D loss: 0.691758] [G loss: 0.704879]\n",
      "[Epoch 0/200] [Batch 1888/3166] [D loss: 0.679851] [G loss: 0.726587]\n",
      "[Epoch 0/200] [Batch 1889/3166] [D loss: 0.674508] [G loss: 0.723555]\n",
      "[Epoch 0/200] [Batch 1890/3166] [D loss: 0.675367] [G loss: 0.743515]\n",
      "[Epoch 0/200] [Batch 1891/3166] [D loss: 0.673335] [G loss: 0.757724]\n",
      "[Epoch 0/200] [Batch 1892/3166] [D loss: 0.686346] [G loss: 0.735603]\n",
      "[Epoch 0/200] [Batch 1893/3166] [D loss: 0.688891] [G loss: 0.709043]\n",
      "[Epoch 0/200] [Batch 1894/3166] [D loss: 0.702332] [G loss: 0.702239]\n",
      "[Epoch 0/200] [Batch 1895/3166] [D loss: 0.711983] [G loss: 0.681341]\n",
      "[Epoch 0/200] [Batch 1896/3166] [D loss: 0.700426] [G loss: 0.678065]\n",
      "[Epoch 0/200] [Batch 1897/3166] [D loss: 0.691094] [G loss: 0.681867]\n",
      "[Epoch 0/200] [Batch 1898/3166] [D loss: 0.699424] [G loss: 0.678193]\n",
      "[Epoch 0/200] [Batch 1899/3166] [D loss: 0.685424] [G loss: 0.688509]\n",
      "[Epoch 0/200] [Batch 1900/3166] [D loss: 0.690852] [G loss: 0.690885]\n",
      "[Epoch 0/200] [Batch 1901/3166] [D loss: 0.698618] [G loss: 0.687550]\n",
      "[Epoch 0/200] [Batch 1902/3166] [D loss: 0.689607] [G loss: 0.685169]\n",
      "[Epoch 0/200] [Batch 1903/3166] [D loss: 0.688410] [G loss: 0.678689]\n",
      "[Epoch 0/200] [Batch 1904/3166] [D loss: 0.697905] [G loss: 0.674023]\n",
      "[Epoch 0/200] [Batch 1905/3166] [D loss: 0.696748] [G loss: 0.675553]\n",
      "[Epoch 0/200] [Batch 1906/3166] [D loss: 0.702401] [G loss: 0.665404]\n",
      "[Epoch 0/200] [Batch 1907/3166] [D loss: 0.697460] [G loss: 0.695218]\n",
      "[Epoch 0/200] [Batch 1908/3166] [D loss: 0.701064] [G loss: 0.681687]\n",
      "[Epoch 0/200] [Batch 1909/3166] [D loss: 0.688555] [G loss: 0.695994]\n",
      "[Epoch 0/200] [Batch 1910/3166] [D loss: 0.685599] [G loss: 0.716816]\n",
      "[Epoch 0/200] [Batch 1911/3166] [D loss: 0.673965] [G loss: 0.728059]\n",
      "[Epoch 0/200] [Batch 1912/3166] [D loss: 0.671416] [G loss: 0.749751]\n",
      "[Epoch 0/200] [Batch 1913/3166] [D loss: 0.684484] [G loss: 0.730551]\n",
      "[Epoch 0/200] [Batch 1914/3166] [D loss: 0.679579] [G loss: 0.721956]\n",
      "[Epoch 0/200] [Batch 1915/3166] [D loss: 0.694296] [G loss: 0.706733]\n",
      "[Epoch 0/200] [Batch 1916/3166] [D loss: 0.682244] [G loss: 0.714261]\n",
      "[Epoch 0/200] [Batch 1917/3166] [D loss: 0.685334] [G loss: 0.706859]\n",
      "[Epoch 0/200] [Batch 1918/3166] [D loss: 0.680595] [G loss: 0.712146]\n",
      "[Epoch 0/200] [Batch 1919/3166] [D loss: 0.695214] [G loss: 0.698873]\n",
      "[Epoch 0/200] [Batch 1920/3166] [D loss: 0.684714] [G loss: 0.699610]\n",
      "[Epoch 0/200] [Batch 1921/3166] [D loss: 0.688849] [G loss: 0.692273]\n",
      "[Epoch 0/200] [Batch 1922/3166] [D loss: 0.689314] [G loss: 0.682299]\n",
      "[Epoch 0/200] [Batch 1923/3166] [D loss: 0.706768] [G loss: 0.672916]\n",
      "[Epoch 0/200] [Batch 1924/3166] [D loss: 0.704916] [G loss: 0.665818]\n",
      "[Epoch 0/200] [Batch 1925/3166] [D loss: 0.698100] [G loss: 0.675500]\n",
      "[Epoch 0/200] [Batch 1926/3166] [D loss: 0.704093] [G loss: 0.671685]\n",
      "[Epoch 0/200] [Batch 1927/3166] [D loss: 0.693573] [G loss: 0.687451]\n",
      "[Epoch 0/200] [Batch 1928/3166] [D loss: 0.690121] [G loss: 0.704802]\n",
      "[Epoch 0/200] [Batch 1929/3166] [D loss: 0.704638] [G loss: 0.705349]\n",
      "[Epoch 0/200] [Batch 1930/3166] [D loss: 0.692201] [G loss: 0.708453]\n",
      "[Epoch 0/200] [Batch 1931/3166] [D loss: 0.703833] [G loss: 0.708043]\n",
      "[Epoch 0/200] [Batch 1932/3166] [D loss: 0.700517] [G loss: 0.715808]\n",
      "[Epoch 0/200] [Batch 1933/3166] [D loss: 0.704398] [G loss: 0.714912]\n",
      "[Epoch 0/200] [Batch 1934/3166] [D loss: 0.692973] [G loss: 0.715586]\n",
      "[Epoch 0/200] [Batch 1935/3166] [D loss: 0.697728] [G loss: 0.715869]\n",
      "[Epoch 0/200] [Batch 1936/3166] [D loss: 0.693298] [G loss: 0.716457]\n",
      "[Epoch 0/200] [Batch 1937/3166] [D loss: 0.693760] [G loss: 0.709110]\n",
      "[Epoch 0/200] [Batch 1938/3166] [D loss: 0.693872] [G loss: 0.708661]\n",
      "[Epoch 0/200] [Batch 1939/3166] [D loss: 0.700722] [G loss: 0.700289]\n",
      "[Epoch 0/200] [Batch 1940/3166] [D loss: 0.691012] [G loss: 0.697485]\n",
      "[Epoch 0/200] [Batch 1941/3166] [D loss: 0.694607] [G loss: 0.699502]\n",
      "[Epoch 0/200] [Batch 1942/3166] [D loss: 0.698334] [G loss: 0.694666]\n",
      "[Epoch 0/200] [Batch 1943/3166] [D loss: 0.695495] [G loss: 0.691876]\n",
      "[Epoch 0/200] [Batch 1944/3166] [D loss: 0.700262] [G loss: 0.693897]\n",
      "[Epoch 0/200] [Batch 1945/3166] [D loss: 0.699472] [G loss: 0.689149]\n",
      "[Epoch 0/200] [Batch 1946/3166] [D loss: 0.696782] [G loss: 0.687668]\n",
      "[Epoch 0/200] [Batch 1947/3166] [D loss: 0.699862] [G loss: 0.686166]\n",
      "[Epoch 0/200] [Batch 1948/3166] [D loss: 0.697583] [G loss: 0.690543]\n",
      "[Epoch 0/200] [Batch 1949/3166] [D loss: 0.701335] [G loss: 0.682414]\n",
      "[Epoch 0/200] [Batch 1950/3166] [D loss: 0.695897] [G loss: 0.688186]\n",
      "[Epoch 0/200] [Batch 1951/3166] [D loss: 0.699599] [G loss: 0.674756]\n",
      "[Epoch 0/200] [Batch 1952/3166] [D loss: 0.702238] [G loss: 0.680036]\n",
      "[Epoch 0/200] [Batch 1953/3166] [D loss: 0.693315] [G loss: 0.678918]\n",
      "[Epoch 0/200] [Batch 1954/3166] [D loss: 0.691985] [G loss: 0.689852]\n",
      "[Epoch 0/200] [Batch 1955/3166] [D loss: 0.695252] [G loss: 0.684722]\n",
      "[Epoch 0/200] [Batch 1956/3166] [D loss: 0.691781] [G loss: 0.687767]\n",
      "[Epoch 0/200] [Batch 1957/3166] [D loss: 0.688246] [G loss: 0.686753]\n",
      "[Epoch 0/200] [Batch 1958/3166] [D loss: 0.690890] [G loss: 0.691190]\n",
      "[Epoch 0/200] [Batch 1959/3166] [D loss: 0.689442] [G loss: 0.690217]\n",
      "[Epoch 0/200] [Batch 1960/3166] [D loss: 0.696559] [G loss: 0.682148]\n",
      "[Epoch 0/200] [Batch 1961/3166] [D loss: 0.695963] [G loss: 0.691003]\n",
      "[Epoch 0/200] [Batch 1962/3166] [D loss: 0.688079] [G loss: 0.686960]\n",
      "[Epoch 0/200] [Batch 1963/3166] [D loss: 0.695291] [G loss: 0.687093]\n",
      "[Epoch 0/200] [Batch 1964/3166] [D loss: 0.695409] [G loss: 0.685615]\n",
      "[Epoch 0/200] [Batch 1965/3166] [D loss: 0.692101] [G loss: 0.682821]\n",
      "[Epoch 0/200] [Batch 1966/3166] [D loss: 0.690297] [G loss: 0.682304]\n",
      "[Epoch 0/200] [Batch 1967/3166] [D loss: 0.696947] [G loss: 0.690781]\n",
      "[Epoch 0/200] [Batch 1968/3166] [D loss: 0.690927] [G loss: 0.680643]\n",
      "[Epoch 0/200] [Batch 1969/3166] [D loss: 0.703576] [G loss: 0.674380]\n",
      "[Epoch 0/200] [Batch 1970/3166] [D loss: 0.699435] [G loss: 0.667758]\n",
      "[Epoch 0/200] [Batch 1971/3166] [D loss: 0.695651] [G loss: 0.685180]\n",
      "[Epoch 0/200] [Batch 1972/3166] [D loss: 0.696137] [G loss: 0.682863]\n",
      "[Epoch 0/200] [Batch 1973/3166] [D loss: 0.693849] [G loss: 0.685994]\n",
      "[Epoch 0/200] [Batch 1974/3166] [D loss: 0.695825] [G loss: 0.684534]\n",
      "[Epoch 0/200] [Batch 1975/3166] [D loss: 0.706136] [G loss: 0.680571]\n",
      "[Epoch 0/200] [Batch 1976/3166] [D loss: 0.692798] [G loss: 0.691577]\n",
      "[Epoch 0/200] [Batch 1977/3166] [D loss: 0.691929] [G loss: 0.699709]\n",
      "[Epoch 0/200] [Batch 1978/3166] [D loss: 0.686014] [G loss: 0.695590]\n",
      "[Epoch 0/200] [Batch 1979/3166] [D loss: 0.703138] [G loss: 0.696509]\n",
      "[Epoch 0/200] [Batch 1980/3166] [D loss: 0.687010] [G loss: 0.701840]\n",
      "[Epoch 0/200] [Batch 1981/3166] [D loss: 0.702206] [G loss: 0.690118]\n",
      "[Epoch 0/200] [Batch 1982/3166] [D loss: 0.700492] [G loss: 0.692199]\n",
      "[Epoch 0/200] [Batch 1983/3166] [D loss: 0.693308] [G loss: 0.695947]\n",
      "[Epoch 0/200] [Batch 1984/3166] [D loss: 0.693272] [G loss: 0.692181]\n",
      "[Epoch 0/200] [Batch 1985/3166] [D loss: 0.696926] [G loss: 0.693170]\n",
      "[Epoch 0/200] [Batch 1986/3166] [D loss: 0.703320] [G loss: 0.698797]\n",
      "[Epoch 0/200] [Batch 1987/3166] [D loss: 0.695515] [G loss: 0.690140]\n",
      "[Epoch 0/200] [Batch 1988/3166] [D loss: 0.695041] [G loss: 0.693720]\n",
      "[Epoch 0/200] [Batch 1989/3166] [D loss: 0.695514] [G loss: 0.695161]\n",
      "[Epoch 0/200] [Batch 1990/3166] [D loss: 0.694221] [G loss: 0.702082]\n",
      "[Epoch 0/200] [Batch 1991/3166] [D loss: 0.694178] [G loss: 0.693503]\n",
      "[Epoch 0/200] [Batch 1992/3166] [D loss: 0.697076] [G loss: 0.699212]\n",
      "[Epoch 0/200] [Batch 1993/3166] [D loss: 0.694961] [G loss: 0.696398]\n",
      "[Epoch 0/200] [Batch 1994/3166] [D loss: 0.690675] [G loss: 0.697489]\n",
      "[Epoch 0/200] [Batch 1995/3166] [D loss: 0.692664] [G loss: 0.699276]\n",
      "[Epoch 0/200] [Batch 1996/3166] [D loss: 0.697973] [G loss: 0.699367]\n",
      "[Epoch 0/200] [Batch 1997/3166] [D loss: 0.694049] [G loss: 0.698735]\n",
      "[Epoch 0/200] [Batch 1998/3166] [D loss: 0.693110] [G loss: 0.697785]\n",
      "[Epoch 0/200] [Batch 1999/3166] [D loss: 0.693213] [G loss: 0.700970]\n",
      "[Epoch 0/200] [Batch 2000/3166] [D loss: 0.696218] [G loss: 0.700641]\n",
      "[Epoch 0/200] [Batch 2001/3166] [D loss: 0.692629] [G loss: 0.703772]\n",
      "[Epoch 0/200] [Batch 2002/3166] [D loss: 0.692435] [G loss: 0.695529]\n",
      "[Epoch 0/200] [Batch 2003/3166] [D loss: 0.692647] [G loss: 0.706447]\n",
      "[Epoch 0/200] [Batch 2004/3166] [D loss: 0.690825] [G loss: 0.701987]\n",
      "[Epoch 0/200] [Batch 2005/3166] [D loss: 0.694085] [G loss: 0.694191]\n",
      "[Epoch 0/200] [Batch 2006/3166] [D loss: 0.695414] [G loss: 0.698419]\n",
      "[Epoch 0/200] [Batch 2007/3166] [D loss: 0.690878] [G loss: 0.704458]\n",
      "[Epoch 0/200] [Batch 2008/3166] [D loss: 0.692105] [G loss: 0.701167]\n",
      "[Epoch 0/200] [Batch 2009/3166] [D loss: 0.684715] [G loss: 0.711563]\n",
      "[Epoch 0/200] [Batch 2010/3166] [D loss: 0.691605] [G loss: 0.706103]\n",
      "[Epoch 0/200] [Batch 2011/3166] [D loss: 0.691722] [G loss: 0.707394]\n",
      "[Epoch 0/200] [Batch 2012/3166] [D loss: 0.692686] [G loss: 0.711362]\n",
      "[Epoch 0/200] [Batch 2013/3166] [D loss: 0.694516] [G loss: 0.699454]\n",
      "[Epoch 0/200] [Batch 2014/3166] [D loss: 0.696000] [G loss: 0.703710]\n",
      "[Epoch 0/200] [Batch 2015/3166] [D loss: 0.695712] [G loss: 0.706230]\n",
      "[Epoch 0/200] [Batch 2016/3166] [D loss: 0.691978] [G loss: 0.705046]\n",
      "[Epoch 0/200] [Batch 2017/3166] [D loss: 0.687456] [G loss: 0.714040]\n",
      "[Epoch 0/200] [Batch 2018/3166] [D loss: 0.693635] [G loss: 0.709415]\n",
      "[Epoch 0/200] [Batch 2019/3166] [D loss: 0.693360] [G loss: 0.709757]\n",
      "[Epoch 0/200] [Batch 2020/3166] [D loss: 0.694158] [G loss: 0.706178]\n",
      "[Epoch 0/200] [Batch 2021/3166] [D loss: 0.696986] [G loss: 0.706529]\n",
      "[Epoch 0/200] [Batch 2022/3166] [D loss: 0.691992] [G loss: 0.704007]\n",
      "[Epoch 0/200] [Batch 2023/3166] [D loss: 0.693657] [G loss: 0.699135]\n",
      "[Epoch 0/200] [Batch 2024/3166] [D loss: 0.694443] [G loss: 0.703935]\n",
      "[Epoch 0/200] [Batch 2025/3166] [D loss: 0.691255] [G loss: 0.704424]\n",
      "[Epoch 0/200] [Batch 2026/3166] [D loss: 0.696572] [G loss: 0.706969]\n",
      "[Epoch 0/200] [Batch 2027/3166] [D loss: 0.695039] [G loss: 0.703561]\n",
      "[Epoch 0/200] [Batch 2028/3166] [D loss: 0.690884] [G loss: 0.698704]\n",
      "[Epoch 0/200] [Batch 2029/3166] [D loss: 0.689424] [G loss: 0.707223]\n",
      "[Epoch 0/200] [Batch 2030/3166] [D loss: 0.688177] [G loss: 0.708657]\n",
      "[Epoch 0/200] [Batch 2031/3166] [D loss: 0.691763] [G loss: 0.704208]\n",
      "[Epoch 0/200] [Batch 2032/3166] [D loss: 0.695391] [G loss: 0.703523]\n",
      "[Epoch 0/200] [Batch 2033/3166] [D loss: 0.691131] [G loss: 0.704527]\n",
      "[Epoch 0/200] [Batch 2034/3166] [D loss: 0.694207] [G loss: 0.704804]\n",
      "[Epoch 0/200] [Batch 2035/3166] [D loss: 0.684781] [G loss: 0.705507]\n",
      "[Epoch 0/200] [Batch 2036/3166] [D loss: 0.692743] [G loss: 0.696550]\n",
      "[Epoch 0/200] [Batch 2037/3166] [D loss: 0.691673] [G loss: 0.699877]\n",
      "[Epoch 0/200] [Batch 2038/3166] [D loss: 0.696321] [G loss: 0.693664]\n",
      "[Epoch 0/200] [Batch 2039/3166] [D loss: 0.696651] [G loss: 0.696936]\n",
      "[Epoch 0/200] [Batch 2040/3166] [D loss: 0.690360] [G loss: 0.700630]\n",
      "[Epoch 0/200] [Batch 2041/3166] [D loss: 0.693010] [G loss: 0.692765]\n",
      "[Epoch 0/200] [Batch 2042/3166] [D loss: 0.697941] [G loss: 0.691567]\n",
      "[Epoch 0/200] [Batch 2043/3166] [D loss: 0.693192] [G loss: 0.690337]\n",
      "[Epoch 0/200] [Batch 2044/3166] [D loss: 0.697815] [G loss: 0.683668]\n",
      "[Epoch 0/200] [Batch 2045/3166] [D loss: 0.692068] [G loss: 0.687677]\n",
      "[Epoch 0/200] [Batch 2046/3166] [D loss: 0.692019] [G loss: 0.696290]\n",
      "[Epoch 0/200] [Batch 2047/3166] [D loss: 0.691032] [G loss: 0.684251]\n",
      "[Epoch 0/200] [Batch 2048/3166] [D loss: 0.696421] [G loss: 0.685574]\n",
      "[Epoch 0/200] [Batch 2049/3166] [D loss: 0.690644] [G loss: 0.691779]\n",
      "[Epoch 0/200] [Batch 2050/3166] [D loss: 0.697654] [G loss: 0.683167]\n",
      "[Epoch 0/200] [Batch 2051/3166] [D loss: 0.694044] [G loss: 0.685168]\n",
      "[Epoch 0/200] [Batch 2052/3166] [D loss: 0.691983] [G loss: 0.690215]\n",
      "[Epoch 0/200] [Batch 2053/3166] [D loss: 0.694272] [G loss: 0.684613]\n",
      "[Epoch 0/200] [Batch 2054/3166] [D loss: 0.690940] [G loss: 0.684657]\n",
      "[Epoch 0/200] [Batch 2055/3166] [D loss: 0.696854] [G loss: 0.674925]\n",
      "[Epoch 0/200] [Batch 2056/3166] [D loss: 0.696354] [G loss: 0.683265]\n",
      "[Epoch 0/200] [Batch 2057/3166] [D loss: 0.692645] [G loss: 0.688743]\n",
      "[Epoch 0/200] [Batch 2058/3166] [D loss: 0.695920] [G loss: 0.680715]\n",
      "[Epoch 0/200] [Batch 2059/3166] [D loss: 0.694011] [G loss: 0.681356]\n",
      "[Epoch 0/200] [Batch 2060/3166] [D loss: 0.697212] [G loss: 0.684585]\n",
      "[Epoch 0/200] [Batch 2061/3166] [D loss: 0.694662] [G loss: 0.688040]\n",
      "[Epoch 0/200] [Batch 2062/3166] [D loss: 0.695399] [G loss: 0.684586]\n",
      "[Epoch 0/200] [Batch 2063/3166] [D loss: 0.700393] [G loss: 0.682609]\n",
      "[Epoch 0/200] [Batch 2064/3166] [D loss: 0.691683] [G loss: 0.683291]\n",
      "[Epoch 0/200] [Batch 2065/3166] [D loss: 0.698902] [G loss: 0.684188]\n",
      "[Epoch 0/200] [Batch 2066/3166] [D loss: 0.694598] [G loss: 0.683709]\n",
      "[Epoch 0/200] [Batch 2067/3166] [D loss: 0.701506] [G loss: 0.685410]\n",
      "[Epoch 0/200] [Batch 2068/3166] [D loss: 0.701328] [G loss: 0.678824]\n",
      "[Epoch 0/200] [Batch 2069/3166] [D loss: 0.697212] [G loss: 0.687510]\n",
      "[Epoch 0/200] [Batch 2070/3166] [D loss: 0.697956] [G loss: 0.684526]\n",
      "[Epoch 0/200] [Batch 2071/3166] [D loss: 0.697529] [G loss: 0.688356]\n",
      "[Epoch 0/200] [Batch 2072/3166] [D loss: 0.692196] [G loss: 0.688791]\n",
      "[Epoch 0/200] [Batch 2073/3166] [D loss: 0.694082] [G loss: 0.690689]\n",
      "[Epoch 0/200] [Batch 2074/3166] [D loss: 0.692454] [G loss: 0.693965]\n",
      "[Epoch 0/200] [Batch 2075/3166] [D loss: 0.696135] [G loss: 0.691150]\n",
      "[Epoch 0/200] [Batch 2076/3166] [D loss: 0.695068] [G loss: 0.695930]\n",
      "[Epoch 0/200] [Batch 2077/3166] [D loss: 0.690146] [G loss: 0.690279]\n",
      "[Epoch 0/200] [Batch 2078/3166] [D loss: 0.691483] [G loss: 0.695692]\n",
      "[Epoch 0/200] [Batch 2079/3166] [D loss: 0.690583] [G loss: 0.692843]\n",
      "[Epoch 0/200] [Batch 2080/3166] [D loss: 0.692058] [G loss: 0.693357]\n",
      "[Epoch 0/200] [Batch 2081/3166] [D loss: 0.688602] [G loss: 0.697416]\n",
      "[Epoch 0/200] [Batch 2082/3166] [D loss: 0.687802] [G loss: 0.694388]\n",
      "[Epoch 0/200] [Batch 2083/3166] [D loss: 0.691368] [G loss: 0.701106]\n",
      "[Epoch 0/200] [Batch 2084/3166] [D loss: 0.689510] [G loss: 0.697468]\n",
      "[Epoch 0/200] [Batch 2085/3166] [D loss: 0.689677] [G loss: 0.696443]\n",
      "[Epoch 0/200] [Batch 2086/3166] [D loss: 0.688826] [G loss: 0.697351]\n",
      "[Epoch 0/200] [Batch 2087/3166] [D loss: 0.687547] [G loss: 0.694658]\n",
      "[Epoch 0/200] [Batch 2088/3166] [D loss: 0.686132] [G loss: 0.694772]\n",
      "[Epoch 0/200] [Batch 2089/3166] [D loss: 0.687519] [G loss: 0.691812]\n",
      "[Epoch 0/200] [Batch 2090/3166] [D loss: 0.691583] [G loss: 0.696276]\n",
      "[Epoch 0/200] [Batch 2091/3166] [D loss: 0.685575] [G loss: 0.686551]\n",
      "[Epoch 0/200] [Batch 2092/3166] [D loss: 0.693401] [G loss: 0.688991]\n",
      "[Epoch 0/200] [Batch 2093/3166] [D loss: 0.693999] [G loss: 0.688396]\n",
      "[Epoch 0/200] [Batch 2094/3166] [D loss: 0.685041] [G loss: 0.688540]\n",
      "[Epoch 0/200] [Batch 2095/3166] [D loss: 0.690695] [G loss: 0.695117]\n",
      "[Epoch 0/200] [Batch 2096/3166] [D loss: 0.694779] [G loss: 0.694584]\n",
      "[Epoch 0/200] [Batch 2097/3166] [D loss: 0.694906] [G loss: 0.688844]\n",
      "[Epoch 0/200] [Batch 2098/3166] [D loss: 0.689542] [G loss: 0.692343]\n",
      "[Epoch 0/200] [Batch 2099/3166] [D loss: 0.690935] [G loss: 0.698261]\n",
      "[Epoch 0/200] [Batch 2100/3166] [D loss: 0.690847] [G loss: 0.695580]\n",
      "[Epoch 0/200] [Batch 2101/3166] [D loss: 0.690716] [G loss: 0.707403]\n",
      "[Epoch 0/200] [Batch 2102/3166] [D loss: 0.688089] [G loss: 0.699567]\n",
      "[Epoch 0/200] [Batch 2103/3166] [D loss: 0.689399] [G loss: 0.702685]\n",
      "[Epoch 0/200] [Batch 2104/3166] [D loss: 0.687886] [G loss: 0.698381]\n",
      "[Epoch 0/200] [Batch 2105/3166] [D loss: 0.691067] [G loss: 0.697128]\n",
      "[Epoch 0/200] [Batch 2106/3166] [D loss: 0.690201] [G loss: 0.700948]\n",
      "[Epoch 0/200] [Batch 2107/3166] [D loss: 0.687122] [G loss: 0.702958]\n",
      "[Epoch 0/200] [Batch 2108/3166] [D loss: 0.689217] [G loss: 0.700057]\n",
      "[Epoch 0/200] [Batch 2109/3166] [D loss: 0.689794] [G loss: 0.697453]\n",
      "[Epoch 0/200] [Batch 2110/3166] [D loss: 0.688644] [G loss: 0.697307]\n",
      "[Epoch 0/200] [Batch 2111/3166] [D loss: 0.695569] [G loss: 0.699359]\n",
      "[Epoch 0/200] [Batch 2112/3166] [D loss: 0.690799] [G loss: 0.702482]\n",
      "[Epoch 0/200] [Batch 2113/3166] [D loss: 0.690812] [G loss: 0.708044]\n",
      "[Epoch 0/200] [Batch 2114/3166] [D loss: 0.689789] [G loss: 0.704567]\n",
      "[Epoch 0/200] [Batch 2115/3166] [D loss: 0.689623] [G loss: 0.702700]\n",
      "[Epoch 0/200] [Batch 2116/3166] [D loss: 0.693360] [G loss: 0.703037]\n",
      "[Epoch 0/200] [Batch 2117/3166] [D loss: 0.689974] [G loss: 0.697889]\n",
      "[Epoch 0/200] [Batch 2118/3166] [D loss: 0.692104] [G loss: 0.695468]\n",
      "[Epoch 0/200] [Batch 2119/3166] [D loss: 0.694005] [G loss: 0.693319]\n",
      "[Epoch 0/200] [Batch 2120/3166] [D loss: 0.695744] [G loss: 0.692128]\n",
      "[Epoch 0/200] [Batch 2121/3166] [D loss: 0.696026] [G loss: 0.693713]\n",
      "[Epoch 0/200] [Batch 2122/3166] [D loss: 0.695927] [G loss: 0.692976]\n",
      "[Epoch 0/200] [Batch 2123/3166] [D loss: 0.701191] [G loss: 0.688470]\n",
      "[Epoch 0/200] [Batch 2124/3166] [D loss: 0.700970] [G loss: 0.690411]\n",
      "[Epoch 0/200] [Batch 2125/3166] [D loss: 0.696032] [G loss: 0.688398]\n",
      "[Epoch 0/200] [Batch 2126/3166] [D loss: 0.694465] [G loss: 0.694294]\n",
      "[Epoch 0/200] [Batch 2127/3166] [D loss: 0.694565] [G loss: 0.685361]\n",
      "[Epoch 0/200] [Batch 2128/3166] [D loss: 0.698497] [G loss: 0.688610]\n",
      "[Epoch 0/200] [Batch 2129/3166] [D loss: 0.694299] [G loss: 0.693508]\n",
      "[Epoch 0/200] [Batch 2130/3166] [D loss: 0.694086] [G loss: 0.686050]\n",
      "[Epoch 0/200] [Batch 2131/3166] [D loss: 0.695157] [G loss: 0.697176]\n",
      "[Epoch 0/200] [Batch 2132/3166] [D loss: 0.692038] [G loss: 0.695987]\n",
      "[Epoch 0/200] [Batch 2133/3166] [D loss: 0.687206] [G loss: 0.696648]\n",
      "[Epoch 0/200] [Batch 2134/3166] [D loss: 0.692065] [G loss: 0.696630]\n",
      "[Epoch 0/200] [Batch 2135/3166] [D loss: 0.687384] [G loss: 0.689076]\n",
      "[Epoch 0/200] [Batch 2136/3166] [D loss: 0.684739] [G loss: 0.707122]\n",
      "[Epoch 0/200] [Batch 2137/3166] [D loss: 0.684671] [G loss: 0.695189]\n",
      "[Epoch 0/200] [Batch 2138/3166] [D loss: 0.687579] [G loss: 0.682267]\n",
      "[Epoch 0/200] [Batch 2139/3166] [D loss: 0.688898] [G loss: 0.694148]\n",
      "[Epoch 0/200] [Batch 2140/3166] [D loss: 0.690893] [G loss: 0.687127]\n",
      "[Epoch 0/200] [Batch 2141/3166] [D loss: 0.695584] [G loss: 0.681807]\n",
      "[Epoch 0/200] [Batch 2142/3166] [D loss: 0.695383] [G loss: 0.684161]\n",
      "[Epoch 0/200] [Batch 2143/3166] [D loss: 0.697106] [G loss: 0.679898]\n",
      "[Epoch 0/200] [Batch 2144/3166] [D loss: 0.700646] [G loss: 0.688042]\n",
      "[Epoch 0/200] [Batch 2145/3166] [D loss: 0.690310] [G loss: 0.703932]\n",
      "[Epoch 0/200] [Batch 2146/3166] [D loss: 0.697055] [G loss: 0.713172]\n",
      "[Epoch 0/200] [Batch 2147/3166] [D loss: 0.696645] [G loss: 0.710156]\n",
      "[Epoch 0/200] [Batch 2148/3166] [D loss: 0.698797] [G loss: 0.710436]\n",
      "[Epoch 0/200] [Batch 2149/3166] [D loss: 0.693500] [G loss: 0.712777]\n",
      "[Epoch 0/200] [Batch 2150/3166] [D loss: 0.693363] [G loss: 0.709325]\n",
      "[Epoch 0/200] [Batch 2151/3166] [D loss: 0.693232] [G loss: 0.716567]\n",
      "[Epoch 0/200] [Batch 2152/3166] [D loss: 0.694350] [G loss: 0.718351]\n",
      "[Epoch 0/200] [Batch 2153/3166] [D loss: 0.693593] [G loss: 0.708438]\n",
      "[Epoch 0/200] [Batch 2154/3166] [D loss: 0.695587] [G loss: 0.709234]\n",
      "[Epoch 0/200] [Batch 2155/3166] [D loss: 0.697024] [G loss: 0.709907]\n",
      "[Epoch 0/200] [Batch 2156/3166] [D loss: 0.692762] [G loss: 0.711112]\n",
      "[Epoch 0/200] [Batch 2157/3166] [D loss: 0.697764] [G loss: 0.704283]\n",
      "[Epoch 0/200] [Batch 2158/3166] [D loss: 0.695207] [G loss: 0.701238]\n",
      "[Epoch 0/200] [Batch 2159/3166] [D loss: 0.691480] [G loss: 0.708360]\n",
      "[Epoch 0/200] [Batch 2160/3166] [D loss: 0.694300] [G loss: 0.702957]\n",
      "[Epoch 0/200] [Batch 2161/3166] [D loss: 0.693872] [G loss: 0.697222]\n",
      "[Epoch 0/200] [Batch 2162/3166] [D loss: 0.695408] [G loss: 0.691514]\n",
      "[Epoch 0/200] [Batch 2163/3166] [D loss: 0.695014] [G loss: 0.683955]\n",
      "[Epoch 0/200] [Batch 2164/3166] [D loss: 0.697876] [G loss: 0.689088]\n",
      "[Epoch 0/200] [Batch 2165/3166] [D loss: 0.694525] [G loss: 0.688172]\n",
      "[Epoch 0/200] [Batch 2166/3166] [D loss: 0.700154] [G loss: 0.686394]\n",
      "[Epoch 0/200] [Batch 2167/3166] [D loss: 0.698791] [G loss: 0.680916]\n",
      "[Epoch 0/200] [Batch 2168/3166] [D loss: 0.695910] [G loss: 0.684177]\n",
      "[Epoch 0/200] [Batch 2169/3166] [D loss: 0.701187] [G loss: 0.681452]\n",
      "[Epoch 0/200] [Batch 2170/3166] [D loss: 0.701170] [G loss: 0.684476]\n",
      "[Epoch 0/200] [Batch 2171/3166] [D loss: 0.694913] [G loss: 0.684407]\n",
      "[Epoch 0/200] [Batch 2172/3166] [D loss: 0.692488] [G loss: 0.684028]\n",
      "[Epoch 0/200] [Batch 2173/3166] [D loss: 0.695068] [G loss: 0.677148]\n",
      "[Epoch 0/200] [Batch 2174/3166] [D loss: 0.695981] [G loss: 0.675434]\n",
      "[Epoch 0/200] [Batch 2175/3166] [D loss: 0.692469] [G loss: 0.684103]\n",
      "[Epoch 0/200] [Batch 2176/3166] [D loss: 0.693972] [G loss: 0.681028]\n",
      "[Epoch 0/200] [Batch 2177/3166] [D loss: 0.688955] [G loss: 0.683780]\n",
      "[Epoch 0/200] [Batch 2178/3166] [D loss: 0.691069] [G loss: 0.681853]\n",
      "[Epoch 0/200] [Batch 2179/3166] [D loss: 0.689505] [G loss: 0.682948]\n",
      "[Epoch 0/200] [Batch 2180/3166] [D loss: 0.692781] [G loss: 0.680097]\n",
      "[Epoch 0/200] [Batch 2181/3166] [D loss: 0.691444] [G loss: 0.684207]\n",
      "[Epoch 0/200] [Batch 2182/3166] [D loss: 0.694949] [G loss: 0.676307]\n",
      "[Epoch 0/200] [Batch 2183/3166] [D loss: 0.698150] [G loss: 0.678035]\n",
      "[Epoch 0/200] [Batch 2184/3166] [D loss: 0.694353] [G loss: 0.673048]\n",
      "[Epoch 0/200] [Batch 2185/3166] [D loss: 0.695664] [G loss: 0.675461]\n",
      "[Epoch 0/200] [Batch 2186/3166] [D loss: 0.697389] [G loss: 0.683941]\n",
      "[Epoch 0/200] [Batch 2187/3166] [D loss: 0.691612] [G loss: 0.685737]\n",
      "[Epoch 0/200] [Batch 2188/3166] [D loss: 0.693398] [G loss: 0.679683]\n",
      "[Epoch 0/200] [Batch 2189/3166] [D loss: 0.692396] [G loss: 0.687732]\n",
      "[Epoch 0/200] [Batch 2190/3166] [D loss: 0.691505] [G loss: 0.688924]\n",
      "[Epoch 0/200] [Batch 2191/3166] [D loss: 0.692100] [G loss: 0.686838]\n",
      "[Epoch 0/200] [Batch 2192/3166] [D loss: 0.690455] [G loss: 0.692828]\n",
      "[Epoch 0/200] [Batch 2193/3166] [D loss: 0.688530] [G loss: 0.697350]\n",
      "[Epoch 0/200] [Batch 2194/3166] [D loss: 0.687297] [G loss: 0.699134]\n",
      "[Epoch 0/200] [Batch 2195/3166] [D loss: 0.687889] [G loss: 0.704611]\n",
      "[Epoch 0/200] [Batch 2196/3166] [D loss: 0.691947] [G loss: 0.710079]\n",
      "[Epoch 0/200] [Batch 2197/3166] [D loss: 0.692585] [G loss: 0.708770]\n",
      "[Epoch 0/200] [Batch 2198/3166] [D loss: 0.692206] [G loss: 0.701289]\n",
      "[Epoch 0/200] [Batch 2199/3166] [D loss: 0.694920] [G loss: 0.696969]\n",
      "[Epoch 0/200] [Batch 2200/3166] [D loss: 0.693391] [G loss: 0.700316]\n",
      "[Epoch 0/200] [Batch 2201/3166] [D loss: 0.693083] [G loss: 0.705859]\n",
      "[Epoch 0/200] [Batch 2202/3166] [D loss: 0.698898] [G loss: 0.696736]\n",
      "[Epoch 0/200] [Batch 2203/3166] [D loss: 0.700860] [G loss: 0.693426]\n",
      "[Epoch 0/200] [Batch 2204/3166] [D loss: 0.699147] [G loss: 0.688610]\n",
      "[Epoch 0/200] [Batch 2205/3166] [D loss: 0.695007] [G loss: 0.689502]\n",
      "[Epoch 0/200] [Batch 2206/3166] [D loss: 0.696468] [G loss: 0.691597]\n",
      "[Epoch 0/200] [Batch 2207/3166] [D loss: 0.694874] [G loss: 0.693089]\n",
      "[Epoch 0/200] [Batch 2208/3166] [D loss: 0.693451] [G loss: 0.692129]\n",
      "[Epoch 0/200] [Batch 2209/3166] [D loss: 0.694083] [G loss: 0.693183]\n",
      "[Epoch 0/200] [Batch 2210/3166] [D loss: 0.694656] [G loss: 0.698736]\n",
      "[Epoch 0/200] [Batch 2211/3166] [D loss: 0.693865] [G loss: 0.695269]\n",
      "[Epoch 0/200] [Batch 2212/3166] [D loss: 0.694101] [G loss: 0.696978]\n",
      "[Epoch 0/200] [Batch 2213/3166] [D loss: 0.694116] [G loss: 0.693179]\n",
      "[Epoch 0/200] [Batch 2214/3166] [D loss: 0.688843] [G loss: 0.698385]\n",
      "[Epoch 0/200] [Batch 2215/3166] [D loss: 0.689535] [G loss: 0.696801]\n",
      "[Epoch 0/200] [Batch 2216/3166] [D loss: 0.691438] [G loss: 0.692544]\n",
      "[Epoch 0/200] [Batch 2217/3166] [D loss: 0.691037] [G loss: 0.697450]\n",
      "[Epoch 0/200] [Batch 2218/3166] [D loss: 0.687340] [G loss: 0.698059]\n",
      "[Epoch 0/200] [Batch 2219/3166] [D loss: 0.689976] [G loss: 0.696955]\n",
      "[Epoch 0/200] [Batch 2220/3166] [D loss: 0.689370] [G loss: 0.697774]\n",
      "[Epoch 0/200] [Batch 2221/3166] [D loss: 0.689364] [G loss: 0.701036]\n",
      "[Epoch 0/200] [Batch 2222/3166] [D loss: 0.687762] [G loss: 0.697644]\n",
      "[Epoch 0/200] [Batch 2223/3166] [D loss: 0.689674] [G loss: 0.696798]\n",
      "[Epoch 0/200] [Batch 2224/3166] [D loss: 0.689313] [G loss: 0.700218]\n",
      "[Epoch 0/200] [Batch 2225/3166] [D loss: 0.685752] [G loss: 0.694798]\n",
      "[Epoch 0/200] [Batch 2226/3166] [D loss: 0.688731] [G loss: 0.695883]\n",
      "[Epoch 0/200] [Batch 2227/3166] [D loss: 0.690994] [G loss: 0.695974]\n",
      "[Epoch 0/200] [Batch 2228/3166] [D loss: 0.690183] [G loss: 0.702273]\n",
      "[Epoch 0/200] [Batch 2229/3166] [D loss: 0.690934] [G loss: 0.696094]\n",
      "[Epoch 0/200] [Batch 2230/3166] [D loss: 0.686878] [G loss: 0.693434]\n",
      "[Epoch 0/200] [Batch 2231/3166] [D loss: 0.688875] [G loss: 0.697651]\n",
      "[Epoch 0/200] [Batch 2232/3166] [D loss: 0.687596] [G loss: 0.697968]\n",
      "[Epoch 0/200] [Batch 2233/3166] [D loss: 0.690145] [G loss: 0.695194]\n",
      "[Epoch 0/200] [Batch 2234/3166] [D loss: 0.693325] [G loss: 0.700354]\n",
      "[Epoch 0/200] [Batch 2235/3166] [D loss: 0.692655] [G loss: 0.699731]\n",
      "[Epoch 0/200] [Batch 2236/3166] [D loss: 0.687710] [G loss: 0.693609]\n",
      "[Epoch 0/200] [Batch 2237/3166] [D loss: 0.691574] [G loss: 0.701245]\n",
      "[Epoch 0/200] [Batch 2238/3166] [D loss: 0.689566] [G loss: 0.703092]\n",
      "[Epoch 0/200] [Batch 2239/3166] [D loss: 0.690582] [G loss: 0.700648]\n",
      "[Epoch 0/200] [Batch 2240/3166] [D loss: 0.689323] [G loss: 0.697688]\n",
      "[Epoch 0/200] [Batch 2241/3166] [D loss: 0.696705] [G loss: 0.695420]\n",
      "[Epoch 0/200] [Batch 2242/3166] [D loss: 0.690333] [G loss: 0.698467]\n",
      "[Epoch 0/200] [Batch 2243/3166] [D loss: 0.691795] [G loss: 0.691062]\n",
      "[Epoch 0/200] [Batch 2244/3166] [D loss: 0.692960] [G loss: 0.696890]\n",
      "[Epoch 0/200] [Batch 2245/3166] [D loss: 0.697941] [G loss: 0.693369]\n",
      "[Epoch 0/200] [Batch 2246/3166] [D loss: 0.693829] [G loss: 0.696876]\n",
      "[Epoch 0/200] [Batch 2247/3166] [D loss: 0.695519] [G loss: 0.692326]\n",
      "[Epoch 0/200] [Batch 2248/3166] [D loss: 0.697034] [G loss: 0.692405]\n",
      "[Epoch 0/200] [Batch 2249/3166] [D loss: 0.693946] [G loss: 0.688143]\n",
      "[Epoch 0/200] [Batch 2250/3166] [D loss: 0.691967] [G loss: 0.692072]\n",
      "[Epoch 0/200] [Batch 2251/3166] [D loss: 0.697323] [G loss: 0.689188]\n",
      "[Epoch 0/200] [Batch 2252/3166] [D loss: 0.693544] [G loss: 0.692179]\n",
      "[Epoch 0/200] [Batch 2253/3166] [D loss: 0.690300] [G loss: 0.694712]\n",
      "[Epoch 0/200] [Batch 2254/3166] [D loss: 0.694325] [G loss: 0.691671]\n",
      "[Epoch 0/200] [Batch 2255/3166] [D loss: 0.695411] [G loss: 0.693931]\n",
      "[Epoch 0/200] [Batch 2256/3166] [D loss: 0.692857] [G loss: 0.692443]\n",
      "[Epoch 0/200] [Batch 2257/3166] [D loss: 0.693096] [G loss: 0.690952]\n",
      "[Epoch 0/200] [Batch 2258/3166] [D loss: 0.692987] [G loss: 0.695658]\n",
      "[Epoch 0/200] [Batch 2259/3166] [D loss: 0.692938] [G loss: 0.693982]\n",
      "[Epoch 0/200] [Batch 2260/3166] [D loss: 0.691957] [G loss: 0.694766]\n",
      "[Epoch 0/200] [Batch 2261/3166] [D loss: 0.691180] [G loss: 0.695639]\n",
      "[Epoch 0/200] [Batch 2262/3166] [D loss: 0.690940] [G loss: 0.695896]\n",
      "[Epoch 0/200] [Batch 2263/3166] [D loss: 0.691855] [G loss: 0.695596]\n",
      "[Epoch 0/200] [Batch 2264/3166] [D loss: 0.690050] [G loss: 0.698904]\n",
      "[Epoch 0/200] [Batch 2265/3166] [D loss: 0.691888] [G loss: 0.692288]\n",
      "[Epoch 0/200] [Batch 2266/3166] [D loss: 0.688123] [G loss: 0.696186]\n",
      "[Epoch 0/200] [Batch 2267/3166] [D loss: 0.689937] [G loss: 0.694649]\n",
      "[Epoch 0/200] [Batch 2268/3166] [D loss: 0.689579] [G loss: 0.689179]\n",
      "[Epoch 0/200] [Batch 2269/3166] [D loss: 0.690899] [G loss: 0.697497]\n",
      "[Epoch 0/200] [Batch 2270/3166] [D loss: 0.687710] [G loss: 0.695394]\n",
      "[Epoch 0/200] [Batch 2271/3166] [D loss: 0.689079] [G loss: 0.693900]\n",
      "[Epoch 0/200] [Batch 2272/3166] [D loss: 0.686146] [G loss: 0.699988]\n",
      "[Epoch 0/200] [Batch 2273/3166] [D loss: 0.689197] [G loss: 0.697959]\n",
      "[Epoch 0/200] [Batch 2274/3166] [D loss: 0.691944] [G loss: 0.692294]\n",
      "[Epoch 0/200] [Batch 2275/3166] [D loss: 0.691055] [G loss: 0.690291]\n",
      "[Epoch 0/200] [Batch 2276/3166] [D loss: 0.690339] [G loss: 0.694563]\n",
      "[Epoch 0/200] [Batch 2277/3166] [D loss: 0.695787] [G loss: 0.684879]\n",
      "[Epoch 0/200] [Batch 2278/3166] [D loss: 0.691917] [G loss: 0.686153]\n",
      "[Epoch 0/200] [Batch 2279/3166] [D loss: 0.693246] [G loss: 0.679839]\n",
      "[Epoch 0/200] [Batch 2280/3166] [D loss: 0.693658] [G loss: 0.687615]\n",
      "[Epoch 0/200] [Batch 2281/3166] [D loss: 0.703306] [G loss: 0.676446]\n",
      "[Epoch 0/200] [Batch 2282/3166] [D loss: 0.693851] [G loss: 0.689203]\n",
      "[Epoch 0/200] [Batch 2283/3166] [D loss: 0.698721] [G loss: 0.678675]\n",
      "[Epoch 0/200] [Batch 2284/3166] [D loss: 0.702690] [G loss: 0.676345]\n",
      "[Epoch 0/200] [Batch 2285/3166] [D loss: 0.694766] [G loss: 0.682992]\n",
      "[Epoch 0/200] [Batch 2286/3166] [D loss: 0.689494] [G loss: 0.687224]\n",
      "[Epoch 0/200] [Batch 2287/3166] [D loss: 0.692146] [G loss: 0.688274]\n",
      "[Epoch 0/200] [Batch 2288/3166] [D loss: 0.693447] [G loss: 0.681066]\n",
      "[Epoch 0/200] [Batch 2289/3166] [D loss: 0.696093] [G loss: 0.680390]\n",
      "[Epoch 0/200] [Batch 2290/3166] [D loss: 0.690659] [G loss: 0.684950]\n",
      "[Epoch 0/200] [Batch 2291/3166] [D loss: 0.695122] [G loss: 0.680542]\n",
      "[Epoch 0/200] [Batch 2292/3166] [D loss: 0.692332] [G loss: 0.682361]\n",
      "[Epoch 0/200] [Batch 2293/3166] [D loss: 0.692249] [G loss: 0.683076]\n",
      "[Epoch 0/200] [Batch 2294/3166] [D loss: 0.694667] [G loss: 0.674672]\n",
      "[Epoch 0/200] [Batch 2295/3166] [D loss: 0.687336] [G loss: 0.682505]\n",
      "[Epoch 0/200] [Batch 2296/3166] [D loss: 0.696113] [G loss: 0.672616]\n",
      "[Epoch 0/200] [Batch 2297/3166] [D loss: 0.699181] [G loss: 0.663992]\n",
      "[Epoch 0/200] [Batch 2298/3166] [D loss: 0.696127] [G loss: 0.674570]\n",
      "[Epoch 0/200] [Batch 2299/3166] [D loss: 0.699462] [G loss: 0.675434]\n",
      "[Epoch 0/200] [Batch 2300/3166] [D loss: 0.704337] [G loss: 0.679470]\n",
      "[Epoch 0/200] [Batch 2301/3166] [D loss: 0.696950] [G loss: 0.680184]\n",
      "[Epoch 0/200] [Batch 2302/3166] [D loss: 0.703088] [G loss: 0.671679]\n",
      "[Epoch 0/200] [Batch 2303/3166] [D loss: 0.697586] [G loss: 0.688127]\n",
      "[Epoch 0/200] [Batch 2304/3166] [D loss: 0.692187] [G loss: 0.688738]\n",
      "[Epoch 0/200] [Batch 2305/3166] [D loss: 0.700372] [G loss: 0.693950]\n",
      "[Epoch 0/200] [Batch 2306/3166] [D loss: 0.690449] [G loss: 0.701926]\n",
      "[Epoch 0/200] [Batch 2307/3166] [D loss: 0.696915] [G loss: 0.700517]\n",
      "[Epoch 0/200] [Batch 2308/3166] [D loss: 0.690884] [G loss: 0.701273]\n",
      "[Epoch 0/200] [Batch 2309/3166] [D loss: 0.694932] [G loss: 0.694502]\n",
      "[Epoch 0/200] [Batch 2310/3166] [D loss: 0.694462] [G loss: 0.703241]\n",
      "[Epoch 0/200] [Batch 2311/3166] [D loss: 0.696092] [G loss: 0.703011]\n",
      "[Epoch 0/200] [Batch 2312/3166] [D loss: 0.698374] [G loss: 0.698358]\n",
      "[Epoch 0/200] [Batch 2313/3166] [D loss: 0.692590] [G loss: 0.704329]\n",
      "[Epoch 0/200] [Batch 2314/3166] [D loss: 0.696872] [G loss: 0.697793]\n",
      "[Epoch 0/200] [Batch 2315/3166] [D loss: 0.697603] [G loss: 0.702115]\n",
      "[Epoch 0/200] [Batch 2316/3166] [D loss: 0.693647] [G loss: 0.704017]\n",
      "[Epoch 0/200] [Batch 2317/3166] [D loss: 0.694970] [G loss: 0.702714]\n",
      "[Epoch 0/200] [Batch 2318/3166] [D loss: 0.696980] [G loss: 0.701625]\n",
      "[Epoch 0/200] [Batch 2319/3166] [D loss: 0.694750] [G loss: 0.705817]\n",
      "[Epoch 0/200] [Batch 2320/3166] [D loss: 0.696017] [G loss: 0.704564]\n",
      "[Epoch 0/200] [Batch 2321/3166] [D loss: 0.693135] [G loss: 0.705538]\n",
      "[Epoch 0/200] [Batch 2322/3166] [D loss: 0.694878] [G loss: 0.711039]\n",
      "[Epoch 0/200] [Batch 2323/3166] [D loss: 0.691710] [G loss: 0.703538]\n",
      "[Epoch 0/200] [Batch 2324/3166] [D loss: 0.691019] [G loss: 0.706291]\n",
      "[Epoch 0/200] [Batch 2325/3166] [D loss: 0.694470] [G loss: 0.698590]\n",
      "[Epoch 0/200] [Batch 2326/3166] [D loss: 0.693247] [G loss: 0.705015]\n",
      "[Epoch 0/200] [Batch 2327/3166] [D loss: 0.695982] [G loss: 0.702752]\n",
      "[Epoch 0/200] [Batch 2328/3166] [D loss: 0.689547] [G loss: 0.705104]\n",
      "[Epoch 0/200] [Batch 2329/3166] [D loss: 0.692507] [G loss: 0.702749]\n",
      "[Epoch 0/200] [Batch 2330/3166] [D loss: 0.690628] [G loss: 0.706393]\n",
      "[Epoch 0/200] [Batch 2331/3166] [D loss: 0.696259] [G loss: 0.704629]\n",
      "[Epoch 0/200] [Batch 2332/3166] [D loss: 0.695964] [G loss: 0.703738]\n",
      "[Epoch 0/200] [Batch 2333/3166] [D loss: 0.692839] [G loss: 0.700964]\n",
      "[Epoch 0/200] [Batch 2334/3166] [D loss: 0.694845] [G loss: 0.695387]\n",
      "[Epoch 0/200] [Batch 2335/3166] [D loss: 0.691974] [G loss: 0.700646]\n",
      "[Epoch 0/200] [Batch 2336/3166] [D loss: 0.688166] [G loss: 0.712750]\n",
      "[Epoch 0/200] [Batch 2337/3166] [D loss: 0.692098] [G loss: 0.700085]\n",
      "[Epoch 0/200] [Batch 2338/3166] [D loss: 0.693844] [G loss: 0.696508]\n",
      "[Epoch 0/200] [Batch 2339/3166] [D loss: 0.694307] [G loss: 0.703294]\n",
      "[Epoch 0/200] [Batch 2340/3166] [D loss: 0.691969] [G loss: 0.703834]\n",
      "[Epoch 0/200] [Batch 2341/3166] [D loss: 0.693487] [G loss: 0.707239]\n",
      "[Epoch 0/200] [Batch 2342/3166] [D loss: 0.687951] [G loss: 0.703918]\n",
      "[Epoch 0/200] [Batch 2343/3166] [D loss: 0.688944] [G loss: 0.698892]\n",
      "[Epoch 0/200] [Batch 2344/3166] [D loss: 0.689861] [G loss: 0.697110]\n",
      "[Epoch 0/200] [Batch 2345/3166] [D loss: 0.689239] [G loss: 0.699853]\n",
      "[Epoch 0/200] [Batch 2346/3166] [D loss: 0.690409] [G loss: 0.696805]\n",
      "[Epoch 0/200] [Batch 2347/3166] [D loss: 0.694708] [G loss: 0.703471]\n",
      "[Epoch 0/200] [Batch 2348/3166] [D loss: 0.691408] [G loss: 0.695770]\n",
      "[Epoch 0/200] [Batch 2349/3166] [D loss: 0.691284] [G loss: 0.694191]\n",
      "[Epoch 0/200] [Batch 2350/3166] [D loss: 0.695509] [G loss: 0.697129]\n",
      "[Epoch 0/200] [Batch 2351/3166] [D loss: 0.690888] [G loss: 0.702258]\n",
      "[Epoch 0/200] [Batch 2352/3166] [D loss: 0.690707] [G loss: 0.699634]\n",
      "[Epoch 0/200] [Batch 2353/3166] [D loss: 0.693612] [G loss: 0.698441]\n",
      "[Epoch 0/200] [Batch 2354/3166] [D loss: 0.693855] [G loss: 0.700568]\n",
      "[Epoch 0/200] [Batch 2355/3166] [D loss: 0.690114] [G loss: 0.700382]\n",
      "[Epoch 0/200] [Batch 2356/3166] [D loss: 0.689083] [G loss: 0.700425]\n",
      "[Epoch 0/200] [Batch 2357/3166] [D loss: 0.692275] [G loss: 0.692382]\n",
      "[Epoch 0/200] [Batch 2358/3166] [D loss: 0.691043] [G loss: 0.703433]\n",
      "[Epoch 0/200] [Batch 2359/3166] [D loss: 0.694645] [G loss: 0.705791]\n",
      "[Epoch 0/200] [Batch 2360/3166] [D loss: 0.688446] [G loss: 0.697737]\n",
      "[Epoch 0/200] [Batch 2361/3166] [D loss: 0.693650] [G loss: 0.699785]\n",
      "[Epoch 0/200] [Batch 2362/3166] [D loss: 0.689614] [G loss: 0.705422]\n",
      "[Epoch 0/200] [Batch 2363/3166] [D loss: 0.693446] [G loss: 0.698815]\n",
      "[Epoch 0/200] [Batch 2364/3166] [D loss: 0.692939] [G loss: 0.699875]\n",
      "[Epoch 0/200] [Batch 2365/3166] [D loss: 0.690891] [G loss: 0.706305]\n",
      "[Epoch 0/200] [Batch 2366/3166] [D loss: 0.688817] [G loss: 0.706957]\n",
      "[Epoch 0/200] [Batch 2367/3166] [D loss: 0.694128] [G loss: 0.702515]\n",
      "[Epoch 0/200] [Batch 2368/3166] [D loss: 0.691652] [G loss: 0.708603]\n",
      "[Epoch 0/200] [Batch 2369/3166] [D loss: 0.691151] [G loss: 0.697029]\n",
      "[Epoch 0/200] [Batch 2370/3166] [D loss: 0.689188] [G loss: 0.708832]\n",
      "[Epoch 0/200] [Batch 2371/3166] [D loss: 0.693839] [G loss: 0.708327]\n",
      "[Epoch 0/200] [Batch 2372/3166] [D loss: 0.683687] [G loss: 0.724006]\n",
      "[Epoch 0/200] [Batch 2373/3166] [D loss: 0.683374] [G loss: 0.711323]\n",
      "[Epoch 0/200] [Batch 2374/3166] [D loss: 0.693263] [G loss: 0.699027]\n",
      "[Epoch 0/200] [Batch 2375/3166] [D loss: 0.692491] [G loss: 0.700266]\n",
      "[Epoch 0/200] [Batch 2376/3166] [D loss: 0.704881] [G loss: 0.680480]\n",
      "[Epoch 0/200] [Batch 2377/3166] [D loss: 0.696687] [G loss: 0.701102]\n",
      "[Epoch 0/200] [Batch 2378/3166] [D loss: 0.698562] [G loss: 0.686624]\n",
      "[Epoch 0/200] [Batch 2379/3166] [D loss: 0.698820] [G loss: 0.689475]\n",
      "[Epoch 0/200] [Batch 2380/3166] [D loss: 0.706292] [G loss: 0.682062]\n",
      "[Epoch 0/200] [Batch 2381/3166] [D loss: 0.696546] [G loss: 0.687569]\n",
      "[Epoch 0/200] [Batch 2382/3166] [D loss: 0.695219] [G loss: 0.692701]\n",
      "[Epoch 0/200] [Batch 2383/3166] [D loss: 0.698291] [G loss: 0.685867]\n",
      "[Epoch 0/200] [Batch 2384/3166] [D loss: 0.693739] [G loss: 0.688035]\n",
      "[Epoch 0/200] [Batch 2385/3166] [D loss: 0.695086] [G loss: 0.687437]\n",
      "[Epoch 0/200] [Batch 2386/3166] [D loss: 0.692348] [G loss: 0.687531]\n",
      "[Epoch 0/200] [Batch 2387/3166] [D loss: 0.697487] [G loss: 0.688812]\n",
      "[Epoch 0/200] [Batch 2388/3166] [D loss: 0.694308] [G loss: 0.690795]\n",
      "[Epoch 0/200] [Batch 2389/3166] [D loss: 0.691048] [G loss: 0.693703]\n",
      "[Epoch 0/200] [Batch 2390/3166] [D loss: 0.689712] [G loss: 0.692625]\n",
      "[Epoch 0/200] [Batch 2391/3166] [D loss: 0.695236] [G loss: 0.689847]\n",
      "[Epoch 0/200] [Batch 2392/3166] [D loss: 0.689541] [G loss: 0.693910]\n",
      "[Epoch 0/200] [Batch 2393/3166] [D loss: 0.692618] [G loss: 0.695448]\n",
      "[Epoch 0/200] [Batch 2394/3166] [D loss: 0.691630] [G loss: 0.694319]\n",
      "[Epoch 0/200] [Batch 2395/3166] [D loss: 0.690585] [G loss: 0.689393]\n",
      "[Epoch 0/200] [Batch 2396/3166] [D loss: 0.688136] [G loss: 0.692372]\n",
      "[Epoch 0/200] [Batch 2397/3166] [D loss: 0.691426] [G loss: 0.677686]\n",
      "[Epoch 0/200] [Batch 2398/3166] [D loss: 0.692720] [G loss: 0.684128]\n",
      "[Epoch 0/200] [Batch 2399/3166] [D loss: 0.691920] [G loss: 0.687586]\n",
      "[Epoch 0/200] [Batch 2400/3166] [D loss: 0.692890] [G loss: 0.688690]\n",
      "[Epoch 0/200] [Batch 2401/3166] [D loss: 0.694054] [G loss: 0.684522]\n",
      "[Epoch 0/200] [Batch 2402/3166] [D loss: 0.689627] [G loss: 0.687488]\n",
      "[Epoch 0/200] [Batch 2403/3166] [D loss: 0.686040] [G loss: 0.687742]\n",
      "[Epoch 0/200] [Batch 2404/3166] [D loss: 0.690525] [G loss: 0.686712]\n",
      "[Epoch 0/200] [Batch 2405/3166] [D loss: 0.690285] [G loss: 0.692963]\n",
      "[Epoch 0/200] [Batch 2406/3166] [D loss: 0.685652] [G loss: 0.691797]\n",
      "[Epoch 0/200] [Batch 2407/3166] [D loss: 0.689708] [G loss: 0.696892]\n",
      "[Epoch 0/200] [Batch 2408/3166] [D loss: 0.688889] [G loss: 0.692819]\n",
      "[Epoch 0/200] [Batch 2409/3166] [D loss: 0.689232] [G loss: 0.696473]\n",
      "[Epoch 0/200] [Batch 2410/3166] [D loss: 0.689538] [G loss: 0.692833]\n",
      "[Epoch 0/200] [Batch 2411/3166] [D loss: 0.688173] [G loss: 0.693287]\n",
      "[Epoch 0/200] [Batch 2412/3166] [D loss: 0.685613] [G loss: 0.695512]\n",
      "[Epoch 0/200] [Batch 2413/3166] [D loss: 0.685897] [G loss: 0.702763]\n",
      "[Epoch 0/200] [Batch 2414/3166] [D loss: 0.680363] [G loss: 0.695803]\n",
      "[Epoch 0/200] [Batch 2415/3166] [D loss: 0.681243] [G loss: 0.695706]\n",
      "[Epoch 0/200] [Batch 2416/3166] [D loss: 0.679833] [G loss: 0.699479]\n",
      "[Epoch 0/200] [Batch 2417/3166] [D loss: 0.687129] [G loss: 0.695040]\n",
      "[Epoch 0/200] [Batch 2418/3166] [D loss: 0.686545] [G loss: 0.692577]\n",
      "[Epoch 0/200] [Batch 2419/3166] [D loss: 0.679816] [G loss: 0.689343]\n",
      "[Epoch 0/200] [Batch 2420/3166] [D loss: 0.680255] [G loss: 0.686342]\n",
      "[Epoch 0/200] [Batch 2421/3166] [D loss: 0.688186] [G loss: 0.677577]\n",
      "[Epoch 0/200] [Batch 2422/3166] [D loss: 0.692698] [G loss: 0.672947]\n",
      "[Epoch 0/200] [Batch 2423/3166] [D loss: 0.691472] [G loss: 0.675901]\n",
      "[Epoch 0/200] [Batch 2424/3166] [D loss: 0.709024] [G loss: 0.658106]\n",
      "[Epoch 0/200] [Batch 2425/3166] [D loss: 0.700247] [G loss: 0.659777]\n",
      "[Epoch 0/200] [Batch 2426/3166] [D loss: 0.691895] [G loss: 0.674787]\n",
      "[Epoch 0/200] [Batch 2427/3166] [D loss: 0.695670] [G loss: 0.666553]\n",
      "[Epoch 0/200] [Batch 2428/3166] [D loss: 0.697612] [G loss: 0.672359]\n",
      "[Epoch 0/200] [Batch 2429/3166] [D loss: 0.709162] [G loss: 0.665294]\n",
      "[Epoch 0/200] [Batch 2430/3166] [D loss: 0.701718] [G loss: 0.669640]\n",
      "[Epoch 0/200] [Batch 2431/3166] [D loss: 0.700897] [G loss: 0.671158]\n",
      "[Epoch 0/200] [Batch 2432/3166] [D loss: 0.702945] [G loss: 0.670555]\n",
      "[Epoch 0/200] [Batch 2433/3166] [D loss: 0.697924] [G loss: 0.673157]\n",
      "[Epoch 0/200] [Batch 2434/3166] [D loss: 0.700141] [G loss: 0.681122]\n",
      "[Epoch 0/200] [Batch 2435/3166] [D loss: 0.694138] [G loss: 0.681569]\n",
      "[Epoch 0/200] [Batch 2436/3166] [D loss: 0.695638] [G loss: 0.684288]\n",
      "[Epoch 0/200] [Batch 2437/3166] [D loss: 0.702057] [G loss: 0.684361]\n",
      "[Epoch 0/200] [Batch 2438/3166] [D loss: 0.691891] [G loss: 0.692459]\n",
      "[Epoch 0/200] [Batch 2439/3166] [D loss: 0.693779] [G loss: 0.687958]\n",
      "[Epoch 0/200] [Batch 2440/3166] [D loss: 0.694154] [G loss: 0.687415]\n",
      "[Epoch 0/200] [Batch 2441/3166] [D loss: 0.693635] [G loss: 0.696407]\n",
      "[Epoch 0/200] [Batch 2442/3166] [D loss: 0.689843] [G loss: 0.694882]\n",
      "[Epoch 0/200] [Batch 2443/3166] [D loss: 0.692301] [G loss: 0.691365]\n",
      "[Epoch 0/200] [Batch 2444/3166] [D loss: 0.692697] [G loss: 0.693026]\n",
      "[Epoch 0/200] [Batch 2445/3166] [D loss: 0.694814] [G loss: 0.692392]\n",
      "[Epoch 0/200] [Batch 2446/3166] [D loss: 0.697245] [G loss: 0.687762]\n",
      "[Epoch 0/200] [Batch 2447/3166] [D loss: 0.691962] [G loss: 0.700126]\n",
      "[Epoch 0/200] [Batch 2448/3166] [D loss: 0.702366] [G loss: 0.692151]\n",
      "[Epoch 0/200] [Batch 2449/3166] [D loss: 0.696240] [G loss: 0.693697]\n",
      "[Epoch 0/200] [Batch 2450/3166] [D loss: 0.692015] [G loss: 0.699306]\n",
      "[Epoch 0/200] [Batch 2451/3166] [D loss: 0.689376] [G loss: 0.707713]\n",
      "[Epoch 0/200] [Batch 2452/3166] [D loss: 0.692669] [G loss: 0.701163]\n",
      "[Epoch 0/200] [Batch 2453/3166] [D loss: 0.693204] [G loss: 0.701958]\n",
      "[Epoch 0/200] [Batch 2454/3166] [D loss: 0.702560] [G loss: 0.690643]\n",
      "[Epoch 0/200] [Batch 2455/3166] [D loss: 0.700469] [G loss: 0.692667]\n",
      "[Epoch 0/200] [Batch 2456/3166] [D loss: 0.696798] [G loss: 0.691524]\n",
      "[Epoch 0/200] [Batch 2457/3166] [D loss: 0.699653] [G loss: 0.685455]\n",
      "[Epoch 0/200] [Batch 2458/3166] [D loss: 0.698952] [G loss: 0.693196]\n",
      "[Epoch 0/200] [Batch 2459/3166] [D loss: 0.696124] [G loss: 0.697281]\n",
      "[Epoch 0/200] [Batch 2460/3166] [D loss: 0.695772] [G loss: 0.704001]\n",
      "[Epoch 0/200] [Batch 2461/3166] [D loss: 0.697338] [G loss: 0.701481]\n",
      "[Epoch 0/200] [Batch 2462/3166] [D loss: 0.694425] [G loss: 0.704546]\n",
      "[Epoch 0/200] [Batch 2463/3166] [D loss: 0.694495] [G loss: 0.704189]\n",
      "[Epoch 0/200] [Batch 2464/3166] [D loss: 0.689933] [G loss: 0.713153]\n",
      "[Epoch 0/200] [Batch 2465/3166] [D loss: 0.690116] [G loss: 0.714866]\n",
      "[Epoch 0/200] [Batch 2466/3166] [D loss: 0.688718] [G loss: 0.711441]\n",
      "[Epoch 0/200] [Batch 2467/3166] [D loss: 0.689396] [G loss: 0.724999]\n",
      "[Epoch 0/200] [Batch 2468/3166] [D loss: 0.687651] [G loss: 0.723836]\n",
      "[Epoch 0/200] [Batch 2469/3166] [D loss: 0.689207] [G loss: 0.717879]\n",
      "[Epoch 0/200] [Batch 2470/3166] [D loss: 0.684119] [G loss: 0.724869]\n",
      "[Epoch 0/200] [Batch 2471/3166] [D loss: 0.681421] [G loss: 0.728426]\n",
      "[Epoch 0/200] [Batch 2472/3166] [D loss: 0.681776] [G loss: 0.733851]\n",
      "[Epoch 0/200] [Batch 2473/3166] [D loss: 0.684237] [G loss: 0.728875]\n",
      "[Epoch 0/200] [Batch 2474/3166] [D loss: 0.683487] [G loss: 0.733491]\n",
      "[Epoch 0/200] [Batch 2475/3166] [D loss: 0.683716] [G loss: 0.731492]\n",
      "[Epoch 0/200] [Batch 2476/3166] [D loss: 0.681826] [G loss: 0.726734]\n",
      "[Epoch 0/200] [Batch 2477/3166] [D loss: 0.688994] [G loss: 0.712910]\n",
      "[Epoch 0/200] [Batch 2478/3166] [D loss: 0.682419] [G loss: 0.707304]\n",
      "[Epoch 0/200] [Batch 2479/3166] [D loss: 0.691302] [G loss: 0.713994]\n",
      "[Epoch 0/200] [Batch 2480/3166] [D loss: 0.698748] [G loss: 0.707862]\n",
      "[Epoch 0/200] [Batch 2481/3166] [D loss: 0.694566] [G loss: 0.705034]\n",
      "[Epoch 0/200] [Batch 2482/3166] [D loss: 0.694082] [G loss: 0.704457]\n",
      "[Epoch 0/200] [Batch 2483/3166] [D loss: 0.690795] [G loss: 0.707376]\n",
      "[Epoch 0/200] [Batch 2484/3166] [D loss: 0.692047] [G loss: 0.713533]\n",
      "[Epoch 0/200] [Batch 2485/3166] [D loss: 0.686406] [G loss: 0.709320]\n",
      "[Epoch 0/200] [Batch 2486/3166] [D loss: 0.696773] [G loss: 0.705082]\n",
      "[Epoch 0/200] [Batch 2487/3166] [D loss: 0.692041] [G loss: 0.706052]\n",
      "[Epoch 0/200] [Batch 2488/3166] [D loss: 0.692116] [G loss: 0.711469]\n",
      "[Epoch 0/200] [Batch 2489/3166] [D loss: 0.705097] [G loss: 0.685912]\n",
      "[Epoch 0/200] [Batch 2490/3166] [D loss: 0.708435] [G loss: 0.695833]\n",
      "[Epoch 0/200] [Batch 2491/3166] [D loss: 0.702015] [G loss: 0.683370]\n",
      "[Epoch 0/200] [Batch 2492/3166] [D loss: 0.701376] [G loss: 0.682940]\n",
      "[Epoch 0/200] [Batch 2493/3166] [D loss: 0.705220] [G loss: 0.674027]\n",
      "[Epoch 0/200] [Batch 2494/3166] [D loss: 0.704789] [G loss: 0.684641]\n",
      "[Epoch 0/200] [Batch 2495/3166] [D loss: 0.703725] [G loss: 0.676334]\n",
      "[Epoch 0/200] [Batch 2496/3166] [D loss: 0.700895] [G loss: 0.684529]\n",
      "[Epoch 0/200] [Batch 2497/3166] [D loss: 0.700376] [G loss: 0.689845]\n",
      "[Epoch 0/200] [Batch 2498/3166] [D loss: 0.699522] [G loss: 0.692015]\n",
      "[Epoch 0/200] [Batch 2499/3166] [D loss: 0.695868] [G loss: 0.688132]\n",
      "[Epoch 0/200] [Batch 2500/3166] [D loss: 0.695468] [G loss: 0.693491]\n",
      "[Epoch 0/200] [Batch 2501/3166] [D loss: 0.693905] [G loss: 0.700479]\n",
      "[Epoch 0/200] [Batch 2502/3166] [D loss: 0.695137] [G loss: 0.690416]\n",
      "[Epoch 0/200] [Batch 2503/3166] [D loss: 0.690693] [G loss: 0.703041]\n",
      "[Epoch 0/200] [Batch 2504/3166] [D loss: 0.689482] [G loss: 0.698609]\n",
      "[Epoch 0/200] [Batch 2505/3166] [D loss: 0.690096] [G loss: 0.699361]\n",
      "[Epoch 0/200] [Batch 2506/3166] [D loss: 0.690485] [G loss: 0.699533]\n",
      "[Epoch 0/200] [Batch 2507/3166] [D loss: 0.689875] [G loss: 0.701228]\n",
      "[Epoch 0/200] [Batch 2508/3166] [D loss: 0.690760] [G loss: 0.699041]\n",
      "[Epoch 0/200] [Batch 2509/3166] [D loss: 0.691250] [G loss: 0.697128]\n",
      "[Epoch 0/200] [Batch 2510/3166] [D loss: 0.684954] [G loss: 0.705482]\n",
      "[Epoch 0/200] [Batch 2511/3166] [D loss: 0.692303] [G loss: 0.698302]\n",
      "[Epoch 0/200] [Batch 2512/3166] [D loss: 0.692492] [G loss: 0.690903]\n",
      "[Epoch 0/200] [Batch 2513/3166] [D loss: 0.686177] [G loss: 0.700540]\n",
      "[Epoch 0/200] [Batch 2514/3166] [D loss: 0.687352] [G loss: 0.697265]\n",
      "[Epoch 0/200] [Batch 2515/3166] [D loss: 0.688478] [G loss: 0.697026]\n",
      "[Epoch 0/200] [Batch 2516/3166] [D loss: 0.688999] [G loss: 0.694716]\n",
      "[Epoch 0/200] [Batch 2517/3166] [D loss: 0.684953] [G loss: 0.698655]\n",
      "[Epoch 0/200] [Batch 2518/3166] [D loss: 0.692451] [G loss: 0.689734]\n",
      "[Epoch 0/200] [Batch 2519/3166] [D loss: 0.700127] [G loss: 0.678890]\n",
      "[Epoch 0/200] [Batch 2520/3166] [D loss: 0.694836] [G loss: 0.681316]\n",
      "[Epoch 0/200] [Batch 2521/3166] [D loss: 0.697511] [G loss: 0.687719]\n",
      "[Epoch 0/200] [Batch 2522/3166] [D loss: 0.693277] [G loss: 0.690727]\n",
      "[Epoch 0/200] [Batch 2523/3166] [D loss: 0.692303] [G loss: 0.686378]\n",
      "[Epoch 0/200] [Batch 2524/3166] [D loss: 0.692680] [G loss: 0.683666]\n",
      "[Epoch 0/200] [Batch 2525/3166] [D loss: 0.689968] [G loss: 0.690180]\n",
      "[Epoch 0/200] [Batch 2526/3166] [D loss: 0.694481] [G loss: 0.688666]\n",
      "[Epoch 0/200] [Batch 2527/3166] [D loss: 0.692097] [G loss: 0.690093]\n",
      "[Epoch 0/200] [Batch 2528/3166] [D loss: 0.689718] [G loss: 0.688266]\n",
      "[Epoch 0/200] [Batch 2529/3166] [D loss: 0.693021] [G loss: 0.686144]\n",
      "[Epoch 0/200] [Batch 2530/3166] [D loss: 0.689328] [G loss: 0.690812]\n",
      "[Epoch 0/200] [Batch 2531/3166] [D loss: 0.695265] [G loss: 0.682755]\n",
      "[Epoch 0/200] [Batch 2532/3166] [D loss: 0.693430] [G loss: 0.684096]\n",
      "[Epoch 0/200] [Batch 2533/3166] [D loss: 0.693979] [G loss: 0.689903]\n",
      "[Epoch 0/200] [Batch 2534/3166] [D loss: 0.693805] [G loss: 0.685645]\n",
      "[Epoch 0/200] [Batch 2535/3166] [D loss: 0.692584] [G loss: 0.690115]\n",
      "[Epoch 0/200] [Batch 2536/3166] [D loss: 0.690265] [G loss: 0.688987]\n",
      "[Epoch 0/200] [Batch 2537/3166] [D loss: 0.691122] [G loss: 0.693400]\n",
      "[Epoch 0/200] [Batch 2538/3166] [D loss: 0.687845] [G loss: 0.691458]\n",
      "[Epoch 0/200] [Batch 2539/3166] [D loss: 0.688363] [G loss: 0.695416]\n",
      "[Epoch 0/200] [Batch 2540/3166] [D loss: 0.691417] [G loss: 0.695281]\n",
      "[Epoch 0/200] [Batch 2541/3166] [D loss: 0.692043] [G loss: 0.692158]\n",
      "[Epoch 0/200] [Batch 2542/3166] [D loss: 0.689519] [G loss: 0.694943]\n",
      "[Epoch 0/200] [Batch 2543/3166] [D loss: 0.690273] [G loss: 0.698225]\n",
      "[Epoch 0/200] [Batch 2544/3166] [D loss: 0.688743] [G loss: 0.692977]\n",
      "[Epoch 0/200] [Batch 2545/3166] [D loss: 0.688362] [G loss: 0.695451]\n",
      "[Epoch 0/200] [Batch 2546/3166] [D loss: 0.691004] [G loss: 0.693713]\n",
      "[Epoch 0/200] [Batch 2547/3166] [D loss: 0.689398] [G loss: 0.694360]\n",
      "[Epoch 0/200] [Batch 2548/3166] [D loss: 0.691672] [G loss: 0.689780]\n",
      "[Epoch 0/200] [Batch 2549/3166] [D loss: 0.692238] [G loss: 0.692361]\n",
      "[Epoch 0/200] [Batch 2550/3166] [D loss: 0.693565] [G loss: 0.691993]\n",
      "[Epoch 0/200] [Batch 2551/3166] [D loss: 0.696311] [G loss: 0.687502]\n",
      "[Epoch 0/200] [Batch 2552/3166] [D loss: 0.694508] [G loss: 0.689206]\n",
      "[Epoch 0/200] [Batch 2553/3166] [D loss: 0.692839] [G loss: 0.689147]\n",
      "[Epoch 0/200] [Batch 2554/3166] [D loss: 0.692790] [G loss: 0.689271]\n",
      "[Epoch 0/200] [Batch 2555/3166] [D loss: 0.696057] [G loss: 0.689487]\n",
      "[Epoch 0/200] [Batch 2556/3166] [D loss: 0.690958] [G loss: 0.699655]\n",
      "[Epoch 0/200] [Batch 2557/3166] [D loss: 0.691528] [G loss: 0.691925]\n",
      "[Epoch 0/200] [Batch 2558/3166] [D loss: 0.691063] [G loss: 0.708230]\n",
      "[Epoch 0/200] [Batch 2559/3166] [D loss: 0.690278] [G loss: 0.704355]\n",
      "[Epoch 0/200] [Batch 2560/3166] [D loss: 0.690034] [G loss: 0.696274]\n",
      "[Epoch 0/200] [Batch 2561/3166] [D loss: 0.690012] [G loss: 0.694488]\n",
      "[Epoch 0/200] [Batch 2562/3166] [D loss: 0.691138] [G loss: 0.704252]\n",
      "[Epoch 0/200] [Batch 2563/3166] [D loss: 0.692888] [G loss: 0.692484]\n",
      "[Epoch 0/200] [Batch 2564/3166] [D loss: 0.690854] [G loss: 0.704810]\n",
      "[Epoch 0/200] [Batch 2565/3166] [D loss: 0.690882] [G loss: 0.709537]\n",
      "[Epoch 0/200] [Batch 2566/3166] [D loss: 0.689500] [G loss: 0.709853]\n",
      "[Epoch 0/200] [Batch 2567/3166] [D loss: 0.695573] [G loss: 0.707522]\n",
      "[Epoch 0/200] [Batch 2568/3166] [D loss: 0.694623] [G loss: 0.709522]\n",
      "[Epoch 0/200] [Batch 2569/3166] [D loss: 0.692106] [G loss: 0.701288]\n",
      "[Epoch 0/200] [Batch 2570/3166] [D loss: 0.693712] [G loss: 0.699223]\n",
      "[Epoch 0/200] [Batch 2571/3166] [D loss: 0.695373] [G loss: 0.701327]\n",
      "[Epoch 0/200] [Batch 2572/3166] [D loss: 0.703993] [G loss: 0.686613]\n",
      "[Epoch 0/200] [Batch 2573/3166] [D loss: 0.699145] [G loss: 0.690963]\n",
      "[Epoch 0/200] [Batch 2574/3166] [D loss: 0.691513] [G loss: 0.696496]\n",
      "[Epoch 0/200] [Batch 2575/3166] [D loss: 0.696828] [G loss: 0.691399]\n",
      "[Epoch 0/200] [Batch 2576/3166] [D loss: 0.698403] [G loss: 0.688348]\n",
      "[Epoch 0/200] [Batch 2577/3166] [D loss: 0.695543] [G loss: 0.683933]\n",
      "[Epoch 0/200] [Batch 2578/3166] [D loss: 0.691670] [G loss: 0.690099]\n",
      "[Epoch 0/200] [Batch 2579/3166] [D loss: 0.692299] [G loss: 0.686599]\n",
      "[Epoch 0/200] [Batch 2580/3166] [D loss: 0.689924] [G loss: 0.692249]\n",
      "[Epoch 0/200] [Batch 2581/3166] [D loss: 0.694689] [G loss: 0.688257]\n",
      "[Epoch 0/200] [Batch 2582/3166] [D loss: 0.689921] [G loss: 0.688645]\n",
      "[Epoch 0/200] [Batch 2583/3166] [D loss: 0.687745] [G loss: 0.694032]\n",
      "[Epoch 0/200] [Batch 2584/3166] [D loss: 0.689751] [G loss: 0.685758]\n",
      "[Epoch 0/200] [Batch 2585/3166] [D loss: 0.689033] [G loss: 0.691147]\n",
      "[Epoch 0/200] [Batch 2586/3166] [D loss: 0.690028] [G loss: 0.691607]\n",
      "[Epoch 0/200] [Batch 2587/3166] [D loss: 0.692120] [G loss: 0.691822]\n",
      "[Epoch 0/200] [Batch 2588/3166] [D loss: 0.689102] [G loss: 0.692713]\n",
      "[Epoch 0/200] [Batch 2589/3166] [D loss: 0.690516] [G loss: 0.690702]\n",
      "[Epoch 0/200] [Batch 2590/3166] [D loss: 0.687179] [G loss: 0.693018]\n",
      "[Epoch 0/200] [Batch 2591/3166] [D loss: 0.685466] [G loss: 0.693235]\n",
      "[Epoch 0/200] [Batch 2592/3166] [D loss: 0.684520] [G loss: 0.692274]\n",
      "[Epoch 0/200] [Batch 2593/3166] [D loss: 0.685009] [G loss: 0.703235]\n",
      "[Epoch 0/200] [Batch 2594/3166] [D loss: 0.687894] [G loss: 0.695277]\n",
      "[Epoch 0/200] [Batch 2595/3166] [D loss: 0.679705] [G loss: 0.696065]\n",
      "[Epoch 0/200] [Batch 2596/3166] [D loss: 0.687808] [G loss: 0.686945]\n",
      "[Epoch 0/200] [Batch 2597/3166] [D loss: 0.683552] [G loss: 0.688459]\n",
      "[Epoch 0/200] [Batch 2598/3166] [D loss: 0.687062] [G loss: 0.683672]\n",
      "[Epoch 0/200] [Batch 2599/3166] [D loss: 0.695318] [G loss: 0.674692]\n",
      "[Epoch 0/200] [Batch 2600/3166] [D loss: 0.695324] [G loss: 0.670480]\n",
      "[Epoch 0/200] [Batch 2601/3166] [D loss: 0.697322] [G loss: 0.672554]\n",
      "[Epoch 0/200] [Batch 2602/3166] [D loss: 0.699372] [G loss: 0.673511]\n",
      "[Epoch 0/200] [Batch 2603/3166] [D loss: 0.698816] [G loss: 0.657916]\n",
      "[Epoch 0/200] [Batch 2604/3166] [D loss: 0.702135] [G loss: 0.664514]\n",
      "[Epoch 0/200] [Batch 2605/3166] [D loss: 0.710307] [G loss: 0.665332]\n",
      "[Epoch 0/200] [Batch 2606/3166] [D loss: 0.706042] [G loss: 0.669312]\n",
      "[Epoch 0/200] [Batch 2607/3166] [D loss: 0.706360] [G loss: 0.672448]\n",
      "[Epoch 0/200] [Batch 2608/3166] [D loss: 0.701596] [G loss: 0.675230]\n",
      "[Epoch 0/200] [Batch 2609/3166] [D loss: 0.703499] [G loss: 0.675354]\n",
      "[Epoch 0/200] [Batch 2610/3166] [D loss: 0.701302] [G loss: 0.683284]\n",
      "[Epoch 0/200] [Batch 2611/3166] [D loss: 0.702104] [G loss: 0.690738]\n",
      "[Epoch 0/200] [Batch 2612/3166] [D loss: 0.702456] [G loss: 0.683354]\n",
      "[Epoch 0/200] [Batch 2613/3166] [D loss: 0.696355] [G loss: 0.700960]\n",
      "[Epoch 0/200] [Batch 2614/3166] [D loss: 0.695944] [G loss: 0.695398]\n",
      "[Epoch 0/200] [Batch 2615/3166] [D loss: 0.697080] [G loss: 0.701730]\n",
      "[Epoch 0/200] [Batch 2616/3166] [D loss: 0.691991] [G loss: 0.701841]\n",
      "[Epoch 0/200] [Batch 2617/3166] [D loss: 0.690031] [G loss: 0.706857]\n",
      "[Epoch 0/200] [Batch 2618/3166] [D loss: 0.695436] [G loss: 0.706579]\n",
      "[Epoch 0/200] [Batch 2619/3166] [D loss: 0.694242] [G loss: 0.707014]\n",
      "[Epoch 0/200] [Batch 2620/3166] [D loss: 0.692475] [G loss: 0.708120]\n",
      "[Epoch 0/200] [Batch 2621/3166] [D loss: 0.695947] [G loss: 0.708689]\n",
      "[Epoch 0/200] [Batch 2622/3166] [D loss: 0.691226] [G loss: 0.711894]\n",
      "[Epoch 0/200] [Batch 2623/3166] [D loss: 0.696946] [G loss: 0.706242]\n",
      "[Epoch 0/200] [Batch 2624/3166] [D loss: 0.689272] [G loss: 0.711583]\n",
      "[Epoch 0/200] [Batch 2625/3166] [D loss: 0.687096] [G loss: 0.714388]\n",
      "[Epoch 0/200] [Batch 2626/3166] [D loss: 0.689755] [G loss: 0.708795]\n",
      "[Epoch 0/200] [Batch 2627/3166] [D loss: 0.688600] [G loss: 0.704619]\n",
      "[Epoch 0/200] [Batch 2628/3166] [D loss: 0.692042] [G loss: 0.711732]\n",
      "[Epoch 0/200] [Batch 2629/3166] [D loss: 0.688479] [G loss: 0.714292]\n",
      "[Epoch 0/200] [Batch 2630/3166] [D loss: 0.689475] [G loss: 0.710035]\n",
      "[Epoch 0/200] [Batch 2631/3166] [D loss: 0.687451] [G loss: 0.708284]\n",
      "[Epoch 0/200] [Batch 2632/3166] [D loss: 0.686768] [G loss: 0.708438]\n",
      "[Epoch 0/200] [Batch 2633/3166] [D loss: 0.686325] [G loss: 0.707317]\n",
      "[Epoch 0/200] [Batch 2634/3166] [D loss: 0.689773] [G loss: 0.704500]\n",
      "[Epoch 0/200] [Batch 2635/3166] [D loss: 0.692331] [G loss: 0.700868]\n",
      "[Epoch 0/200] [Batch 2636/3166] [D loss: 0.693580] [G loss: 0.700702]\n",
      "[Epoch 0/200] [Batch 2637/3166] [D loss: 0.692727] [G loss: 0.691464]\n",
      "[Epoch 0/200] [Batch 2638/3166] [D loss: 0.690121] [G loss: 0.699973]\n",
      "[Epoch 0/200] [Batch 2639/3166] [D loss: 0.693098] [G loss: 0.693334]\n",
      "[Epoch 0/200] [Batch 2640/3166] [D loss: 0.689713] [G loss: 0.698063]\n",
      "[Epoch 0/200] [Batch 2641/3166] [D loss: 0.691929] [G loss: 0.694432]\n",
      "[Epoch 0/200] [Batch 2642/3166] [D loss: 0.690161] [G loss: 0.694338]\n",
      "[Epoch 0/200] [Batch 2643/3166] [D loss: 0.689815] [G loss: 0.688758]\n",
      "[Epoch 0/200] [Batch 2644/3166] [D loss: 0.691613] [G loss: 0.693678]\n",
      "[Epoch 0/200] [Batch 2645/3166] [D loss: 0.688237] [G loss: 0.689929]\n",
      "[Epoch 0/200] [Batch 2646/3166] [D loss: 0.693065] [G loss: 0.686129]\n",
      "[Epoch 0/200] [Batch 2647/3166] [D loss: 0.696202] [G loss: 0.687551]\n",
      "[Epoch 0/200] [Batch 2648/3166] [D loss: 0.699471] [G loss: 0.677658]\n",
      "[Epoch 0/200] [Batch 2649/3166] [D loss: 0.697380] [G loss: 0.678864]\n",
      "[Epoch 0/200] [Batch 2650/3166] [D loss: 0.697646] [G loss: 0.675881]\n",
      "[Epoch 0/200] [Batch 2651/3166] [D loss: 0.691306] [G loss: 0.689782]\n",
      "[Epoch 0/200] [Batch 2652/3166] [D loss: 0.699667] [G loss: 0.685300]\n",
      "[Epoch 0/200] [Batch 2653/3166] [D loss: 0.695468] [G loss: 0.694286]\n",
      "[Epoch 0/200] [Batch 2654/3166] [D loss: 0.689065] [G loss: 0.694471]\n",
      "[Epoch 0/200] [Batch 2655/3166] [D loss: 0.692528] [G loss: 0.703384]\n",
      "[Epoch 0/200] [Batch 2656/3166] [D loss: 0.687213] [G loss: 0.707554]\n",
      "[Epoch 0/200] [Batch 2657/3166] [D loss: 0.684417] [G loss: 0.714321]\n",
      "[Epoch 0/200] [Batch 2658/3166] [D loss: 0.688631] [G loss: 0.708943]\n",
      "[Epoch 0/200] [Batch 2659/3166] [D loss: 0.688635] [G loss: 0.709998]\n",
      "[Epoch 0/200] [Batch 2660/3166] [D loss: 0.688272] [G loss: 0.714216]\n",
      "[Epoch 0/200] [Batch 2661/3166] [D loss: 0.689395] [G loss: 0.709591]\n",
      "[Epoch 0/200] [Batch 2662/3166] [D loss: 0.688633] [G loss: 0.703546]\n",
      "[Epoch 0/200] [Batch 2663/3166] [D loss: 0.690671] [G loss: 0.703122]\n",
      "[Epoch 0/200] [Batch 2664/3166] [D loss: 0.686668] [G loss: 0.701916]\n",
      "[Epoch 0/200] [Batch 2665/3166] [D loss: 0.689945] [G loss: 0.703002]\n",
      "[Epoch 0/200] [Batch 2666/3166] [D loss: 0.684610] [G loss: 0.701141]\n",
      "[Epoch 0/200] [Batch 2667/3166] [D loss: 0.686738] [G loss: 0.698106]\n",
      "[Epoch 0/200] [Batch 2668/3166] [D loss: 0.685532] [G loss: 0.688628]\n",
      "[Epoch 0/200] [Batch 2669/3166] [D loss: 0.687592] [G loss: 0.695479]\n",
      "[Epoch 0/200] [Batch 2670/3166] [D loss: 0.691172] [G loss: 0.689728]\n",
      "[Epoch 0/200] [Batch 2671/3166] [D loss: 0.685241] [G loss: 0.691766]\n",
      "[Epoch 0/200] [Batch 2672/3166] [D loss: 0.688527] [G loss: 0.686456]\n",
      "[Epoch 0/200] [Batch 2673/3166] [D loss: 0.690315] [G loss: 0.679766]\n",
      "[Epoch 0/200] [Batch 2674/3166] [D loss: 0.692531] [G loss: 0.674519]\n",
      "[Epoch 0/200] [Batch 2675/3166] [D loss: 0.698726] [G loss: 0.670205]\n",
      "[Epoch 0/200] [Batch 2676/3166] [D loss: 0.694003] [G loss: 0.680211]\n",
      "[Epoch 0/200] [Batch 2677/3166] [D loss: 0.698329] [G loss: 0.672917]\n",
      "[Epoch 0/200] [Batch 2678/3166] [D loss: 0.698567] [G loss: 0.684448]\n",
      "[Epoch 0/200] [Batch 2679/3166] [D loss: 0.699449] [G loss: 0.668449]\n",
      "[Epoch 0/200] [Batch 2680/3166] [D loss: 0.704660] [G loss: 0.679255]\n",
      "[Epoch 0/200] [Batch 2681/3166] [D loss: 0.694846] [G loss: 0.690857]\n",
      "[Epoch 0/200] [Batch 2682/3166] [D loss: 0.695199] [G loss: 0.699414]\n",
      "[Epoch 0/200] [Batch 2683/3166] [D loss: 0.698626] [G loss: 0.694865]\n",
      "[Epoch 0/200] [Batch 2684/3166] [D loss: 0.694284] [G loss: 0.698011]\n",
      "[Epoch 0/200] [Batch 2685/3166] [D loss: 0.693274] [G loss: 0.692700]\n",
      "[Epoch 0/200] [Batch 2686/3166] [D loss: 0.699328] [G loss: 0.693133]\n",
      "[Epoch 0/200] [Batch 2687/3166] [D loss: 0.696418] [G loss: 0.691261]\n",
      "[Epoch 0/200] [Batch 2688/3166] [D loss: 0.697454] [G loss: 0.693779]\n",
      "[Epoch 0/200] [Batch 2689/3166] [D loss: 0.692678] [G loss: 0.689575]\n",
      "[Epoch 0/200] [Batch 2690/3166] [D loss: 0.701143] [G loss: 0.686139]\n",
      "[Epoch 0/200] [Batch 2691/3166] [D loss: 0.698397] [G loss: 0.687529]\n",
      "[Epoch 0/200] [Batch 2692/3166] [D loss: 0.697293] [G loss: 0.690370]\n",
      "[Epoch 0/200] [Batch 2693/3166] [D loss: 0.695623] [G loss: 0.683641]\n",
      "[Epoch 0/200] [Batch 2694/3166] [D loss: 0.694708] [G loss: 0.685910]\n",
      "[Epoch 0/200] [Batch 2695/3166] [D loss: 0.697639] [G loss: 0.686198]\n",
      "[Epoch 0/200] [Batch 2696/3166] [D loss: 0.696813] [G loss: 0.687861]\n",
      "[Epoch 0/200] [Batch 2697/3166] [D loss: 0.700630] [G loss: 0.685807]\n",
      "[Epoch 0/200] [Batch 2698/3166] [D loss: 0.694928] [G loss: 0.691245]\n",
      "[Epoch 0/200] [Batch 2699/3166] [D loss: 0.691915] [G loss: 0.688141]\n",
      "[Epoch 0/200] [Batch 2700/3166] [D loss: 0.690775] [G loss: 0.691997]\n",
      "[Epoch 0/200] [Batch 2701/3166] [D loss: 0.695111] [G loss: 0.689648]\n",
      "[Epoch 0/200] [Batch 2702/3166] [D loss: 0.695204] [G loss: 0.688434]\n",
      "[Epoch 0/200] [Batch 2703/3166] [D loss: 0.693110] [G loss: 0.694284]\n",
      "[Epoch 0/200] [Batch 2704/3166] [D loss: 0.689176] [G loss: 0.690931]\n",
      "[Epoch 0/200] [Batch 2705/3166] [D loss: 0.694302] [G loss: 0.692743]\n",
      "[Epoch 0/200] [Batch 2706/3166] [D loss: 0.693360] [G loss: 0.697098]\n",
      "[Epoch 0/200] [Batch 2707/3166] [D loss: 0.691100] [G loss: 0.694545]\n",
      "[Epoch 0/200] [Batch 2708/3166] [D loss: 0.690479] [G loss: 0.704466]\n",
      "[Epoch 0/200] [Batch 2709/3166] [D loss: 0.688216] [G loss: 0.699923]\n",
      "[Epoch 0/200] [Batch 2710/3166] [D loss: 0.692359] [G loss: 0.704699]\n",
      "[Epoch 0/200] [Batch 2711/3166] [D loss: 0.690434] [G loss: 0.701381]\n",
      "[Epoch 0/200] [Batch 2712/3166] [D loss: 0.691821] [G loss: 0.701588]\n",
      "[Epoch 0/200] [Batch 2713/3166] [D loss: 0.696320] [G loss: 0.691924]\n",
      "[Epoch 0/200] [Batch 2714/3166] [D loss: 0.694388] [G loss: 0.698040]\n",
      "[Epoch 0/200] [Batch 2715/3166] [D loss: 0.692876] [G loss: 0.689793]\n",
      "[Epoch 0/200] [Batch 2716/3166] [D loss: 0.697084] [G loss: 0.689666]\n",
      "[Epoch 0/200] [Batch 2717/3166] [D loss: 0.692687] [G loss: 0.691603]\n",
      "[Epoch 0/200] [Batch 2718/3166] [D loss: 0.696212] [G loss: 0.682526]\n",
      "[Epoch 0/200] [Batch 2719/3166] [D loss: 0.695485] [G loss: 0.685885]\n",
      "[Epoch 0/200] [Batch 2720/3166] [D loss: 0.699846] [G loss: 0.685119]\n",
      "[Epoch 0/200] [Batch 2721/3166] [D loss: 0.692207] [G loss: 0.687898]\n",
      "[Epoch 0/200] [Batch 2722/3166] [D loss: 0.698262] [G loss: 0.682632]\n",
      "[Epoch 0/200] [Batch 2723/3166] [D loss: 0.697308] [G loss: 0.684905]\n",
      "[Epoch 0/200] [Batch 2724/3166] [D loss: 0.702106] [G loss: 0.680660]\n",
      "[Epoch 0/200] [Batch 2725/3166] [D loss: 0.696616] [G loss: 0.681476]\n",
      "[Epoch 0/200] [Batch 2726/3166] [D loss: 0.694280] [G loss: 0.691089]\n",
      "[Epoch 0/200] [Batch 2727/3166] [D loss: 0.695584] [G loss: 0.690294]\n",
      "[Epoch 0/200] [Batch 2728/3166] [D loss: 0.692148] [G loss: 0.693082]\n",
      "[Epoch 0/200] [Batch 2729/3166] [D loss: 0.694719] [G loss: 0.695653]\n",
      "[Epoch 0/200] [Batch 2730/3166] [D loss: 0.696006] [G loss: 0.690397]\n",
      "[Epoch 0/200] [Batch 2731/3166] [D loss: 0.693843] [G loss: 0.695158]\n",
      "[Epoch 0/200] [Batch 2732/3166] [D loss: 0.696210] [G loss: 0.699420]\n",
      "[Epoch 0/200] [Batch 2733/3166] [D loss: 0.690508] [G loss: 0.699002]\n",
      "[Epoch 0/200] [Batch 2734/3166] [D loss: 0.687748] [G loss: 0.705609]\n",
      "[Epoch 0/200] [Batch 2735/3166] [D loss: 0.691961] [G loss: 0.700417]\n",
      "[Epoch 0/200] [Batch 2736/3166] [D loss: 0.690869] [G loss: 0.701472]\n",
      "[Epoch 0/200] [Batch 2737/3166] [D loss: 0.693489] [G loss: 0.701753]\n",
      "[Epoch 0/200] [Batch 2738/3166] [D loss: 0.694040] [G loss: 0.702080]\n",
      "[Epoch 0/200] [Batch 2739/3166] [D loss: 0.691025] [G loss: 0.696311]\n",
      "[Epoch 0/200] [Batch 2740/3166] [D loss: 0.696015] [G loss: 0.697159]\n",
      "[Epoch 0/200] [Batch 2741/3166] [D loss: 0.694664] [G loss: 0.698795]\n",
      "[Epoch 0/200] [Batch 2742/3166] [D loss: 0.692693] [G loss: 0.699868]\n",
      "[Epoch 0/200] [Batch 2743/3166] [D loss: 0.690658] [G loss: 0.700745]\n",
      "[Epoch 0/200] [Batch 2744/3166] [D loss: 0.690606] [G loss: 0.697487]\n",
      "[Epoch 0/200] [Batch 2745/3166] [D loss: 0.691367] [G loss: 0.695796]\n",
      "[Epoch 0/200] [Batch 2746/3166] [D loss: 0.692012] [G loss: 0.695043]\n",
      "[Epoch 0/200] [Batch 2747/3166] [D loss: 0.691122] [G loss: 0.696355]\n",
      "[Epoch 0/200] [Batch 2748/3166] [D loss: 0.694079] [G loss: 0.689581]\n",
      "[Epoch 0/200] [Batch 2749/3166] [D loss: 0.691930] [G loss: 0.691604]\n",
      "[Epoch 0/200] [Batch 2750/3166] [D loss: 0.691224] [G loss: 0.692486]\n",
      "[Epoch 0/200] [Batch 2751/3166] [D loss: 0.697333] [G loss: 0.689482]\n",
      "[Epoch 0/200] [Batch 2752/3166] [D loss: 0.696845] [G loss: 0.688543]\n",
      "[Epoch 0/200] [Batch 2753/3166] [D loss: 0.695111] [G loss: 0.696564]\n",
      "[Epoch 0/200] [Batch 2754/3166] [D loss: 0.699550] [G loss: 0.690577]\n",
      "[Epoch 0/200] [Batch 2755/3166] [D loss: 0.695055] [G loss: 0.688809]\n",
      "[Epoch 0/200] [Batch 2756/3166] [D loss: 0.699018] [G loss: 0.686800]\n",
      "[Epoch 0/200] [Batch 2757/3166] [D loss: 0.692960] [G loss: 0.692729]\n",
      "[Epoch 0/200] [Batch 2758/3166] [D loss: 0.693871] [G loss: 0.695263]\n",
      "[Epoch 0/200] [Batch 2759/3166] [D loss: 0.694234] [G loss: 0.692590]\n",
      "[Epoch 0/200] [Batch 2760/3166] [D loss: 0.698630] [G loss: 0.696066]\n",
      "[Epoch 0/200] [Batch 2761/3166] [D loss: 0.695803] [G loss: 0.698216]\n",
      "[Epoch 0/200] [Batch 2762/3166] [D loss: 0.695765] [G loss: 0.695840]\n",
      "[Epoch 0/200] [Batch 2763/3166] [D loss: 0.692833] [G loss: 0.696197]\n",
      "[Epoch 0/200] [Batch 2764/3166] [D loss: 0.696890] [G loss: 0.696807]\n",
      "[Epoch 0/200] [Batch 2765/3166] [D loss: 0.697052] [G loss: 0.699454]\n",
      "[Epoch 0/200] [Batch 2766/3166] [D loss: 0.693756] [G loss: 0.697768]\n",
      "[Epoch 0/200] [Batch 2767/3166] [D loss: 0.693368] [G loss: 0.701630]\n",
      "[Epoch 0/200] [Batch 2768/3166] [D loss: 0.696918] [G loss: 0.698064]\n",
      "[Epoch 0/200] [Batch 2769/3166] [D loss: 0.693816] [G loss: 0.701182]\n",
      "[Epoch 0/200] [Batch 2770/3166] [D loss: 0.694343] [G loss: 0.701573]\n",
      "[Epoch 0/200] [Batch 2771/3166] [D loss: 0.691816] [G loss: 0.700813]\n",
      "[Epoch 0/200] [Batch 2772/3166] [D loss: 0.696792] [G loss: 0.699620]\n",
      "[Epoch 0/200] [Batch 2773/3166] [D loss: 0.695417] [G loss: 0.699098]\n",
      "[Epoch 0/200] [Batch 2774/3166] [D loss: 0.692744] [G loss: 0.698810]\n",
      "[Epoch 0/200] [Batch 2775/3166] [D loss: 0.696084] [G loss: 0.700975]\n",
      "[Epoch 0/200] [Batch 2776/3166] [D loss: 0.693839] [G loss: 0.698783]\n",
      "[Epoch 0/200] [Batch 2777/3166] [D loss: 0.694715] [G loss: 0.696037]\n",
      "[Epoch 0/200] [Batch 2778/3166] [D loss: 0.693069] [G loss: 0.697922]\n",
      "[Epoch 0/200] [Batch 2779/3166] [D loss: 0.695165] [G loss: 0.695732]\n",
      "[Epoch 0/200] [Batch 2780/3166] [D loss: 0.693435] [G loss: 0.693961]\n",
      "[Epoch 0/200] [Batch 2781/3166] [D loss: 0.696085] [G loss: 0.695846]\n",
      "[Epoch 0/200] [Batch 2782/3166] [D loss: 0.694742] [G loss: 0.691110]\n",
      "[Epoch 0/200] [Batch 2783/3166] [D loss: 0.693450] [G loss: 0.695023]\n",
      "[Epoch 0/200] [Batch 2784/3166] [D loss: 0.692291] [G loss: 0.698338]\n",
      "[Epoch 0/200] [Batch 2785/3166] [D loss: 0.692223] [G loss: 0.698165]\n",
      "[Epoch 0/200] [Batch 2786/3166] [D loss: 0.692009] [G loss: 0.699445]\n",
      "[Epoch 0/200] [Batch 2787/3166] [D loss: 0.692181] [G loss: 0.695041]\n",
      "[Epoch 0/200] [Batch 2788/3166] [D loss: 0.694729] [G loss: 0.695294]\n",
      "[Epoch 0/200] [Batch 2789/3166] [D loss: 0.693743] [G loss: 0.696073]\n",
      "[Epoch 0/200] [Batch 2790/3166] [D loss: 0.693175] [G loss: 0.697744]\n",
      "[Epoch 0/200] [Batch 2791/3166] [D loss: 0.694856] [G loss: 0.693128]\n",
      "[Epoch 0/200] [Batch 2792/3166] [D loss: 0.690404] [G loss: 0.696094]\n",
      "[Epoch 0/200] [Batch 2793/3166] [D loss: 0.693687] [G loss: 0.694884]\n",
      "[Epoch 0/200] [Batch 2794/3166] [D loss: 0.692397] [G loss: 0.696617]\n",
      "[Epoch 0/200] [Batch 2795/3166] [D loss: 0.694700] [G loss: 0.690560]\n",
      "[Epoch 0/200] [Batch 2796/3166] [D loss: 0.694247] [G loss: 0.693071]\n",
      "[Epoch 0/200] [Batch 2797/3166] [D loss: 0.692219] [G loss: 0.696621]\n",
      "[Epoch 0/200] [Batch 2798/3166] [D loss: 0.697180] [G loss: 0.689777]\n",
      "[Epoch 0/200] [Batch 2799/3166] [D loss: 0.695708] [G loss: 0.691046]\n",
      "[Epoch 0/200] [Batch 2800/3166] [D loss: 0.692608] [G loss: 0.693727]\n",
      "[Epoch 0/200] [Batch 2801/3166] [D loss: 0.694927] [G loss: 0.692809]\n",
      "[Epoch 0/200] [Batch 2802/3166] [D loss: 0.693721] [G loss: 0.692575]\n",
      "[Epoch 0/200] [Batch 2803/3166] [D loss: 0.695341] [G loss: 0.692064]\n",
      "[Epoch 0/200] [Batch 2804/3166] [D loss: 0.696859] [G loss: 0.692782]\n",
      "[Epoch 0/200] [Batch 2805/3166] [D loss: 0.696847] [G loss: 0.694249]\n",
      "[Epoch 0/200] [Batch 2806/3166] [D loss: 0.695285] [G loss: 0.693505]\n",
      "[Epoch 0/200] [Batch 2807/3166] [D loss: 0.692497] [G loss: 0.699215]\n",
      "[Epoch 0/200] [Batch 2808/3166] [D loss: 0.693583] [G loss: 0.696749]\n",
      "[Epoch 0/200] [Batch 2809/3166] [D loss: 0.694382] [G loss: 0.692008]\n",
      "[Epoch 0/200] [Batch 2810/3166] [D loss: 0.694466] [G loss: 0.692832]\n",
      "[Epoch 0/200] [Batch 2811/3166] [D loss: 0.691681] [G loss: 0.696513]\n",
      "[Epoch 0/200] [Batch 2812/3166] [D loss: 0.693442] [G loss: 0.696654]\n",
      "[Epoch 0/200] [Batch 2813/3166] [D loss: 0.693785] [G loss: 0.694960]\n",
      "[Epoch 0/200] [Batch 2814/3166] [D loss: 0.693639] [G loss: 0.693600]\n",
      "[Epoch 0/200] [Batch 2815/3166] [D loss: 0.694733] [G loss: 0.694109]\n",
      "[Epoch 0/200] [Batch 2816/3166] [D loss: 0.696305] [G loss: 0.692813]\n",
      "[Epoch 0/200] [Batch 2817/3166] [D loss: 0.694349] [G loss: 0.696448]\n",
      "[Epoch 0/200] [Batch 2818/3166] [D loss: 0.695646] [G loss: 0.696937]\n",
      "[Epoch 0/200] [Batch 2819/3166] [D loss: 0.694408] [G loss: 0.693942]\n",
      "[Epoch 0/200] [Batch 2820/3166] [D loss: 0.694712] [G loss: 0.693830]\n",
      "[Epoch 0/200] [Batch 2821/3166] [D loss: 0.692239] [G loss: 0.698463]\n",
      "[Epoch 0/200] [Batch 2822/3166] [D loss: 0.694947] [G loss: 0.694250]\n",
      "[Epoch 0/200] [Batch 2823/3166] [D loss: 0.694310] [G loss: 0.694643]\n",
      "[Epoch 0/200] [Batch 2824/3166] [D loss: 0.694529] [G loss: 0.693473]\n",
      "[Epoch 0/200] [Batch 2825/3166] [D loss: 0.694719] [G loss: 0.694220]\n",
      "[Epoch 0/200] [Batch 2826/3166] [D loss: 0.694279] [G loss: 0.697942]\n",
      "[Epoch 0/200] [Batch 2827/3166] [D loss: 0.690543] [G loss: 0.697008]\n",
      "[Epoch 0/200] [Batch 2828/3166] [D loss: 0.691678] [G loss: 0.702112]\n",
      "[Epoch 0/200] [Batch 2829/3166] [D loss: 0.692345] [G loss: 0.696215]\n",
      "[Epoch 0/200] [Batch 2830/3166] [D loss: 0.692532] [G loss: 0.699055]\n",
      "[Epoch 0/200] [Batch 2831/3166] [D loss: 0.694243] [G loss: 0.696524]\n",
      "[Epoch 0/200] [Batch 2832/3166] [D loss: 0.693667] [G loss: 0.696973]\n",
      "[Epoch 0/200] [Batch 2833/3166] [D loss: 0.691169] [G loss: 0.698172]\n",
      "[Epoch 0/200] [Batch 2834/3166] [D loss: 0.691346] [G loss: 0.693702]\n",
      "[Epoch 0/200] [Batch 2835/3166] [D loss: 0.691125] [G loss: 0.698278]\n",
      "[Epoch 0/200] [Batch 2836/3166] [D loss: 0.693940] [G loss: 0.695586]\n",
      "[Epoch 0/200] [Batch 2837/3166] [D loss: 0.692368] [G loss: 0.698453]\n",
      "[Epoch 0/200] [Batch 2838/3166] [D loss: 0.692919] [G loss: 0.696932]\n",
      "[Epoch 0/200] [Batch 2839/3166] [D loss: 0.690849] [G loss: 0.697105]\n",
      "[Epoch 0/200] [Batch 2840/3166] [D loss: 0.693994] [G loss: 0.695584]\n",
      "[Epoch 0/200] [Batch 2841/3166] [D loss: 0.692247] [G loss: 0.694588]\n",
      "[Epoch 0/200] [Batch 2842/3166] [D loss: 0.693102] [G loss: 0.693926]\n",
      "[Epoch 0/200] [Batch 2843/3166] [D loss: 0.694213] [G loss: 0.693771]\n",
      "[Epoch 0/200] [Batch 2844/3166] [D loss: 0.693369] [G loss: 0.695424]\n",
      "[Epoch 0/200] [Batch 2845/3166] [D loss: 0.692625] [G loss: 0.695383]\n",
      "[Epoch 0/200] [Batch 2846/3166] [D loss: 0.692790] [G loss: 0.694660]\n",
      "[Epoch 0/200] [Batch 2847/3166] [D loss: 0.691543] [G loss: 0.696361]\n",
      "[Epoch 0/200] [Batch 2848/3166] [D loss: 0.693632] [G loss: 0.693920]\n",
      "[Epoch 0/200] [Batch 2849/3166] [D loss: 0.692471] [G loss: 0.692566]\n",
      "[Epoch 0/200] [Batch 2850/3166] [D loss: 0.693298] [G loss: 0.696746]\n",
      "[Epoch 0/200] [Batch 2851/3166] [D loss: 0.694396] [G loss: 0.694217]\n",
      "[Epoch 0/200] [Batch 2852/3166] [D loss: 0.692078] [G loss: 0.693766]\n",
      "[Epoch 0/200] [Batch 2853/3166] [D loss: 0.694047] [G loss: 0.693495]\n",
      "[Epoch 0/200] [Batch 2854/3166] [D loss: 0.693464] [G loss: 0.696297]\n",
      "[Epoch 0/200] [Batch 2855/3166] [D loss: 0.695429] [G loss: 0.692678]\n",
      "[Epoch 0/200] [Batch 2856/3166] [D loss: 0.692837] [G loss: 0.695029]\n",
      "[Epoch 0/200] [Batch 2857/3166] [D loss: 0.693557] [G loss: 0.692175]\n",
      "[Epoch 0/200] [Batch 2858/3166] [D loss: 0.693043] [G loss: 0.691401]\n",
      "[Epoch 0/200] [Batch 2859/3166] [D loss: 0.693493] [G loss: 0.691520]\n",
      "[Epoch 0/200] [Batch 2860/3166] [D loss: 0.692428] [G loss: 0.693772]\n",
      "[Epoch 0/200] [Batch 2861/3166] [D loss: 0.692766] [G loss: 0.692783]\n",
      "[Epoch 0/200] [Batch 2862/3166] [D loss: 0.693585] [G loss: 0.691108]\n",
      "[Epoch 0/200] [Batch 2863/3166] [D loss: 0.692417] [G loss: 0.693393]\n",
      "[Epoch 0/200] [Batch 2864/3166] [D loss: 0.690682] [G loss: 0.692498]\n",
      "[Epoch 0/200] [Batch 2865/3166] [D loss: 0.693485] [G loss: 0.689332]\n",
      "[Epoch 0/200] [Batch 2866/3166] [D loss: 0.692219] [G loss: 0.694028]\n",
      "[Epoch 0/200] [Batch 2867/3166] [D loss: 0.693896] [G loss: 0.691862]\n",
      "[Epoch 0/200] [Batch 2868/3166] [D loss: 0.692222] [G loss: 0.692590]\n",
      "[Epoch 0/200] [Batch 2869/3166] [D loss: 0.694255] [G loss: 0.690491]\n",
      "[Epoch 0/200] [Batch 2870/3166] [D loss: 0.691872] [G loss: 0.693348]\n",
      "[Epoch 0/200] [Batch 2871/3166] [D loss: 0.691298] [G loss: 0.692219]\n",
      "[Epoch 0/200] [Batch 2872/3166] [D loss: 0.693268] [G loss: 0.692148]\n",
      "[Epoch 0/200] [Batch 2873/3166] [D loss: 0.692766] [G loss: 0.692076]\n",
      "[Epoch 0/200] [Batch 2874/3166] [D loss: 0.691768] [G loss: 0.690686]\n",
      "[Epoch 0/200] [Batch 2875/3166] [D loss: 0.693401] [G loss: 0.690377]\n",
      "[Epoch 0/200] [Batch 2876/3166] [D loss: 0.692121] [G loss: 0.690045]\n",
      "[Epoch 0/200] [Batch 2877/3166] [D loss: 0.694244] [G loss: 0.691593]\n",
      "[Epoch 0/200] [Batch 2878/3166] [D loss: 0.693727] [G loss: 0.690789]\n",
      "[Epoch 0/200] [Batch 2879/3166] [D loss: 0.693103] [G loss: 0.691414]\n",
      "[Epoch 0/200] [Batch 2880/3166] [D loss: 0.693705] [G loss: 0.690660]\n",
      "[Epoch 0/200] [Batch 2881/3166] [D loss: 0.694172] [G loss: 0.689218]\n",
      "[Epoch 0/200] [Batch 2882/3166] [D loss: 0.692903] [G loss: 0.689191]\n",
      "[Epoch 0/200] [Batch 2883/3166] [D loss: 0.693165] [G loss: 0.691781]\n",
      "[Epoch 0/200] [Batch 2884/3166] [D loss: 0.693038] [G loss: 0.689324]\n",
      "[Epoch 0/200] [Batch 2885/3166] [D loss: 0.692821] [G loss: 0.691958]\n",
      "[Epoch 0/200] [Batch 2886/3166] [D loss: 0.691482] [G loss: 0.690390]\n",
      "[Epoch 0/200] [Batch 2887/3166] [D loss: 0.693292] [G loss: 0.686468]\n",
      "[Epoch 0/200] [Batch 2888/3166] [D loss: 0.693173] [G loss: 0.690078]\n",
      "[Epoch 0/200] [Batch 2889/3166] [D loss: 0.690334] [G loss: 0.691287]\n",
      "[Epoch 0/200] [Batch 2890/3166] [D loss: 0.692933] [G loss: 0.687853]\n",
      "[Epoch 0/200] [Batch 2891/3166] [D loss: 0.693965] [G loss: 0.689410]\n",
      "[Epoch 0/200] [Batch 2892/3166] [D loss: 0.691477] [G loss: 0.688586]\n",
      "[Epoch 0/200] [Batch 2893/3166] [D loss: 0.692039] [G loss: 0.690417]\n",
      "[Epoch 0/200] [Batch 2894/3166] [D loss: 0.692170] [G loss: 0.689340]\n",
      "[Epoch 0/200] [Batch 2895/3166] [D loss: 0.692372] [G loss: 0.689345]\n",
      "[Epoch 0/200] [Batch 2896/3166] [D loss: 0.690195] [G loss: 0.689188]\n",
      "[Epoch 0/200] [Batch 2897/3166] [D loss: 0.692092] [G loss: 0.691778]\n",
      "[Epoch 0/200] [Batch 2898/3166] [D loss: 0.691239] [G loss: 0.690634]\n",
      "[Epoch 0/200] [Batch 2899/3166] [D loss: 0.693035] [G loss: 0.690921]\n",
      "[Epoch 0/200] [Batch 2900/3166] [D loss: 0.691263] [G loss: 0.691860]\n",
      "[Epoch 0/200] [Batch 2901/3166] [D loss: 0.691664] [G loss: 0.690593]\n",
      "[Epoch 0/200] [Batch 2902/3166] [D loss: 0.689117] [G loss: 0.696173]\n",
      "[Epoch 0/200] [Batch 2903/3166] [D loss: 0.691837] [G loss: 0.695611]\n",
      "[Epoch 0/200] [Batch 2904/3166] [D loss: 0.690254] [G loss: 0.697170]\n",
      "[Epoch 0/200] [Batch 2905/3166] [D loss: 0.691309] [G loss: 0.691349]\n",
      "[Epoch 0/200] [Batch 2906/3166] [D loss: 0.691528] [G loss: 0.693629]\n",
      "[Epoch 0/200] [Batch 2907/3166] [D loss: 0.690480] [G loss: 0.691781]\n",
      "[Epoch 0/200] [Batch 2908/3166] [D loss: 0.690939] [G loss: 0.694066]\n",
      "[Epoch 0/200] [Batch 2909/3166] [D loss: 0.694636] [G loss: 0.693148]\n",
      "[Epoch 0/200] [Batch 2910/3166] [D loss: 0.690162] [G loss: 0.693013]\n",
      "[Epoch 0/200] [Batch 2911/3166] [D loss: 0.692083] [G loss: 0.691551]\n",
      "[Epoch 0/200] [Batch 2912/3166] [D loss: 0.690669] [G loss: 0.696088]\n",
      "[Epoch 0/200] [Batch 2913/3166] [D loss: 0.691669] [G loss: 0.689869]\n",
      "[Epoch 0/200] [Batch 2914/3166] [D loss: 0.689641] [G loss: 0.697628]\n",
      "[Epoch 0/200] [Batch 2915/3166] [D loss: 0.686602] [G loss: 0.695569]\n",
      "[Epoch 0/200] [Batch 2916/3166] [D loss: 0.688775] [G loss: 0.699576]\n",
      "[Epoch 0/200] [Batch 2917/3166] [D loss: 0.686280] [G loss: 0.698181]\n",
      "[Epoch 0/200] [Batch 2918/3166] [D loss: 0.690198] [G loss: 0.696568]\n",
      "[Epoch 0/200] [Batch 2919/3166] [D loss: 0.687494] [G loss: 0.701209]\n",
      "[Epoch 0/200] [Batch 2920/3166] [D loss: 0.692236] [G loss: 0.690808]\n",
      "[Epoch 0/200] [Batch 2921/3166] [D loss: 0.689577] [G loss: 0.698332]\n",
      "[Epoch 0/200] [Batch 2922/3166] [D loss: 0.688201] [G loss: 0.694069]\n",
      "[Epoch 0/200] [Batch 2923/3166] [D loss: 0.695131] [G loss: 0.691562]\n",
      "[Epoch 0/200] [Batch 2924/3166] [D loss: 0.690810] [G loss: 0.695668]\n",
      "[Epoch 0/200] [Batch 2925/3166] [D loss: 0.692269] [G loss: 0.690891]\n",
      "[Epoch 0/200] [Batch 2926/3166] [D loss: 0.690870] [G loss: 0.697400]\n",
      "[Epoch 0/200] [Batch 2927/3166] [D loss: 0.689222] [G loss: 0.691165]\n",
      "[Epoch 0/200] [Batch 2928/3166] [D loss: 0.692747] [G loss: 0.694297]\n",
      "[Epoch 0/200] [Batch 2929/3166] [D loss: 0.699051] [G loss: 0.687739]\n",
      "[Epoch 0/200] [Batch 2930/3166] [D loss: 0.692120] [G loss: 0.696764]\n",
      "[Epoch 0/200] [Batch 2931/3166] [D loss: 0.695356] [G loss: 0.688682]\n",
      "[Epoch 0/200] [Batch 2932/3166] [D loss: 0.692012] [G loss: 0.688112]\n",
      "[Epoch 0/200] [Batch 2933/3166] [D loss: 0.692670] [G loss: 0.696565]\n",
      "[Epoch 0/200] [Batch 2934/3166] [D loss: 0.695221] [G loss: 0.688016]\n",
      "[Epoch 0/200] [Batch 2935/3166] [D loss: 0.694344] [G loss: 0.688631]\n",
      "[Epoch 0/200] [Batch 2936/3166] [D loss: 0.695841] [G loss: 0.688903]\n",
      "[Epoch 0/200] [Batch 2937/3166] [D loss: 0.693956] [G loss: 0.688359]\n",
      "[Epoch 0/200] [Batch 2938/3166] [D loss: 0.694783] [G loss: 0.691229]\n",
      "[Epoch 0/200] [Batch 2939/3166] [D loss: 0.698462] [G loss: 0.692878]\n",
      "[Epoch 0/200] [Batch 2940/3166] [D loss: 0.698786] [G loss: 0.687936]\n",
      "[Epoch 0/200] [Batch 2941/3166] [D loss: 0.695582] [G loss: 0.694185]\n",
      "[Epoch 0/200] [Batch 2942/3166] [D loss: 0.695851] [G loss: 0.695622]\n",
      "[Epoch 0/200] [Batch 2943/3166] [D loss: 0.693776] [G loss: 0.697902]\n",
      "[Epoch 0/200] [Batch 2944/3166] [D loss: 0.697109] [G loss: 0.694434]\n",
      "[Epoch 0/200] [Batch 2945/3166] [D loss: 0.693415] [G loss: 0.700557]\n",
      "[Epoch 0/200] [Batch 2946/3166] [D loss: 0.696703] [G loss: 0.695339]\n",
      "[Epoch 0/200] [Batch 2947/3166] [D loss: 0.693273] [G loss: 0.698454]\n",
      "[Epoch 0/200] [Batch 2948/3166] [D loss: 0.694873] [G loss: 0.697483]\n",
      "[Epoch 0/200] [Batch 2949/3166] [D loss: 0.695019] [G loss: 0.696793]\n",
      "[Epoch 0/200] [Batch 2950/3166] [D loss: 0.693741] [G loss: 0.698071]\n",
      "[Epoch 0/200] [Batch 2951/3166] [D loss: 0.695585] [G loss: 0.696520]\n",
      "[Epoch 0/200] [Batch 2952/3166] [D loss: 0.695748] [G loss: 0.692956]\n",
      "[Epoch 0/200] [Batch 2953/3166] [D loss: 0.695200] [G loss: 0.695119]\n",
      "[Epoch 0/200] [Batch 2954/3166] [D loss: 0.692840] [G loss: 0.697585]\n",
      "[Epoch 0/200] [Batch 2955/3166] [D loss: 0.693710] [G loss: 0.698161]\n",
      "[Epoch 0/200] [Batch 2956/3166] [D loss: 0.693439] [G loss: 0.698990]\n",
      "[Epoch 0/200] [Batch 2957/3166] [D loss: 0.693244] [G loss: 0.697692]\n",
      "[Epoch 0/200] [Batch 2958/3166] [D loss: 0.694305] [G loss: 0.697631]\n",
      "[Epoch 0/200] [Batch 2959/3166] [D loss: 0.690474] [G loss: 0.702253]\n",
      "[Epoch 0/200] [Batch 2960/3166] [D loss: 0.695344] [G loss: 0.698106]\n",
      "[Epoch 0/200] [Batch 2961/3166] [D loss: 0.690056] [G loss: 0.700768]\n",
      "[Epoch 0/200] [Batch 2962/3166] [D loss: 0.690191] [G loss: 0.703713]\n",
      "[Epoch 0/200] [Batch 2963/3166] [D loss: 0.690460] [G loss: 0.702474]\n",
      "[Epoch 0/200] [Batch 2964/3166] [D loss: 0.689668] [G loss: 0.701827]\n",
      "[Epoch 0/200] [Batch 2965/3166] [D loss: 0.687841] [G loss: 0.701580]\n",
      "[Epoch 0/200] [Batch 2966/3166] [D loss: 0.690586] [G loss: 0.703332]\n",
      "[Epoch 0/200] [Batch 2967/3166] [D loss: 0.690418] [G loss: 0.699858]\n",
      "[Epoch 0/200] [Batch 2968/3166] [D loss: 0.687692] [G loss: 0.698414]\n",
      "[Epoch 0/200] [Batch 2969/3166] [D loss: 0.686307] [G loss: 0.702727]\n",
      "[Epoch 0/200] [Batch 2970/3166] [D loss: 0.689316] [G loss: 0.705022]\n",
      "[Epoch 0/200] [Batch 2971/3166] [D loss: 0.688192] [G loss: 0.698828]\n",
      "[Epoch 0/200] [Batch 2972/3166] [D loss: 0.691226] [G loss: 0.695689]\n",
      "[Epoch 0/200] [Batch 2973/3166] [D loss: 0.688284] [G loss: 0.700110]\n",
      "[Epoch 0/200] [Batch 2974/3166] [D loss: 0.688152] [G loss: 0.697316]\n",
      "[Epoch 0/200] [Batch 2975/3166] [D loss: 0.692615] [G loss: 0.690630]\n",
      "[Epoch 0/200] [Batch 2976/3166] [D loss: 0.690204] [G loss: 0.699333]\n",
      "[Epoch 0/200] [Batch 2977/3166] [D loss: 0.692007] [G loss: 0.691912]\n",
      "[Epoch 0/200] [Batch 2978/3166] [D loss: 0.689062] [G loss: 0.691556]\n",
      "[Epoch 0/200] [Batch 2979/3166] [D loss: 0.696042] [G loss: 0.682343]\n",
      "[Epoch 0/200] [Batch 2980/3166] [D loss: 0.695349] [G loss: 0.683702]\n",
      "[Epoch 0/200] [Batch 2981/3166] [D loss: 0.696285] [G loss: 0.680213]\n",
      "[Epoch 0/200] [Batch 2982/3166] [D loss: 0.692401] [G loss: 0.683651]\n",
      "[Epoch 0/200] [Batch 2983/3166] [D loss: 0.696279] [G loss: 0.683688]\n",
      "[Epoch 0/200] [Batch 2984/3166] [D loss: 0.697109] [G loss: 0.678802]\n",
      "[Epoch 0/200] [Batch 2985/3166] [D loss: 0.697488] [G loss: 0.674658]\n",
      "[Epoch 0/200] [Batch 2986/3166] [D loss: 0.701776] [G loss: 0.676926]\n",
      "[Epoch 0/200] [Batch 2987/3166] [D loss: 0.700566] [G loss: 0.680884]\n",
      "[Epoch 0/200] [Batch 2988/3166] [D loss: 0.700872] [G loss: 0.676677]\n",
      "[Epoch 0/200] [Batch 2989/3166] [D loss: 0.698374] [G loss: 0.684838]\n",
      "[Epoch 0/200] [Batch 2990/3166] [D loss: 0.699382] [G loss: 0.680203]\n",
      "[Epoch 0/200] [Batch 2991/3166] [D loss: 0.700816] [G loss: 0.684763]\n",
      "[Epoch 0/200] [Batch 2992/3166] [D loss: 0.699606] [G loss: 0.685812]\n",
      "[Epoch 0/200] [Batch 2993/3166] [D loss: 0.698302] [G loss: 0.693211]\n",
      "[Epoch 0/200] [Batch 2994/3166] [D loss: 0.698585] [G loss: 0.690096]\n",
      "[Epoch 0/200] [Batch 2995/3166] [D loss: 0.695651] [G loss: 0.695869]\n",
      "[Epoch 0/200] [Batch 2996/3166] [D loss: 0.697171] [G loss: 0.696306]\n",
      "[Epoch 0/200] [Batch 2997/3166] [D loss: 0.696252] [G loss: 0.698152]\n",
      "[Epoch 0/200] [Batch 2998/3166] [D loss: 0.694345] [G loss: 0.695437]\n",
      "[Epoch 0/200] [Batch 2999/3166] [D loss: 0.692501] [G loss: 0.699679]\n",
      "[Epoch 0/200] [Batch 3000/3166] [D loss: 0.694481] [G loss: 0.700535]\n",
      "[Epoch 0/200] [Batch 3001/3166] [D loss: 0.691846] [G loss: 0.702425]\n",
      "[Epoch 0/200] [Batch 3002/3166] [D loss: 0.690406] [G loss: 0.703252]\n",
      "[Epoch 0/200] [Batch 3003/3166] [D loss: 0.689933] [G loss: 0.706375]\n",
      "[Epoch 0/200] [Batch 3004/3166] [D loss: 0.692180] [G loss: 0.704624]\n",
      "[Epoch 0/200] [Batch 3005/3166] [D loss: 0.691019] [G loss: 0.703488]\n",
      "[Epoch 0/200] [Batch 3006/3166] [D loss: 0.691283] [G loss: 0.705716]\n",
      "[Epoch 0/200] [Batch 3007/3166] [D loss: 0.689411] [G loss: 0.708726]\n",
      "[Epoch 0/200] [Batch 3008/3166] [D loss: 0.688283] [G loss: 0.707026]\n",
      "[Epoch 0/200] [Batch 3009/3166] [D loss: 0.690173] [G loss: 0.705950]\n",
      "[Epoch 0/200] [Batch 3010/3166] [D loss: 0.688496] [G loss: 0.708267]\n",
      "[Epoch 0/200] [Batch 3011/3166] [D loss: 0.688176] [G loss: 0.713402]\n",
      "[Epoch 0/200] [Batch 3012/3166] [D loss: 0.687071] [G loss: 0.708374]\n",
      "[Epoch 0/200] [Batch 3013/3166] [D loss: 0.688085] [G loss: 0.704757]\n",
      "[Epoch 0/200] [Batch 3014/3166] [D loss: 0.688708] [G loss: 0.705820]\n",
      "[Epoch 0/200] [Batch 3015/3166] [D loss: 0.685309] [G loss: 0.712867]\n",
      "[Epoch 0/200] [Batch 3016/3166] [D loss: 0.684375] [G loss: 0.706851]\n",
      "[Epoch 0/200] [Batch 3017/3166] [D loss: 0.688246] [G loss: 0.702770]\n",
      "[Epoch 0/200] [Batch 3018/3166] [D loss: 0.689014] [G loss: 0.703413]\n",
      "[Epoch 0/200] [Batch 3019/3166] [D loss: 0.689175] [G loss: 0.705498]\n",
      "[Epoch 0/200] [Batch 3020/3166] [D loss: 0.693269] [G loss: 0.698431]\n",
      "[Epoch 0/200] [Batch 3021/3166] [D loss: 0.692517] [G loss: 0.695490]\n",
      "[Epoch 0/200] [Batch 3022/3166] [D loss: 0.692807] [G loss: 0.698486]\n",
      "[Epoch 0/200] [Batch 3023/3166] [D loss: 0.693242] [G loss: 0.689880]\n",
      "[Epoch 0/200] [Batch 3024/3166] [D loss: 0.699126] [G loss: 0.687777]\n",
      "[Epoch 0/200] [Batch 3025/3166] [D loss: 0.699011] [G loss: 0.683040]\n",
      "[Epoch 0/200] [Batch 3026/3166] [D loss: 0.698715] [G loss: 0.683482]\n",
      "[Epoch 0/200] [Batch 3027/3166] [D loss: 0.697126] [G loss: 0.684652]\n",
      "[Epoch 0/200] [Batch 3028/3166] [D loss: 0.699340] [G loss: 0.680153]\n",
      "[Epoch 0/200] [Batch 3029/3166] [D loss: 0.697820] [G loss: 0.683780]\n",
      "[Epoch 0/200] [Batch 3030/3166] [D loss: 0.699968] [G loss: 0.682577]\n",
      "[Epoch 0/200] [Batch 3031/3166] [D loss: 0.696072] [G loss: 0.682837]\n",
      "[Epoch 0/200] [Batch 3032/3166] [D loss: 0.698190] [G loss: 0.684375]\n",
      "[Epoch 0/200] [Batch 3033/3166] [D loss: 0.695781] [G loss: 0.683438]\n",
      "[Epoch 0/200] [Batch 3034/3166] [D loss: 0.698660] [G loss: 0.686463]\n",
      "[Epoch 0/200] [Batch 3035/3166] [D loss: 0.697440] [G loss: 0.682883]\n",
      "[Epoch 0/200] [Batch 3036/3166] [D loss: 0.697481] [G loss: 0.691167]\n",
      "[Epoch 0/200] [Batch 3037/3166] [D loss: 0.697102] [G loss: 0.690151]\n",
      "[Epoch 0/200] [Batch 3038/3166] [D loss: 0.694131] [G loss: 0.689625]\n",
      "[Epoch 0/200] [Batch 3039/3166] [D loss: 0.695565] [G loss: 0.693228]\n",
      "[Epoch 0/200] [Batch 3040/3166] [D loss: 0.693490] [G loss: 0.692158]\n",
      "[Epoch 0/200] [Batch 3041/3166] [D loss: 0.694856] [G loss: 0.690269]\n",
      "[Epoch 0/200] [Batch 3042/3166] [D loss: 0.693262] [G loss: 0.690533]\n",
      "[Epoch 0/200] [Batch 3043/3166] [D loss: 0.695065] [G loss: 0.691426]\n",
      "[Epoch 0/200] [Batch 3044/3166] [D loss: 0.689177] [G loss: 0.694238]\n",
      "[Epoch 0/200] [Batch 3045/3166] [D loss: 0.694238] [G loss: 0.693841]\n",
      "[Epoch 0/200] [Batch 3046/3166] [D loss: 0.695155] [G loss: 0.696310]\n",
      "[Epoch 0/200] [Batch 3047/3166] [D loss: 0.694222] [G loss: 0.696228]\n",
      "[Epoch 0/200] [Batch 3048/3166] [D loss: 0.691850] [G loss: 0.700376]\n",
      "[Epoch 0/200] [Batch 3049/3166] [D loss: 0.691771] [G loss: 0.699906]\n",
      "[Epoch 0/200] [Batch 3050/3166] [D loss: 0.692238] [G loss: 0.694634]\n",
      "[Epoch 0/200] [Batch 3051/3166] [D loss: 0.691107] [G loss: 0.697386]\n",
      "[Epoch 0/200] [Batch 3052/3166] [D loss: 0.691018] [G loss: 0.697406]\n",
      "[Epoch 0/200] [Batch 3053/3166] [D loss: 0.689538] [G loss: 0.701647]\n",
      "[Epoch 0/200] [Batch 3054/3166] [D loss: 0.687618] [G loss: 0.701735]\n",
      "[Epoch 0/200] [Batch 3055/3166] [D loss: 0.691266] [G loss: 0.697009]\n",
      "[Epoch 0/200] [Batch 3056/3166] [D loss: 0.693533] [G loss: 0.697529]\n",
      "[Epoch 0/200] [Batch 3057/3166] [D loss: 0.692232] [G loss: 0.694525]\n",
      "[Epoch 0/200] [Batch 3058/3166] [D loss: 0.693803] [G loss: 0.696620]\n",
      "[Epoch 0/200] [Batch 3059/3166] [D loss: 0.691612] [G loss: 0.696983]\n",
      "[Epoch 0/200] [Batch 3060/3166] [D loss: 0.692343] [G loss: 0.691391]\n",
      "[Epoch 0/200] [Batch 3061/3166] [D loss: 0.690748] [G loss: 0.693712]\n",
      "[Epoch 0/200] [Batch 3062/3166] [D loss: 0.694515] [G loss: 0.689624]\n",
      "[Epoch 0/200] [Batch 3063/3166] [D loss: 0.694683] [G loss: 0.690552]\n",
      "[Epoch 0/200] [Batch 3064/3166] [D loss: 0.698473] [G loss: 0.687191]\n",
      "[Epoch 0/200] [Batch 3065/3166] [D loss: 0.696511] [G loss: 0.690239]\n",
      "[Epoch 0/200] [Batch 3066/3166] [D loss: 0.694256] [G loss: 0.691304]\n",
      "[Epoch 0/200] [Batch 3067/3166] [D loss: 0.694041] [G loss: 0.692020]\n",
      "[Epoch 0/200] [Batch 3068/3166] [D loss: 0.695526] [G loss: 0.691890]\n",
      "[Epoch 0/200] [Batch 3069/3166] [D loss: 0.693520] [G loss: 0.691701]\n",
      "[Epoch 0/200] [Batch 3070/3166] [D loss: 0.694846] [G loss: 0.692481]\n",
      "[Epoch 0/200] [Batch 3071/3166] [D loss: 0.692985] [G loss: 0.696473]\n",
      "[Epoch 0/200] [Batch 3072/3166] [D loss: 0.694372] [G loss: 0.694992]\n",
      "[Epoch 0/200] [Batch 3073/3166] [D loss: 0.692187] [G loss: 0.698133]\n",
      "[Epoch 0/200] [Batch 3074/3166] [D loss: 0.692098] [G loss: 0.697061]\n",
      "[Epoch 0/200] [Batch 3075/3166] [D loss: 0.690702] [G loss: 0.698050]\n",
      "[Epoch 0/200] [Batch 3076/3166] [D loss: 0.690149] [G loss: 0.700323]\n",
      "[Epoch 0/200] [Batch 3077/3166] [D loss: 0.691467] [G loss: 0.700091]\n",
      "[Epoch 0/200] [Batch 3078/3166] [D loss: 0.689677] [G loss: 0.704064]\n",
      "[Epoch 0/200] [Batch 3079/3166] [D loss: 0.691265] [G loss: 0.696832]\n",
      "[Epoch 0/200] [Batch 3080/3166] [D loss: 0.688936] [G loss: 0.703107]\n",
      "[Epoch 0/200] [Batch 3081/3166] [D loss: 0.688565] [G loss: 0.703742]\n",
      "[Epoch 0/200] [Batch 3082/3166] [D loss: 0.690985] [G loss: 0.705883]\n",
      "[Epoch 0/200] [Batch 3083/3166] [D loss: 0.689876] [G loss: 0.697523]\n",
      "[Epoch 0/200] [Batch 3084/3166] [D loss: 0.691785] [G loss: 0.700268]\n",
      "[Epoch 0/200] [Batch 3085/3166] [D loss: 0.690522] [G loss: 0.701716]\n",
      "[Epoch 0/200] [Batch 3086/3166] [D loss: 0.690796] [G loss: 0.701876]\n",
      "[Epoch 0/200] [Batch 3087/3166] [D loss: 0.687384] [G loss: 0.702585]\n",
      "[Epoch 0/200] [Batch 3088/3166] [D loss: 0.691483] [G loss: 0.698224]\n",
      "[Epoch 0/200] [Batch 3089/3166] [D loss: 0.692466] [G loss: 0.702195]\n",
      "[Epoch 0/200] [Batch 3090/3166] [D loss: 0.692354] [G loss: 0.696636]\n",
      "[Epoch 0/200] [Batch 3091/3166] [D loss: 0.692109] [G loss: 0.702707]\n",
      "[Epoch 0/200] [Batch 3092/3166] [D loss: 0.692969] [G loss: 0.696964]\n",
      "[Epoch 0/200] [Batch 3093/3166] [D loss: 0.695579] [G loss: 0.697078]\n",
      "[Epoch 0/200] [Batch 3094/3166] [D loss: 0.692546] [G loss: 0.695410]\n",
      "[Epoch 0/200] [Batch 3095/3166] [D loss: 0.691899] [G loss: 0.693321]\n",
      "[Epoch 0/200] [Batch 3096/3166] [D loss: 0.690578] [G loss: 0.695823]\n",
      "[Epoch 0/200] [Batch 3097/3166] [D loss: 0.695051] [G loss: 0.693582]\n",
      "[Epoch 0/200] [Batch 3098/3166] [D loss: 0.691220] [G loss: 0.696253]\n",
      "[Epoch 0/200] [Batch 3099/3166] [D loss: 0.694028] [G loss: 0.694122]\n",
      "[Epoch 0/200] [Batch 3100/3166] [D loss: 0.689991] [G loss: 0.700311]\n",
      "[Epoch 0/200] [Batch 3101/3166] [D loss: 0.692154] [G loss: 0.694235]\n",
      "[Epoch 0/200] [Batch 3102/3166] [D loss: 0.699215] [G loss: 0.686300]\n",
      "[Epoch 0/200] [Batch 3103/3166] [D loss: 0.696023] [G loss: 0.687649]\n",
      "[Epoch 0/200] [Batch 3104/3166] [D loss: 0.698756] [G loss: 0.689554]\n",
      "[Epoch 0/200] [Batch 3105/3166] [D loss: 0.697112] [G loss: 0.690910]\n",
      "[Epoch 0/200] [Batch 3106/3166] [D loss: 0.693386] [G loss: 0.688006]\n",
      "[Epoch 0/200] [Batch 3107/3166] [D loss: 0.697751] [G loss: 0.690856]\n",
      "[Epoch 0/200] [Batch 3108/3166] [D loss: 0.698020] [G loss: 0.684221]\n",
      "[Epoch 0/200] [Batch 3109/3166] [D loss: 0.695464] [G loss: 0.687376]\n",
      "[Epoch 0/200] [Batch 3110/3166] [D loss: 0.695350] [G loss: 0.687715]\n",
      "[Epoch 0/200] [Batch 3111/3166] [D loss: 0.694214] [G loss: 0.687975]\n",
      "[Epoch 0/200] [Batch 3112/3166] [D loss: 0.692661] [G loss: 0.687121]\n",
      "[Epoch 0/200] [Batch 3113/3166] [D loss: 0.695136] [G loss: 0.686602]\n",
      "[Epoch 0/200] [Batch 3114/3166] [D loss: 0.693521] [G loss: 0.687181]\n",
      "[Epoch 0/200] [Batch 3115/3166] [D loss: 0.694006] [G loss: 0.687688]\n",
      "[Epoch 0/200] [Batch 3116/3166] [D loss: 0.690828] [G loss: 0.688667]\n",
      "[Epoch 0/200] [Batch 3117/3166] [D loss: 0.693660] [G loss: 0.688600]\n",
      "[Epoch 0/200] [Batch 3118/3166] [D loss: 0.692010] [G loss: 0.689569]\n",
      "[Epoch 0/200] [Batch 3119/3166] [D loss: 0.693717] [G loss: 0.689415]\n",
      "[Epoch 0/200] [Batch 3120/3166] [D loss: 0.691266] [G loss: 0.689354]\n",
      "[Epoch 0/200] [Batch 3121/3166] [D loss: 0.692422] [G loss: 0.689296]\n",
      "[Epoch 0/200] [Batch 3122/3166] [D loss: 0.691253] [G loss: 0.689421]\n",
      "[Epoch 0/200] [Batch 3123/3166] [D loss: 0.691577] [G loss: 0.689974]\n",
      "[Epoch 0/200] [Batch 3124/3166] [D loss: 0.690864] [G loss: 0.689218]\n",
      "[Epoch 0/200] [Batch 3125/3166] [D loss: 0.689970] [G loss: 0.690953]\n",
      "[Epoch 0/200] [Batch 3126/3166] [D loss: 0.690236] [G loss: 0.688713]\n",
      "[Epoch 0/200] [Batch 3127/3166] [D loss: 0.690552] [G loss: 0.693411]\n",
      "[Epoch 0/200] [Batch 3128/3166] [D loss: 0.691490] [G loss: 0.688875]\n",
      "[Epoch 0/200] [Batch 3129/3166] [D loss: 0.689060] [G loss: 0.694901]\n",
      "[Epoch 0/200] [Batch 3130/3166] [D loss: 0.690044] [G loss: 0.689249]\n",
      "[Epoch 0/200] [Batch 3131/3166] [D loss: 0.690883] [G loss: 0.692625]\n",
      "[Epoch 0/200] [Batch 3132/3166] [D loss: 0.690411] [G loss: 0.690575]\n",
      "[Epoch 0/200] [Batch 3133/3166] [D loss: 0.687426] [G loss: 0.692544]\n",
      "[Epoch 0/200] [Batch 3134/3166] [D loss: 0.690268] [G loss: 0.687342]\n",
      "[Epoch 0/200] [Batch 3135/3166] [D loss: 0.691273] [G loss: 0.687009]\n",
      "[Epoch 0/200] [Batch 3136/3166] [D loss: 0.689332] [G loss: 0.685790]\n",
      "[Epoch 0/200] [Batch 3137/3166] [D loss: 0.692258] [G loss: 0.689276]\n",
      "[Epoch 0/200] [Batch 3138/3166] [D loss: 0.691612] [G loss: 0.690255]\n",
      "[Epoch 0/200] [Batch 3139/3166] [D loss: 0.697408] [G loss: 0.682232]\n",
      "[Epoch 0/200] [Batch 3140/3166] [D loss: 0.693968] [G loss: 0.684596]\n",
      "[Epoch 0/200] [Batch 3141/3166] [D loss: 0.694874] [G loss: 0.682565]\n",
      "[Epoch 0/200] [Batch 3142/3166] [D loss: 0.695721] [G loss: 0.687951]\n",
      "[Epoch 0/200] [Batch 3143/3166] [D loss: 0.692728] [G loss: 0.683333]\n",
      "[Epoch 0/200] [Batch 3144/3166] [D loss: 0.692778] [G loss: 0.686965]\n",
      "[Epoch 0/200] [Batch 3145/3166] [D loss: 0.694460] [G loss: 0.680327]\n",
      "[Epoch 0/200] [Batch 3146/3166] [D loss: 0.690355] [G loss: 0.683369]\n",
      "[Epoch 0/200] [Batch 3147/3166] [D loss: 0.692661] [G loss: 0.688486]\n",
      "[Epoch 0/200] [Batch 3148/3166] [D loss: 0.688583] [G loss: 0.686416]\n",
      "[Epoch 0/200] [Batch 3149/3166] [D loss: 0.692119] [G loss: 0.685244]\n",
      "[Epoch 0/200] [Batch 3150/3166] [D loss: 0.695454] [G loss: 0.686340]\n",
      "[Epoch 0/200] [Batch 3151/3166] [D loss: 0.692208] [G loss: 0.690200]\n",
      "[Epoch 0/200] [Batch 3152/3166] [D loss: 0.695353] [G loss: 0.688121]\n",
      "[Epoch 0/200] [Batch 3153/3166] [D loss: 0.690472] [G loss: 0.692510]\n",
      "[Epoch 0/200] [Batch 3154/3166] [D loss: 0.690865] [G loss: 0.693974]\n",
      "[Epoch 0/200] [Batch 3155/3166] [D loss: 0.693490] [G loss: 0.692446]\n",
      "[Epoch 0/200] [Batch 3156/3166] [D loss: 0.689719] [G loss: 0.695851]\n",
      "[Epoch 0/200] [Batch 3157/3166] [D loss: 0.694626] [G loss: 0.697193]\n",
      "[Epoch 0/200] [Batch 3158/3166] [D loss: 0.693551] [G loss: 0.691868]\n",
      "[Epoch 0/200] [Batch 3159/3166] [D loss: 0.693107] [G loss: 0.697091]\n",
      "[Epoch 0/200] [Batch 3160/3166] [D loss: 0.689128] [G loss: 0.702045]\n",
      "[Epoch 0/200] [Batch 3161/3166] [D loss: 0.689989] [G loss: 0.699345]\n",
      "[Epoch 0/200] [Batch 3162/3166] [D loss: 0.689275] [G loss: 0.702185]\n",
      "[Epoch 0/200] [Batch 3163/3166] [D loss: 0.692575] [G loss: 0.696524]\n",
      "[Epoch 0/200] [Batch 3164/3166] [D loss: 0.693352] [G loss: 0.696456]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShawnD98\\Anaconda3\\envs\\Deeplearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([39, 1])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (39) != input nelement (64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-b4fef04bab9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Measure discriminator's ability to classify real from generated samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mreal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mfake_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfake_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Deeplearning\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Deeplearning\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[1;32m-> 2058\u001b[1;33m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (39) != input nelement (64)"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        real_imgs = real_imgs.resize_(imgs.shape[0], opt.channels, opt.img_size,  opt.img_size)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
